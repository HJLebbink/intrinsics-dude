<?xml version = "1.0" encoding = "utf-8" ?>
<intrinsicsdudedata>
<intrinsic>
<id>4208</id>
<name>__RDTSCP</name>
<cpuid>RDTSCP</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED_INT_PTR mem_addr</sign>
<instr>RDTSCP</instr>
<desc>Copy the current 64-bit value of the processor's time-stamp counter into dst, and store the IA32_TSC_AUX MSR (signature value) into memory at mem_addr.</desc>
<oper>dst[63:0] := TimeStampCounter
MEM[mem_addr+31:mem_addr] := IA32_TSC_AUX[31:0]</oper>
</intrinsic>
<intrinsic>
<id>137</id>
<name>_ADDCARRY_U32</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED_CHAR c_in,UNSIGNED_INT a,UNSIGNED_INT b,UNSIGNED_INT_PTR out</sign>
<instr>ADC</instr>
<desc>Add unsigned 32-bit integers a and b with unsigned 8-bit carry-in c_in (carry flag), and store the unsigned 32-bit result in out, and the carry-out in dst (carry or overflow flag).</desc>
<oper>dst:out[31:0] := a[31:0] + b[31:0] + c_in;</oper>
</intrinsic>
<intrinsic>
<id>138</id>
<name>_ADDCARRY_U64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED_CHAR c_in,UNSIGNED__INT64 a,UNSIGNED__INT64 b,UNSIGNED__INT64_PTR out</sign>
<instr>ADC</instr>
<desc>Add unsigned 64-bit integers a and b with unsigned 8-bit carry-in c_in (carry flag), and store the unsigned 64-bit result in out, and the carry-out in dst (carry or overflow flag).</desc>
<oper>dst:out[63:0] := a[63:0] + b[63:0] + c_in;</oper>
</intrinsic>
<intrinsic>
<id>139</id>
<name>_ADDCARRYX_U32</name>
<cpuid>ADX</cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED_CHAR c_in,UNSIGNED_INT a,UNSIGNED_INT b,UNSIGNED_INT_PTR out</sign>
<instr>ADCX, ADOX</instr>
<desc>Add unsigned 32-bit integers a and b with unsigned 8-bit carry-in c_in (carry or overflow flag), and store the unsigned 32-bit result in out, and the carry-out in dst (carry or overflow flag).</desc>
<oper>dst:out[31:0] := a[31:0] + b[31:0] + c_in;</oper>
</intrinsic>
<intrinsic>
<id>140</id>
<name>_ADDCARRYX_U64</name>
<cpuid>ADX</cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED_CHAR c_in,UNSIGNED__INT64 a,UNSIGNED__INT64 b,UNSIGNED__INT64_PTR out</sign>
<instr>ADCX, ADOX</instr>
<desc>Add unsigned 64-bit integers a and b with unsigned 8-bit carry-in c_in (carry or overflow flag), and store the unsigned 64-bit result in out, and the carry-out in dst (carry or overflow flag).</desc>
<oper>dst:out[63:0] := a[63:0] + b[63:0] + c_in;</oper>
</intrinsic>
<intrinsic>
<id>235</id>
<name>_ALLOW_CPU_FEATURES</name>
<cpuid>SVML</cpuid>
<ret>void</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>NONE</instr>
<desc>Treat the processor-specific feature(s) specified in a as available. Multiple features may be OR'd together. See the valid feature flags below:</desc>
<oper>_FEATURE_GENERIC_IA32
_FEATURE_FPU
_FEATURE_CMOV
_FEATURE_MMX
_FEATURE_FXSAVE
_FEATURE_SSE
_FEATURE_SSE2
_FEATURE_SSE3
_FEATURE_SSSE3
_FEATURE_SSE4_1
_FEATURE_SSE4_2
_FEATURE_MOVBE
_FEATURE_POPCNT
_FEATURE_PCLMULQDQ
_FEATURE_AES
_FEATURE_F16C
_FEATURE_AVX
_FEATURE_RDRND
_FEATURE_FMA
_FEATURE_BMI
_FEATURE_LZCNT
_FEATURE_HLE
_FEATURE_RTM
_FEATURE_AVX2
_FEATURE_KNCNI
_FEATURE_AVX512F
_FEATURE_ADX
_FEATURE_RDSEED
_FEATURE_AVX512ER
_FEATURE_AVX512PF
_FEATURE_AVX512CD
_FEATURE_SHA
_FEATURE_MPX</oper>
</intrinsic>
<intrinsic>
<id>368</id>
<name>_BEXTR_U32</name>
<cpuid>BMI1</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a,UNSIGNED_INT start,UNSIGNED_INT len</sign>
<instr>BEXTR</instr>
<desc>Extract contiguous bits from unsigned 32-bit integer a, and store the result in dst. Extract the number of bits specified by len, starting at the bit specified by start.</desc>
<oper>tmp := ZERO_EXTEND_TO_512(a)
dst := ZERO_EXTEND(tmp[start+len-1:start])</oper>
</intrinsic>
<intrinsic>
<id>369</id>
<name>_BEXTR_U64</name>
<cpuid>BMI1</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a,UNSIGNED_INT start,UNSIGNED_INT len</sign>
<instr>BEXTR</instr>
<desc>Extract contiguous bits from unsigned 64-bit integer a, and store the result in dst. Extract the number of bits specified by len, starting at the bit specified by start.</desc>
<oper>tmp := ZERO_EXTEND_TO_512(a)
dst := ZERO_EXTEND(tmp[start+len-1:start])</oper>
</intrinsic>
<intrinsic>
<id>370</id>
<name>_BIT_SCAN_FORWARD</name>
<cpuid></cpuid>
<ret>int</ret>
<sign>INT a</sign>
<instr>BSF</instr>
<desc>Set dst to the index of the lowest set bit in 32-bit integer a. If no bits are set in a then dst is undefined.</desc>
<oper>tmp := 0
IF a = 0
	dst := undefined
ELSE
	DO WHILE ((tmp &amp;lt; 32) AND a[tmp] = 0)
		tmp := tmp + 1
		dst := tmp
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>371</id>
<name>_BIT_SCAN_REVERSE</name>
<cpuid></cpuid>
<ret>int</ret>
<sign>INT a</sign>
<instr>BSR</instr>
<desc>Set dst to the index of the highest set bit in 32-bit integer a. If no bits are set in a then dst is undefined.</desc>
<oper>tmp := 31
IF a = 0
	dst := undefined
ELSE
	DO WHILE ((tmp &amp;gt; 0) AND a[tmp] = 0)
		tmp := tmp - 1
		dst := tmp
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>372</id>
<name>_BITSCANFORWARD</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED__INT32_PTR index,UNSIGNED__INT32 mask</sign>
<instr>BSF</instr>
<desc>Set index to the index of the lowest set bit in 32-bit integer mask. If no bits are set in mask, then set dst to 0, otherwise set dst to 1.</desc>
<oper>tmp := 0
IF mask = 0
	dst := 0
ELSE
	DO WHILE ((tmp &amp;lt; 32) AND mask[tmp] = 0)
		tmp := tmp + 1
		index := tmp
		dst := 1
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>373</id>
<name>_BITSCANFORWARD64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED__INT32_PTR index,UNSIGNED__INT64 mask</sign>
<instr>BSF</instr>
<desc>Set index to the index of the lowest set bit in 64-bit integer mask. If no bits are set in mask, then set dst to 0, otherwise set dst to 1.</desc>
<oper>tmp := 0
IF mask = 0
	dst := 0
ELSE
	DO WHILE ((tmp &amp;lt; 64) AND mask[tmp] = 0)
		tmp := tmp + 1
		index := tmp
		dst := 1
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>374</id>
<name>_BITSCANREVERSE</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED__INT32_PTR index,UNSIGNED__INT32 mask</sign>
<instr>BSR</instr>
<desc>Set index to the index of the highest set bit in 32-bit integer mask. If no bits are set in mask, then set dst to 0, otherwise set dst to 1.</desc>
<oper>tmp := 31
IF mask = 0
	dst := 0
ELSE
	DO WHILE ((tmp &amp;gt; 0) AND mask[tmp] = 0)
		tmp := tmp - 1
		index := tmp
		dst := 1
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>375</id>
<name>_BITSCANREVERSE64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED__INT32_PTR index,UNSIGNED__INT64 mask</sign>
<instr>BSR</instr>
<desc>Set index to the index of the highest set bit in 64-bit integer mask. If no bits are set in mask, then set dst to 0, otherwise set dst to 1.</desc>
<oper>tmp := 31
IF mask = 0
	dst := 0
ELSE
	DO WHILE ((tmp &amp;gt; 0) AND mask[tmp] = 0)
		tmp := tmp - 1
		index := tmp
		dst := 1
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>376</id>
<name>_BITTEST</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT32_PTR a,__INT32 b</sign>
<instr>BT</instr>
<desc>Return the bit at index b of 32-bit integer a.</desc>
<oper>dst := a[b]</oper>
</intrinsic>
<intrinsic>
<id>377</id>
<name>_BITTEST64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT64_PTR a,__INT64 b</sign>
<instr>BT</instr>
<desc>Return the bit at index b of 64-bit integer a.</desc>
<oper>dst := a[b]</oper>
</intrinsic>
<intrinsic>
<id>378</id>
<name>_BITTESTANDCOMPLEMENT</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT32_PTR a,__INT32 b</sign>
<instr>BTC</instr>
<desc>Return the bit at index b of 32-bit integer a, and set that bit to its complement.</desc>
<oper>dst := a[b]
a[b] := ~a[b]</oper>
</intrinsic>
<intrinsic>
<id>379</id>
<name>_BITTESTANDCOMPLEMENT64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT64_PTR a,__INT64 b</sign>
<instr>BTC</instr>
<desc>Return the bit at index b of 64-bit integer a, and set that bit to its complement.</desc>
<oper>dst := a[b]
a[b] := ~a[b]</oper>
</intrinsic>
<intrinsic>
<id>380</id>
<name>_BITTESTANDRESET</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT32_PTR a,__INT32 b</sign>
<instr>BTR</instr>
<desc>Return the bit at index b of 32-bit integer a, and set that bit to zero.</desc>
<oper>dst := a[b]
a[b] := 0</oper>
</intrinsic>
<intrinsic>
<id>381</id>
<name>_BITTESTANDRESET64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT64_PTR a,__INT64 b</sign>
<instr>BTR</instr>
<desc>Return the bit at index b of 64-bit integer a, and set that bit to zero.</desc>
<oper>dst := a[b]
a[b] := 0</oper>
</intrinsic>
<intrinsic>
<id>382</id>
<name>_BITTESTANDSET</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT32_PTR a,__INT32 b</sign>
<instr>BTS</instr>
<desc>Return the bit at index b of 32-bit integer a, and set that bit to one.</desc>
<oper>dst := a[b]
a[b] := 1</oper>
</intrinsic>
<intrinsic>
<id>383</id>
<name>_BITTESTANDSET64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>__INT64_PTR a,__INT64 b</sign>
<instr>BTS</instr>
<desc>Return the bit at index b of 64-bit integer a, and set that bit to one.</desc>
<oper>dst := a[b]
a[b] := 1</oper>
</intrinsic>
<intrinsic>
<id>416</id>
<name>_BLSI_U32</name>
<cpuid>BMI1</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>BLSI</instr>
<desc>Extract the lowest set bit from unsigned 32-bit integer a and set the corresponding bit in dst. All other bits in dst are zeroed, and all bits are zeroed if no bits are set in a.</desc>
<oper>dst := (-a) BITWISE AND a</oper>
</intrinsic>
<intrinsic>
<id>417</id>
<name>_BLSI_U64</name>
<cpuid>BMI1</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>BLSI</instr>
<desc>Extract the lowest set bit from unsigned 64-bit integer a and set the corresponding bit in dst. All other bits in dst are zeroed, and all bits are zeroed if no bits are set in a.</desc>
<oper>dst := (-a) BITWISE AND a</oper>
</intrinsic>
<intrinsic>
<id>418</id>
<name>_BLSMSK_U32</name>
<cpuid>BMI1</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>BLSMSK</instr>
<desc>Set all the lower bits of dst up to and including the lowest set bit in unsigned 32-bit integer a.</desc>
<oper>dst := (a - 1) XOR a</oper>
</intrinsic>
<intrinsic>
<id>419</id>
<name>_BLSMSK_U64</name>
<cpuid>BMI1</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>BLSMSK</instr>
<desc>Set all the lower bits of dst up to and including the lowest set bit in unsigned 64-bit integer a.</desc>
<oper>dst := (a - 1) XOR a</oper>
</intrinsic>
<intrinsic>
<id>420</id>
<name>_BLSR_U32</name>
<cpuid>BMI1</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>BLSR</instr>
<desc>Copy all bits from unsigned 32-bit integer a to dst, and reset (set to 0) the bit in dst that corresponds to the lowest set bit in a.</desc>
<oper>dst := (a - 1) BITWISE AND a</oper>
</intrinsic>
<intrinsic>
<id>421</id>
<name>_BLSR_U64</name>
<cpuid>BMI1</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>BLSR</instr>
<desc>Copy all bits from unsigned 64-bit integer a to dst, and reset (set to 0) the bit in dst that corresponds to the lowest set bit in a.</desc>
<oper>dst := (a - 1) BITWISE AND a</oper>
</intrinsic>
<intrinsic>
<id>422</id>
<name>_BND_CHK_PTR_BOUNDS</name>
<cpuid>MPX</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR q,SIZE_T size</sign>
<instr>BNDCU, BNDCN</instr>
<desc>Checks if [q, q + size - 1] is within the lower and upper bounds of q and throws a #BR if not.</desc>
<oper>IF (q + size - 1) &amp;lt; q.LB OR (q + size - 1) &amp;gt; q.UB THEN
	#BR;
FI;</oper>
</intrinsic>
<intrinsic>
<id>423</id>
<name>_BND_CHK_PTR_LBOUNDS</name>
<cpuid>MPX</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR q</sign>
<instr>BNDCL</instr>
<desc>Checks if q is within its lower bound, and throws a #BR if not.</desc>
<oper>IF q &amp;lt; q.LB THEN
	#BR;
FI;</oper>
</intrinsic>
<intrinsic>
<id>424</id>
<name>_BND_CHK_PTR_UBOUNDS</name>
<cpuid>MPX</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR q</sign>
<instr>BNDCU, BNDCN</instr>
<desc>Checks if q is within its upper bound, and throws a #BR if not.</desc>
<oper>IF q &amp;gt; q.UB THEN
	#BR;
FI;</oper>
</intrinsic>
<intrinsic>
<id>425</id>
<name>_BND_COPY_PTR_BOUNDS</name>
<cpuid>MPX, SVML</cpuid>
<ret>void *</ret>
<sign>CONST_VOID_PTR q,CONST_VOID_PTR r</sign>
<instr>NONE</instr>
<desc>Make a pointer with the value of q and bounds set to the bounds of r (e.g. copy the bounds of r to pointer q), and store the result in dst.</desc>
<oper>dst := q;
dst.LB := r.LB;
dst.UB := r.UB;</oper>
</intrinsic>
<intrinsic>
<id>426</id>
<name>_BND_GET_PTR_LBOUND</name>
<cpuid>MPX, SVML</cpuid>
<ret>const void *</ret>
<sign>CONST_VOID_PTR q</sign>
<instr>NONE</instr>
<desc>Return the lower bound of q.</desc>
<oper>dst := q.LB</oper>
</intrinsic>
<intrinsic>
<id>427</id>
<name>_BND_GET_PTR_UBOUND</name>
<cpuid>MPX, SVML</cpuid>
<ret>const void *</ret>
<sign>CONST_VOID_PTR q</sign>
<instr>NONE</instr>
<desc>Return the upper bound of q.</desc>
<oper>dst := q.UB</oper>
</intrinsic>
<intrinsic>
<id>428</id>
<name>_BND_INIT_PTR_BOUNDS</name>
<cpuid>MPX, SVML</cpuid>
<ret>void *</ret>
<sign>CONST_VOID_PTR q</sign>
<instr>NONE</instr>
<desc>Make a pointer with the value of q and open bounds, which allow the pointer to access the entire virtual address space, and store the result in dst.</desc>
<oper>dst := q;
dst.LB := 0;
dst.UB := 0;</oper>
</intrinsic>
<intrinsic>
<id>429</id>
<name>_BND_NARROW_PTR_BOUNDS</name>
<cpuid>MPX, SVML</cpuid>
<ret>void *</ret>
<sign>CONST_VOID_PTR q,CONST_VOID_PTR r,SIZE_T size</sign>
<instr>NONE</instr>
<desc>Narrow the bounds for pointer q to the intersection of the bounds of r and the bounds [q, q + size - 1], and store the result in dst.</desc>
<oper>dst := q;
IF r.LB &amp;gt; (q + size - 1) OR r.UB &amp;lt; q THEN
	dst.LB := 1;
	dst.UB := 0;
ELSE
	dst.LB := MAX(r.LB, q);
	dst.UB := MIN(r.UB, (q + size - 1));
FI;</oper>
</intrinsic>
<intrinsic>
<id>430</id>
<name>_BND_SET_PTR_BOUNDS</name>
<cpuid>MPX</cpuid>
<ret>void *</ret>
<sign>CONST_VOID_PTR srcmem,SIZE_T size</sign>
<instr>BNDMK</instr>
<desc>Make a pointer with the value of srcmem and bounds set to [srcmem, srcmem + size - 1], and store the result in dst.</desc>
<oper>dst := srcmem;
dst.LB := srcmem.LB;
dst.UB := srcmem + size - 1;</oper>
</intrinsic>
<intrinsic>
<id>431</id>
<name>_BND_STORE_PTR_BOUNDS</name>
<cpuid>MPX</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR_PTR ptr_addr,CONST_VOID_PTR ptr_val</sign>
<instr>BNDSTX</instr>
<desc>Stores the bounds of ptr_val pointer in memory at address ptr_addr.</desc>
<oper>MEM[ptr_addr].LB := ptr_val.LB;
MEM[ptr_addr].UB := ptr_val.UB;</oper>
</intrinsic>
<intrinsic>
<id>553</id>
<name>_BSWAP</name>
<cpuid></cpuid>
<ret>int</ret>
<sign>INT a</sign>
<instr>BSWAP</instr>
<desc>Reverse the byte order of 32-bit integer a, and store the result in dst. This intrinsic is provided for conversion between little and big endian values.</desc>
<oper>dst[7:0] := a[31:24]
dst[15:8] := a[23:16]
dst[23:16] := a[15:8]
dst[31:24] := a[7:0]</oper>
</intrinsic>
<intrinsic>
<id>554</id>
<name>_BSWAP64</name>
<cpuid></cpuid>
<ret>__int64</ret>
<sign>__INT64 a</sign>
<instr>BSWAP</instr>
<desc>Reverse the byte order of 64-bit integer a, and store the result in dst. This intrinsic is provided for conversion between little and big endian values.</desc>
<oper>dst[7:0] := a[63:56]
dst[15:8] := a[55:48]
dst[23:16] := a[47:40]
dst[31:24] := a[39:32]
dst[39:32] := a[31:24]
dst[47:40] := a[23:16]
dst[55:48] := a[15:8]
dst[63:56] := a[7:0]</oper>
</intrinsic>
<intrinsic>
<id>555</id>
<name>_BZHI_U32</name>
<cpuid>BMI2</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a,UNSIGNED_INT index</sign>
<instr>BZHI</instr>
<desc>Copy all bits from unsigned 32-bit integer a to dst, and reset (set to 0) the high bits in dst starting at index.</desc>
<oper>n := index[7:0]
dst := a
IF (n &amp;lt; 32)
	dst[31:n] := 0
FI</oper>
</intrinsic>
<intrinsic>
<id>556</id>
<name>_BZHI_U64</name>
<cpuid>BMI2</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a,UNSIGNED_INT index</sign>
<instr>BZHI</instr>
<desc>Copy all bits from unsigned 64-bit integer a to dst, and reset (set to 0) the high bits in dst starting at index.</desc>
<oper>n := index[7:0]
dst := a
IF (n &amp;lt; 64)
	dst[63:n] := 0
FI</oper>
</intrinsic>
<intrinsic>
<id>557</id>
<name>_CASTF32_U32</name>
<cpuid></cpuid>
<ret>unsigned __int32</ret>
<sign>FLOAT a</sign>
<instr></instr>
<desc>Cast from type float to type unsigned __int32 without conversion.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>558</id>
<name>_CASTF64_U64</name>
<cpuid></cpuid>
<ret>unsigned __int64</ret>
<sign>DOUBLE a</sign>
<instr></instr>
<desc>Cast from type double to type unsigned __int64 without conversion.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>595</id>
<name>_CASTU32_F32</name>
<cpuid></cpuid>
<ret>float</ret>
<sign>UNSIGNED__INT32 a</sign>
<instr></instr>
<desc>Cast from type unsigned __int32 to type float without conversion.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>596</id>
<name>_CASTU64_F64</name>
<cpuid></cpuid>
<ret>double</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr></instr>
<desc>Cast from type unsigned __int64 to type double without conversion.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>1783</id>
<name>_CVTSH_SS</name>
<cpuid>SVML</cpuid>
<ret>float</ret>
<sign>UNSIGNED_SHORT a</sign>
<instr>NONE</instr>
<desc>Convert the half-precision (16-bit) floating-point value a to a single-precision (32-bit) floating-point value, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP16_To_FP32(a[15:0])</oper>
</intrinsic>
<intrinsic>
<id>1804</id>
<name>_CVTSS_SH</name>
<cpuid>SVML</cpuid>
<ret>unsigned short</ret>
<sign>FLOAT a,INT imm8</sign>
<instr>NONE</instr>
<desc>Convert the single-precision (32-bit) floating-point value a to a half-precision (16-bit) floating-point value, and store the result in dst.</desc>
<oper>dst[15:0] := Convert_FP32_To_FP16(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>2638</id>
<name>_FXRSTOR</name>
<cpuid>FXSR</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr</sign>
<instr>FXRSTOR</instr>
<desc>Reload the x87 FPU, MMX technology, XMM, and MXCSR registers from the 512-byte memory image at mem_addr. This data should have been written to memory previously using the FXSAVE instruction, and in the same format as required by the operating mode. mem_addr must be aligned on a 16-byte boundary.</desc>
<oper>(x87 FPU, MMX, XMM7-XMM0, MXCSR) := Load(MEM[mem_addr])</oper>
</intrinsic>
<intrinsic>
<id>2639</id>
<name>_FXRSTOR64</name>
<cpuid>FXSR</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr</sign>
<instr>FXRSTOR64</instr>
<desc>Reload the x87 FPU, MMX technology, XMM, and MXCSR registers from the 512-byte memory image at mem_addr. This data should have been written to memory previously using the FXSAVE64 instruction, and in the same format as required by the operating mode. mem_addr must be aligned on a 16-byte boundary.</desc>
<oper>(x87 FPU, MMX, XMM7-XMM0, MXCSR) := Load(MEM[mem_addr])</oper>
</intrinsic>
<intrinsic>
<id>2640</id>
<name>_FXSAVE</name>
<cpuid>FXSR</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr</sign>
<instr>FXSAVE</instr>
<desc>Save the current state of the x87 FPU, MMX technology, XMM, and MXCSR registers to a 512-byte memory location at mem_addr. The clayout of the 512-byte region depends on the operating mode. Bytes [511:464] are available for software use and will not be overwritten by the processor.</desc>
<oper>MEM[mem_addr+511*8:mem_addr] := Fxsave(x87 FPU, MMX, XMM7-XMM0, MXCSR)</oper>
</intrinsic>
<intrinsic>
<id>2641</id>
<name>_FXSAVE64</name>
<cpuid>FXSR</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr</sign>
<instr>FXSAVE64</instr>
<desc>Save the current state of the x87 FPU, MMX technology, XMM, and MXCSR registers to a 512-byte memory location at mem_addr. The layout of the 512-byte region depends on the operating mode. Bytes [511:464] are available for software use and will not be overwritten by the processor.</desc>
<oper>MEM[mem_addr+511*8:mem_addr] := Fxsave64(x87 FPU, MMX, XMM7-XMM0, MXCSR)</oper>
</intrinsic>
<intrinsic>
<id>2983</id>
<name>_INVPCID</name>
<cpuid>INVPCID</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT type,VOID_PTR descriptor</sign>
<instr>INVPCID</instr>
<desc>
	Invalidate mappings in the Translation Lookaside Buffers (TLBs) and paging-structure caches for the processor context identifier (PCID) specified by descriptor based on the invalidation type specified in type. 
	The PCID descriptor is specified as a 16-byte memory operand (with no alignment restrictions) where bits [11:0] specify the PCID, and bits [127:64] specify the linear address; bits [63:12] are reserved.
	The types supported are:
		0) Individual-address invalidation: If type is 0, the logical processor invalidates mappings for a single linear address and tagged with the PCID specified in descriptor, except global translations. The instruction may also invalidate global translations, mappings for other linear addresses, or mappings tagged with other PCIDs.
		1) Single-context invalidation: If type is 1, the logical processor invalidates all mappings tagged with the PCID specified in descriptor except global translations. In some cases, it may invalidate mappings for other PCIDs as well.
		2) All-context invalidation: If type is 2, the logical processor invalidates all mappings tagged with any PCID.
		3) All-context invalidation, retaining global translations: If type is 3, the logical processor invalidates all mappings tagged with any PCID except global translations, ignoring descriptor. The instruction may also invalidate global translations as well.
	</desc>
<oper>CASE type OF
0: // individual-address invalidation retaining global translations
	OP_PCID := descriptor[11:0]
	ADDR := descriptor[127:64]
	BREAK
1: // single PCID invalidation retaining globals
	OP_PCID := descriptor[11:0]
	// invalidate all mappings tagged with OP_PCID except global translations
	BREAK
2: // all PCID invalidation
	// invalidate all mappings tagged with any PCID
	BREAK
3: // all PCID invalidation retaining global translations
	// invalidate all mappings tagged with any PCID except global translations
	BREAK
ESAC</oper>
</intrinsic>
<intrinsic>
<id>3071</id>
<name>_LOADBE_I16</name>
<cpuid>SVML</cpuid>
<ret>short</ret>
<sign>VOID_CONST_PTR ptr</sign>
<instr>NONE</instr>
<desc>Loads a big-endian word (16-bit) value from address ptr and stores the result in dst.
	</desc>
<oper>addr := MEM[ptr]
FOR j := 0 to 1
	i := j*8
	dst[i+7:i] := addr[15-i:15-i-7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3072</id>
<name>_LOADBE_I32</name>
<cpuid>SVML</cpuid>
<ret>int</ret>
<sign>VOID_CONST_PTR ptr</sign>
<instr>NONE</instr>
<desc>Loads a big-endian double word (32-bit) value from address ptr and stores the result in dst.
	</desc>
<oper>addr := MEM[ptr]
FOR j := 0 to 4
	i := j*8
	dst[i+7:i] := addr[31-i:31-i-7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3073</id>
<name>_LOADBE_I64</name>
<cpuid>SVML</cpuid>
<ret>__int64</ret>
<sign>VOID_CONST_PTR ptr</sign>
<instr>NONE</instr>
<desc>Loads a big-endian quad word (64-bit) value from address ptr and stores the result in dst.
	</desc>
<oper>addr := MEM[ptr]
FOR j := 0 to 8
	i := j*8
	dst[i+7:i] := addr[63-i:63-i-7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3194</id>
<name>_LROTL</name>
<cpuid></cpuid>
<ret>unsigned long</ret>
<sign>UNSIGNED_LONG a,INT shift</sign>
<instr>ROL</instr>
<desc>Shift the bits of unsigned 64-bit integer a left by the number of bits specified in shift, rotating the most-significant bit to the least-significant bit location, and store the unsigned result in dst.</desc>
<oper>dst := a
count := shift BITWISE AND 63
DO WHILE (count &amp;gt; 0)
	tmp[0] := dst[63]
	dst := (dst &amp;lt;&amp;lt; 1) OR tmp[0]
	count := count - 1
OD</oper>
</intrinsic>
<intrinsic>
<id>3195</id>
<name>_LROTR</name>
<cpuid></cpuid>
<ret>unsigned long</ret>
<sign>UNSIGNED_LONG a,INT shift</sign>
<instr>ROR</instr>
<desc>Shift the bits of unsigned 64-bit integer a right by the number of bits specified in shift, rotating the least-significant bit to the most-significant bit location, and store the unsigned result in dst.</desc>
<oper>dst := a
count := shift BITWISE AND 63
DO WHILE (count &amp;gt; 0)
	tmp[63] := dst[0]
	dst := (dst &amp;gt;&amp;gt; 1) OR tmp[63]
	count := count - 1
OD</oper>
</intrinsic>
<intrinsic>
<id>3214</id>
<name>_LZCNT_U32</name>
<cpuid>LZCNT</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>LZCNT</instr>
<desc>Count the number of leading zero bits in unsigned 32-bit integer a, and return that count in dst.</desc>
<oper>tmp := 31
dst := 0
DO WHILE (tmp &amp;gt;= 0 AND a[tmp] = 0)
	tmp := tmp - 1
	dst := dst + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>3215</id>
<name>_LZCNT_U64</name>
<cpuid>LZCNT</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>LZCNT</instr>
<desc>Count the number of leading zero bits in unsigned 64-bit integer a, and return that count in dst.</desc>
<oper>tmp := 63
dst := 0
DO WHILE (tmp &amp;gt;= 0 AND a[tmp] = 0)
	tmp := tmp - 1
	dst := dst + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>2086</id>
<name>_M_EMPTY</name>
<cpuid>MMX</cpuid>
<ret>void</ret>
<sign></sign>
<instr>EMMS</instr>
<desc>Empty the MMX state, which marks the x87 FPU registers as available for use by x87 instructions. This instruction must be used at the end of all MMX technology procedures.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>2636</id>
<name>_M_FROM_INT</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>INT a</sign>
<instr>MOVD</instr>
<desc>Copy 32-bit integer a to the lower elements of dst, and zero the upper element of dst.</desc>
<oper>dst[31:0] := a[31:0]
dst[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>2637</id>
<name>_M_FROM_INT64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__INT64 a</sign>
<instr>MOVQ</instr>
<desc>Copy 64-bit integer a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>3267</id>
<name>_M_MASKMOVQ</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>__M64 a,__M64 mask,CHAR_PTR mem_addr</sign>
<instr>MASKMOVQ</instr>
<desc>Conditionally store 8-bit integer elements from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element).</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF mask[i+7]
		MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3799</id>
<name>_M_PACKSSDW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
dst[47:32] := Saturate_Int32_To_Int16 (b[31:0])
dst[63:48] := Saturate_Int32_To_Int16 (b[63:32])</oper>
</intrinsic>
<intrinsic>
<id>3800</id>
<name>_M_PACKSSWB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst.
	</desc>
<oper>dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
dst[39:32] := Saturate_Int16_To_Int8 (b[15:0])
dst[47:40] := Saturate_Int16_To_Int8 (b[31:16])
dst[55:48] := Saturate_Int16_To_Int8 (b[47:32])
dst[63:56] := Saturate_Int16_To_Int8 (b[63:48])</oper>
</intrinsic>
<intrinsic>
<id>3835</id>
<name>_M_PACKUSWB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
dst[39:32] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
dst[47:40] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
dst[55:48] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
dst[63:56] := Saturate_Int16_To_UnsignedInt8 (b[63:48])</oper>
</intrinsic>
<intrinsic>
<id>3836</id>
<name>_M_PADDB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := a[i+7:i] + b[i+7:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3837</id>
<name>_M_PADDD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3838</id>
<name>_M_PADDSB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3839</id>
<name>_M_PADDSW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3840</id>
<name>_M_PADDUSB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3841</id>
<name>_M_PADDUSW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3842</id>
<name>_M_PADDW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := a[i+15:i] + b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3843</id>
<name>_M_PAND</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PAND</instr>
<desc>Compute the bitwise AND of 64 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[63:0] := (a[63:0] AND b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>3844</id>
<name>_M_PANDN</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PANDN</instr>
<desc>Compute the bitwise NOT of 64 bits (representing integer data) in a and then AND with b, and store the result in dst.</desc>
<oper>dst[63:0] := ((NOT a[63:0]) AND b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>3846</id>
<name>_M_PAVGB</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3847</id>
<name>_M_PAVGW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3848</id>
<name>_M_PCMPEQB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPEQB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3849</id>
<name>_M_PCMPEQD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPEQD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3850</id>
<name>_M_PCMPEQW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPEQW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3851</id>
<name>_M_PCMPGTB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPGTB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3852</id>
<name>_M_PCMPGTD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3853</id>
<name>_M_PCMPGTW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPGTW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4034</id>
<name>_M_PEXTRW</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PEXTRW</instr>
<desc>Extract a 16-bit integer from a, selected with imm8, and store the result in the lower element of dst.</desc>
<oper>dst[15:0] := (a[63:0] &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
dst[31:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>4035</id>
<name>_M_PINSRW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT i,INT imm8</sign>
<instr>PINSRW</instr>
<desc>Copy a to dst, and insert the 16-bit integer i into dst at the location specified by imm8. </desc>
<oper>dst[63:0] := a[63:0]
sel := imm8[1:0]*16
dst[sel+15:sel] := i[15:0]</oper>
</intrinsic>
<intrinsic>
<id>4036</id>
<name>_M_PMADDWD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4037</id>
<name>_M_PMAXSW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4038</id>
<name>_M_PMAXUB</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4039</id>
<name>_M_PMINSW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4040</id>
<name>_M_PMINUB</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4041</id>
<name>_M_PMOVMSKB</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M64 a</sign>
<instr>PMOVMSKB</instr>
<desc>Create mask from the most significant bit of each 8-bit element in a, and store the result in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[j] := a[i+7]
ENDFOR
dst[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>4042</id>
<name>_M_PMULHUW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4043</id>
<name>_M_PMULHW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4044</id>
<name>_M_PMULLW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[15:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4049</id>
<name>_M_POR</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>POR</instr>
<desc>Compute the bitwise OR of 64 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[63:0] := (a[63:0] OR b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>4081</id>
<name>_M_PSADBW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSADBW</instr>
<desc>Compute the absolute differences of packed unsigned 8-bit integers in a and b, then horizontally sum each consecutive 8 differences to produce four unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low 16 bits of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
ENDFOR

dst[15:0] := tmp[7:0] + tmp[15:8] + tmp[23:16] + tmp[31:24] + tmp[39:32] + tmp[47:40] + tmp[55:48] + tmp[63:56]
dst[63:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>4082</id>
<name>_M_PSHUFW</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSHUFW</instr>
<desc>Shuffle 16-bit integers in a using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[15:0] := src[15:0]
	1:	tmp[15:0] := src[31:16]
	2:	tmp[15:0] := src[47:32]
	3:	tmp[15:0] := src[63:48]
	ESAC
	RETURN tmp[15:0]
}

dst[15:0] := SELECT4(a[63:0], imm8[1:0])
dst[31:16] := SELECT4(a[63:0], imm8[3:2])
dst[47:32] := SELECT4(a[63:0], imm8[5:4])
dst[63:48] := SELECT4(a[63:0], imm8[7:6])</oper>
</intrinsic>
<intrinsic>
<id>4083</id>
<name>_M_PSLLD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4084</id>
<name>_M_PSLLDI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4085</id>
<name>_M_PSLLQ</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSLLQ</instr>
<desc>Shift 64-bit integer a left by count while shifting in zeros, and store the result in dst. </desc>
<oper>IF count[63:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;lt;&amp;lt; count[63:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>4086</id>
<name>_M_PSLLQI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSLLQ</instr>
<desc>Shift 64-bit integer a left by imm8 while shifting in zeros, and store the result in dst. </desc>
<oper>IF imm8[7:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;lt;&amp;lt; imm8[7:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>4087</id>
<name>_M_PSLLW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4088</id>
<name>_M_PSLLWI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4089</id>
<name>_M_PSRAD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4090</id>
<name>_M_PSRADI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4091</id>
<name>_M_PSRAW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4092</id>
<name>_M_PSRAWI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4093</id>
<name>_M_PSRLD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4094</id>
<name>_M_PSRLDI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4095</id>
<name>_M_PSRLQ</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRLQ</instr>
<desc>Shift 64-bit integer a right by count while shifting in zeros, and store the result in dst. </desc>
<oper>IF count[63:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;gt;&amp;gt; count[63:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>4096</id>
<name>_M_PSRLQI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRLQ</instr>
<desc>Shift 64-bit integer a right by imm8 while shifting in zeros, and store the result in dst. </desc>
<oper>IF imm8[7:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;gt;&amp;gt; imm8[7:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>4097</id>
<name>_M_PSRLW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4098</id>
<name>_M_PSRLWI</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4099</id>
<name>_M_PSUBB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := a[i+7:i] - b[i+7:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4100</id>
<name>_M_PSUBD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4101</id>
<name>_M_PSUBSB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4102</id>
<name>_M_PSUBSW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4103</id>
<name>_M_PSUBUSB</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4104</id>
<name>_M_PSUBUSW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4105</id>
<name>_M_PSUBW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := a[i+15:i] - b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4106</id>
<name>_M_PUNPCKHBW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[63:0], src2[63:0]){
	dst[7:0] := src1[39:32]
	dst[15:8] := src2[39:32] 
	dst[23:16] := src1[47:40]
	dst[31:24] := src2[47:40]
	dst[39:32] := src1[55:48]
	dst[47:40] := src2[55:48]
	dst[55:48] := src1[63:56]
	dst[63:56] := src2[63:56]
	RETURN dst[63:0]
}	
	
dst[63:0] := INTERLEAVE_HIGH_BYTES(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>4107</id>
<name>_M_PUNPCKHDQ</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of a and b, and store the results in dst.</desc>
<oper>dst[31:0] := a[63:32]
dst[63:32] := b[63:32]</oper>
</intrinsic>
<intrinsic>
<id>4108</id>
<name>_M_PUNPCKHWD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLBW</instr>
<desc>Unpack and interleave 16-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[63:0], src2[63:0]){
	dst[15:0] := src1[47:32]
	dst[31:16] := src2[47:32]
	dst[47:32] := src1[63:48]
	dst[63:48] := src2[63:48]
	RETURN dst[63:0]
}

dst[63:0] := INTERLEAVE_HIGH_WORDS(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>4109</id>
<name>_M_PUNPCKLBW</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_BYTES(src1[63:0], src2[63:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	RETURN dst[63:0]
}	

dst[63:0] := INTERLEAVE_BYTES(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>4110</id>
<name>_M_PUNPCKLDQ</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>dst[31:0] := a[31:0]
dst[63:32] := b[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4111</id>
<name>_M_PUNPCKLWD</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_WORDS(src1[63:0], src2[63:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	RETURN dst[63:0]
}	

dst[63:0] := INTERLEAVE_WORDS(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>4112</id>
<name>_M_PXOR</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PXOR</instr>
<desc>Compute the bitwise OR of 64 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[63:0] := (a[63:0] XOR b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>5475</id>
<name>_M_TO_INT</name>
<cpuid>MMX</cpuid>
<ret>int</ret>
<sign>__M64 a</sign>
<instr>MOVD</instr>
<desc>Copy the lower 32-bit integer in a to dst.</desc>
<oper>dst[31:0] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5476</id>
<name>_M_TO_INT64</name>
<cpuid>MMX</cpuid>
<ret>__int64</ret>
<sign>__M64 a</sign>
<instr>MOVQ</instr>
<desc>Copy 64-bit integer a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>3388</id>
<name>_MAY_I_USE_CPU_FEATURE</name>
<cpuid>SVML</cpuid>
<ret>int</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>NONE</instr>
<desc>Dynamically query the processor to determine if the processor-specific feature(s) specified in a are available, and return true or false (1 or 0) if the set of features is available. Multiple features may be OR'd together. This intrinsic does not check the processor vendor. See the valid feature flags below:</desc>
<oper>_FEATURE_GENERIC_IA32
_FEATURE_FPU
_FEATURE_CMOV
_FEATURE_MMX
_FEATURE_FXSAVE
_FEATURE_SSE
_FEATURE_SSE2
_FEATURE_SSE3
_FEATURE_SSSE3
_FEATURE_SSE4_1
_FEATURE_SSE4_2
_FEATURE_MOVBE
_FEATURE_POPCNT
_FEATURE_PCLMULQDQ
_FEATURE_AES
_FEATURE_F16C
_FEATURE_AVX
_FEATURE_RDRND
_FEATURE_FMA
_FEATURE_BMI
_FEATURE_LZCNT
_FEATURE_HLE
_FEATURE_RTM
_FEATURE_AVX2
_FEATURE_KNCNI
_FEATURE_AVX512F
_FEATURE_ADX
_FEATURE_RDSEED
_FEATURE_AVX512ER
_FEATURE_AVX512PF
_FEATURE_AVX512CD
_FEATURE_SHA
_FEATURE_MPX</oper>
</intrinsic>
<intrinsic>
<id>0</id>
<name>_MM_ABS_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := ABS(a[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>9</id>
<name>_MM_ABS_EPI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ABS(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>18</id>
<name>_MM_ABS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ABS(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>27</id>
<name>_MM_ABS_EPI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := ABS(a[i+7:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>38</id>
<name>_MM_ABS_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a</sign>
<instr>PABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := ABS(a[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>39</id>
<name>_MM_ABS_PI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a</sign>
<instr>PABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := ABS(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>40</id>
<name>_MM_ABS_PI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a</sign>
<instr>PABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := ABS(a[i+7:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>43</id>
<name>_MM_ACOS_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ACOS(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>47</id>
<name>_MM_ACOS_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ACOS(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>51</id>
<name>_MM_ACOSH_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ACOSH(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>55</id>
<name>_MM_ACOSH_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ACOSH(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>61</id>
<name>_MM_ADD_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := a[i+15:i] + b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>70</id>
<name>_MM_ADD_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>79</id>
<name>_MM_ADD_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>88</id>
<name>_MM_ADD_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := a[i+7:i] + b[i+7:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>97</id>
<name>_MM_ADD_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>106</id>
<name>_MM_ADD_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := a[i+15:i] + b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>107</id>
<name>_MM_ADD_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>108</id>
<name>_MM_ADD_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := a[i+7:i] + b[i+7:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>109</id>
<name>_MM_ADD_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>124</id>
<name>_MM_ADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VADDSD</instr>
<desc>Add the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := a[63:0] + b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>127</id>
<name>_MM_ADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VADDSS</instr>
<desc>Add the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>dst[31:0] := a[31:0] + b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>130</id>
<name>_MM_ADD_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ADDSD</instr>
<desc>Add the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := a[63:0] + b[63:0]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>133</id>
<name>_MM_ADD_SI64</name>
<cpuid>SSE2</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDQ</instr>
<desc>Add 64-bit integers a and b, and store the result in dst.</desc>
<oper>dst[63:0] := a[63:0] + b[63:0]</oper>
</intrinsic>
<intrinsic>
<id>134</id>
<name>_MM_ADD_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ADDSS</instr>
<desc>Add the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. </desc>
<oper>dst[31:0] := a[31:0] + b[31:0]
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>149</id>
<name>_MM_ADDS_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>158</id>
<name>_MM_ADDS_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>167</id>
<name>_MM_ADDS_EPU16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>176</id>
<name>_MM_ADDS_EPU8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>185</id>
<name>_MM_ADDS_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>186</id>
<name>_MM_ADDS_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>187</id>
<name>_MM_ADDS_PU16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>188</id>
<name>_MM_ADDS_PU8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>197</id>
<name>_MM_ADDSUB_PD</name>
<cpuid>SSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ADDSUBPD</instr>
<desc>Alternatively add and subtract packed double-precision (64-bit) floating-point elements in a to/from packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF (j is even) 
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>199</id>
<name>_MM_ADDSUB_PS</name>
<cpuid>SSE3</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ADDSUBPS</instr>
<desc>Alternatively add and subtract packed single-precision (32-bit) floating-point elements in a to/from packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF (j is even) 
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>201</id>
<name>_MM_AESDEC_SI128</name>
<cpuid>AES</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I RoundKey</sign>
<instr>AESDEC</instr>
<desc>Perform one round of an AES decryption flow on data (state) in a using the round key in RoundKey, and store the result in dst."</desc>
<oper>state := a
a[127:0] := InvShiftRows(a[127:0])
a[127:0] := InvSubBytes(a[127:0])
a[127:0] := InvMixColumns(a[127:0])
dst[127:0] := a[127:0] XOR RoundKey[127:0]</oper>
</intrinsic>
<intrinsic>
<id>202</id>
<name>_MM_AESDECLAST_SI128</name>
<cpuid>AES</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I RoundKey</sign>
<instr>AESDECLAST</instr>
<desc>Perform the last round of an AES decryption flow on data (state) in a using the round key in RoundKey, and store the result in dst."</desc>
<oper>state := a
a[127:0] := InvShiftRows(a[127:0])
a[127:0] := InvSubBytes(a[127:0])
dst[127:0] := a[127:0] XOR RoundKey[127:0]</oper>
</intrinsic>
<intrinsic>
<id>203</id>
<name>_MM_AESENC_SI128</name>
<cpuid>AES</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I RoundKey</sign>
<instr>AESENC</instr>
<desc>Perform one round of an AES encryption flow on data (state) in a using the round key in RoundKey, and store the result in dst."</desc>
<oper>state := a
a[127:0] := ShiftRows(a[127:0])
a[127:0] := SubBytes(a[127:0])
a[127:0] := MixColumns(a[127:0])
dst[127:0] := a[127:0] XOR RoundKey[127:0]</oper>
</intrinsic>
<intrinsic>
<id>204</id>
<name>_MM_AESENCLAST_SI128</name>
<cpuid>AES</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I RoundKey</sign>
<instr>AESENCLAST</instr>
<desc>Perform the last round of an AES encryption flow on data (state) in a using the round key in RoundKey, and store the result in dst."</desc>
<oper>state := a
a[127:0] := ShiftRows(a[127:0])
a[127:0] := SubBytes(a[127:0])
dst[127:0] := a[127:0] XOR RoundKey[127:0]</oper>
</intrinsic>
<intrinsic>
<id>205</id>
<name>_MM_AESIMC_SI128</name>
<cpuid>AES</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>AESIMC</instr>
<desc>Perform the InvMixColumns transformation on a and store the result in dst.</desc>
<oper>dst[127:0] := InvMixColumns(a[127:0])</oper>
</intrinsic>
<intrinsic>
<id>206</id>
<name>_MM_AESKEYGENASSIST_SI128</name>
<cpuid>AES</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>AESKEYGENASSIST</instr>
<desc>Assist in expanding the AES cipher key by computing steps towards generating a round key for encryption cipher using data from a and an 8-bit round constant specified in imm8, and store the result in dst."
	</desc>
<oper>X3[31:0] := a[127:96]
X2[31:0] := a[95:64]
X1[31:0] := a[63:32]
X0[31:0] := a[31:0]
RCON[31:0] := ZeroExtend(imm8[7:0]);
dst[31:0] := SubWord(X1)
dst[63:32] := (RotWord(SubWord(X1)) XOR RCON;
dst[95:64] := SubWord(X3)
dst[127:96] := RotWord(SubWord(X3)) XOR RCON;</oper>
</intrinsic>
<intrinsic>
<id>207</id>
<name>_MM_ALIGNR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 32-byte immediate result, shift the result right by count 32-bit elements, and store the low 16 bytes (4 elements) in dst.</desc>
<oper>temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &amp;gt;&amp;gt; (32*count)
dst[127:0] := temp[127:0]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>216</id>
<name>_MM_ALIGNR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 32-byte immediate result, shift the result right by count 64-bit elements, and store the low 16 bytes (2 elements) in dst.</desc>
<oper>temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &amp;gt;&amp;gt; (64*count)
dst[127:0] := temp[127:0]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>225</id>
<name>_MM_ALIGNR_EPI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,INT count</sign>
<instr>PALIGNR</instr>
<desc>Concatenate 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst. </desc>
<oper>tmp[255:0] := ((a[127:0] &amp;lt;&amp;lt; 128) OR b[127:0]) &amp;gt;&amp;gt; (count[7:0]*8)
dst[127:0] := tmp[127:0]</oper>
</intrinsic>
<intrinsic>
<id>234</id>
<name>_MM_ALIGNR_PI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b,INT count</sign>
<instr>PALIGNR</instr>
<desc>Concatenate 8-byte blocks in a and b into a 16-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst. </desc>
<oper>tmp[127:0] := ((a[63:0] &amp;lt;&amp;lt; 64) OR b[63:0]) &amp;gt;&amp;gt; (count[7:0]*8)
dst[63:0] := tmp[63:0]</oper>
</intrinsic>
<intrinsic>
<id>250</id>
<name>_MM_AND_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>259</id>
<name>_MM_AND_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>268</id>
<name>_MM_AND_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PAND</instr>
<desc>Compute the bitwise AND of 128 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[127:0] := (a[127:0] AND b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>271</id>
<name>_MM_AND_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PAND</instr>
<desc>Compute the bitwise AND of 64 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[63:0] := (a[63:0] AND b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>286</id>
<name>_MM_ANDNOT_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>295</id>
<name>_MM_ANDNOT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>304</id>
<name>_MM_ANDNOT_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PANDN</instr>
<desc>Compute the bitwise NOT of 128 bits (representing integer data) in a and then AND with b, and store the result in dst.</desc>
<oper>dst[127:0] := ((NOT a[127:0]) AND b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>307</id>
<name>_MM_ANDNOT_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PANDN</instr>
<desc>Compute the bitwise NOT of 64 bits (representing integer data) in a and then AND with b, and store the result in dst.</desc>
<oper>dst[63:0] := ((NOT a[63:0]) AND b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>308</id>
<name>_MM_ASIN_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ASIN(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>312</id>
<name>_MM_ASIN_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ASIN(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>316</id>
<name>_MM_ASINH_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ASINH(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>320</id>
<name>_MM_ASINH_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ASINH(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>324</id>
<name>_MM_ATAN_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ATAN(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>328</id>
<name>_MM_ATAN_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ATAN(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>332</id>
<name>_MM_ATAN2_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>336</id>
<name>_MM_ATAN2_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>340</id>
<name>_MM_ATANH_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ATANH(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>344</id>
<name>_MM_ATANH_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ATANH(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>348</id>
<name>_MM_AVG_EPU16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>357</id>
<name>_MM_AVG_EPU8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>366</id>
<name>_MM_AVG_PU16</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>367</id>
<name>_MM_AVG_PU8</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>384</id>
<name>_MM_BLEND_EPI16</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PBLENDW</instr>
<desc>Blend packed 16-bit integers from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF imm8[j%8]
		dst[i+15:i] := b[i+15:i]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>389</id>
<name>_MM_BLEND_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPBLENDD</instr>
<desc>Blend packed 32-bit integers from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF imm8[j%8]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>400</id>
<name>_MM_BLEND_PD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>BLENDPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF imm8[j%8]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>405</id>
<name>_MM_BLEND_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>BLENDPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF imm8[j%8]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>410</id>
<name>_MM_BLENDV_EPI8</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,__M128I mask</sign>
<instr>PBLENDVB</instr>
<desc>Blend packed 8-bit integers from a and b using mask, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF mask[i+7]
		dst[i+7:i] := b[i+7:i]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>412</id>
<name>_MM_BLENDV_PD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D mask</sign>
<instr>BLENDVPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using mask, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>414</id>
<name>_MM_BLENDV_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 mask</sign>
<instr>BLENDVPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using mask, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>456</id>
<name>_MM_BROADCAST_I32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of "dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	n := (j mod 2)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>486</id>
<name>_MM_BROADCAST_SS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast a single-precision (32-bit) floating-point element from memory to all elements of dst.</desc>
<oper>tmp[31:0] = MEM[mem_addr+31:mem_addr]
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := tmp[31:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>488</id>
<name>_MM_BROADCASTB_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>497</id>
<name>_MM_BROADCASTD_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>506</id>
<name>_MM_BROADCASTMB_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k</sign>
<instr>VPBROADCASTMB2Q</instr>
<desc>Broadcast the low 8-bits from input mask k to all 64-bit elements of dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ZeroExtend(k[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>509</id>
<name>_MM_BROADCASTMW_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k</sign>
<instr>VPBROADCASTMW2D</instr>
<desc>Broadcast the low 16-bits from input mask k to all 32-bit elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ZeroExtend(k[15:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>512</id>
<name>_MM_BROADCASTQ_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>521</id>
<name>_MM_BROADCASTSD_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>MOVDDUP</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>529</id>
<name>_MM_BROADCASTSS_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>538</id>
<name>_MM_BROADCASTW_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>549</id>
<name>_MM_BSLLI_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSLLDQ</instr>
<desc>Shift a left by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;lt;&amp;lt; (tmp*8)</oper>
</intrinsic>
<intrinsic>
<id>552</id>
<name>_MM_BSRLI_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRLDQ</instr>
<desc>Shift a right by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;gt;&amp;gt; (tmp*8)</oper>
</intrinsic>
<intrinsic>
<id>559</id>
<name>_MM_CASTPD_PS</name>
<cpuid>SSE2</cpuid>
<ret>__m128</ret>
<sign>__M128D a</sign>
<instr></instr>
<desc>Cast vector of type __m128d to type __m128. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>562</id>
<name>_MM_CASTPD_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr></instr>
<desc>Cast vector of type __m128d to type __m128i. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>571</id>
<name>_MM_CASTPS_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr></instr>
<desc>Cast vector of type __m128 to type __m128d. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>574</id>
<name>_MM_CASTPS_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr></instr>
<desc>Cast vector of type __m128 to type __m128i. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>583</id>
<name>_MM_CASTSI128_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr></instr>
<desc>Cast vector of type __m128i to type __m128d. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>584</id>
<name>_MM_CASTSI128_PS</name>
<cpuid>SSE2</cpuid>
<ret>__m128</ret>
<sign>__M128I a</sign>
<instr></instr>
<desc>Cast vector of type __m128i to type __m128. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>597</id>
<name>_MM_CBRT_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := CubeRoot(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>601</id>
<name>_MM_CBRT_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := CubeRoot(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>605</id>
<name>_MM_CDFNORM_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := CDFNormal(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>609</id>
<name>_MM_CDFNORM_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := CDFNormal(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>613</id>
<name>_MM_CDFNORMINV_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := InverseCDFNormal(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>617</id>
<name>_MM_CDFNORMINV_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := InverseCDFNormal(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>621</id>
<name>_MM_CEIL_PD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>ROUNDPD</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a up to an integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := CEIL(a[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>625</id>
<name>_MM_CEIL_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>ROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a up to an integer value, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := CEIL(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>629</id>
<name>_MM_CEIL_SD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ROUNDSD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in b up to an integer value, store the result as a double-precision floating-point element in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := CEIL(b[63:0])
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>630</id>
<name>_MM_CEIL_SS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ROUNDSS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in b up to an integer value, store the result as a single-precision floating-point element in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := CEIL(b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>631</id>
<name>_MM_CEXP_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed complex single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := e^(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>633</id>
<name>_MM_CLEVICT</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR ptr,INT level</sign>
<instr>CLEVICT0, CLEVICT1</instr>
<desc>Evicts the cache line containing the address ptr from cache level level (can be either 0 or 1).</desc>
<oper>CacheLineEvict(ptr, level)</oper>
</intrinsic>
<intrinsic>
<id>634</id>
<name>_MM_CLFLUSH</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>VOID_CONST_PTR p</sign>
<instr>CLFLUSH</instr>
<desc>Invalidate and flush the cache line that contains p from all levels of the cache hierarchy.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>635</id>
<name>_MM_CLFLUSHOPT</name>
<cpuid>CLFLUSHOPT</cpuid>
<ret>void</ret>
<sign>VOID_CONST_PTR p</sign>
<instr>CLFLUSHOPT</instr>
<desc>Invalidate and flush the cache line that contains p from all levels of the cache hierarchy.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>636</id>
<name>_MM_CLMULEPI64_SI128</name>
<cpuid>PCLMULQDQ</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCLMULQDQ</instr>
<desc>Perform a carry-less multiplication of two 64-bit integers, selected from a and b according to imm8, and store the results in dst.
	</desc>
<oper>IF (imm8[0] = 0)
	TEMP1 := a[63:0];
ELSE
	TEMP1 := a[127:64];
FI 
IF (imm8[4] = 0)
	TEMP2 := b[63:0];
ELSE 
	TEMP2 := b[127:64];
FI

FOR i := 0 to 63
	TEMP[i] := (TEMP1[0] and TEMP2[i]);
	FOR j := 1 to i
		TEMP [i] := TEMP [i] XOR (TEMP1[j] AND TEMP2[i-j])
	ENDFOR 
	dst[i] := TEMP[i];
ENDFOR
FOR i := 64 to 127
	TEMP [i] := 0;
	FOR j := (i - 63) to 63
		TEMP [i] := TEMP [i] XOR (TEMP1[j] AND TEMP2[i-j])
	ENDFOR
	dst[i] := TEMP[i];
ENDFOR
dst[127] := 0</oper>
</intrinsic>
<intrinsic>
<id>637</id>
<name>_MM_CLOG_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed complex single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ln(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>639</id>
<name>_MM_CMP_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>645</id>
<name>_MM_CMP_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>651</id>
<name>_MM_CMP_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>657</id>
<name>_MM_CMP_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>663</id>
<name>_MM_CMP_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>669</id>
<name>_MM_CMP_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>675</id>
<name>_MM_CMP_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>681</id>
<name>_MM_CMP_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>687</id>
<name>_MM_CMP_PD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in dst.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ( a[i+63:i] OP b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>689</id>
<name>_MM_CMP_PD_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>695</id>
<name>_MM_CMP_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in dst.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] OP b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>697</id>
<name>_MM_CMP_PS_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>707</id>
<name>_MM_CMP_ROUND_SD_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0

k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>709</id>
<name>_MM_CMP_ROUND_SS_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0

k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>711</id>
<name>_MM_CMP_SD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VCMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b based on the comparison operand specified by imm8, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

dst[63:0] := ( a[63:0] OP b[63:0] ) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>712</id>
<name>_MM_CMP_SD_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VCMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0

k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>714</id>
<name>_MM_CMP_SS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VCMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b based on the comparison operand specified by imm8, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

dst[31:0] := ( a[31:0] OP b[31:0] ) ? 0xFFFFFFFF : 0
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>715</id>
<name>_MM_CMP_SS_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VCMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0

k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>717</id>
<name>_MM_CMPEQ_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPEQW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>719</id>
<name>_MM_CMPEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>725</id>
<name>_MM_CMPEQ_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPEQD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>727</id>
<name>_MM_CMPEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>733</id>
<name>_MM_CMPEQ_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPEQQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ( a[i+63:i] == b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>735</id>
<name>_MM_CMPEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>741</id>
<name>_MM_CMPEQ_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPEQB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>743</id>
<name>_MM_CMPEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>749</id>
<name>_MM_CMPEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>755</id>
<name>_MM_CMPEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>761</id>
<name>_MM_CMPEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>767</id>
<name>_MM_CMPEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>773</id>
<name>_MM_CMPEQ_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] == b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>776</id>
<name>_MM_CMPEQ_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPEQW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>777</id>
<name>_MM_CMPEQ_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPEQD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>778</id>
<name>_MM_CMPEQ_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPEQB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>779</id>
<name>_MM_CMPEQ_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>782</id>
<name>_MM_CMPEQ_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for equality, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] == b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>783</id>
<name>_MM_CMPEQ_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for equality, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] == b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>784</id>
<name>_MM_CMPESTRA</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRI</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and returns 1 if b did not contain a null character and the resulting mask was zero, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF i == la
			aInvalid := 1
		FI
		IF j == lb
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
			0:  // equal any
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				FI
			1:  // ranges
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				FI
			2:  // equal each
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 1
				FI
			3:  // equal ordered
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 1
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 1
				FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
	0:  // equal any
		IntRes1 := 0
		FOR i := 0 to UpperBound
			FOR j := 0 to UpperBound
				IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
			ENDFOR
		ENDFOR
	1:  // ranges
		IntRes1 := 0
		FOR i := 0 to UpperBound
			FOR j := 0 to UpperBound, j += 2
				IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
			ENDFOR
		ENDFOR
	2:  // equal each
		IntRes1 := 0
		FOR i := 0 to UpperBound
			IntRes1[i] := BoolRes[i][i]
		ENDFOR
	3:  // equal ordered
		IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
		FOR i := 0 to UpperBound
			k := i
			FOR j := 0 to UpperBound-i
				IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
				k++
			ENDFOR
		ENDFOR
ESAC

// optionally negate results
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF i &amp;gt;= lb // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
dst := (IntRes2 == 0) AND (lb &amp;gt; UpperBound)</oper>
</intrinsic>
<intrinsic>
<id>785</id>
<name>_MM_CMPESTRC</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRI</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and returns 1 if the resulting mask was non-zero, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF i == la
			aInvalid := 1
		FI
		IF j == lb
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
			0:  // equal any
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				FI
			1:  // ranges
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				FI
			2:  // equal each
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 1
				FI
			3:  // equal ordered
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 1
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 1
				FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
	0:  // equal any
		IntRes1 := 0
		FOR i := 0 to UpperBound
			FOR j := 0 to UpperBound
				IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
			ENDFOR
		ENDFOR
	1:  // ranges
		IntRes1 := 0
		FOR i := 0 to UpperBound
			FOR j := 0 to UpperBound, j += 2
				IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
			ENDFOR
		ENDFOR
	2:  // equal each
		IntRes1 := 0
		FOR i := 0 to UpperBound
			IntRes1[i] := BoolRes[i][i]
		ENDFOR
	3:  // equal ordered
		IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
		FOR i := 0 to UpperBound
			k := i
			FOR j := 0 to UpperBound-i
				IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
				k++
			ENDFOR
		ENDFOR
ESAC

// optionally negate results
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF i &amp;gt;= lb // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
dst := (IntRes2 != 0)</oper>
</intrinsic>
<intrinsic>
<id>786</id>
<name>_MM_CMPESTRI</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRI</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and store the generated index in dst.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF i == la
			aInvalid := 1
		FI
		IF j == lb
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF i &amp;gt;= lb // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
IF imm8[6] // most significant bit
	tmp := UpperBound
	dst := tmp
	DO WHILE ((tmp &amp;gt;= 0) AND a[tmp] = 0)
		tmp := tmp - 1
		dst := tmp
	OD
ELSE // least significant bit
	tmp := 0
	dst := tmp
	DO WHILE ((tmp &amp;lt;= UpperBound) AND a[tmp] = 0)
		tmp := tmp + 1
		dst := tmp
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>787</id>
<name>_MM_CMPESTRM</name>
<cpuid>SSE4_2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRM</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and store the generated mask in dst.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF i == la
			aInvalid := 1
		FI
		IF j == lb
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF i &amp;gt;= lb // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
IF imm8[6] // byte / word mask
	FOR i := 0 to UpperBound
		j := i*size
		IF IntRes2[i]
			dst[j+size-1:j] := (imm8[0] ? 0xFF : 0xFFFF)
		ELSE
			dst[j+size-1:j] := 0
		FI
	ENDFOR
ELSE // bit mask
	dst[UpperBound:0] := IntRes[UpperBound:0]
	dst[127:UpperBound+1] := 0
FI</oper>
</intrinsic>
<intrinsic>
<id>788</id>
<name>_MM_CMPESTRO</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRI</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and returns bit 0 of the resulting bit mask.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF i == la
			aInvalid := 1
		FI
		IF j == lb
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
			0:  // equal any
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				FI
			1:  // ranges
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				FI
			2:  // equal each
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 0
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 1
				FI
			3:  // equal ordered
				IF (!aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 0
				ELSE IF (aInvalid &amp;&amp; !bInvalid)
					BoolRes[i][j] := 1
				ELSE If (aInvalid &amp;&amp; bInvalid)
					BoolRes[i][j] := 1
				FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
	0:  // equal any
		IntRes1 := 0
		FOR i := 0 to UpperBound
			FOR j := 0 to UpperBound
				IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
			ENDFOR
		ENDFOR
	1:  // ranges
		IntRes1 := 0
		FOR i := 0 to UpperBound
			FOR j := 0 to UpperBound, j += 2
				IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
			ENDFOR
		ENDFOR
	2:  // equal each
		IntRes1 := 0
		FOR i := 0 to UpperBound
			IntRes1[i] := BoolRes[i][i]
		ENDFOR
	3:  // equal ordered
		IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
		FOR i := 0 to UpperBound
			k := i
			FOR j := 0 to UpperBound-i
				IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
				k++
			ENDFOR
		ENDFOR
ESAC

// optionally negate results
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF i &amp;gt;= lb // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
dst := IntRes2[0</oper>
</intrinsic>
<intrinsic>
<id>789</id>
<name>_MM_CMPESTRS</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRI</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and returns 1 if any character in a was null, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

dst := (la &amp;lt;= UpperBound)</oper>
</intrinsic>
<intrinsic>
<id>790</id>
<name>_MM_CMPESTRZ</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT la,__M128I b,INT lb,CONST_INT imm8</sign>
<instr>PCMPESTRI</instr>
<desc>Compare packed strings in a and b with lengths la and lb using the control in imm8, and returns 1 if any character in b was null, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

dst := (lb &amp;lt;= UpperBound)</oper>
</intrinsic>
<intrinsic>
<id>791</id>
<name>_MM_CMPGE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>797</id>
<name>_MM_CMPGE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>803</id>
<name>_MM_CMPGE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>809</id>
<name>_MM_CMPGE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>815</id>
<name>_MM_CMPGE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>821</id>
<name>_MM_CMPGE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>827</id>
<name>_MM_CMPGE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>833</id>
<name>_MM_CMPGE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>839</id>
<name>_MM_CMPGE_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for greater-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] &amp;gt;= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>840</id>
<name>_MM_CMPGE_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for greater-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>841</id>
<name>_MM_CMPGE_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for greater-than-or-equal, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] &amp;gt;= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>842</id>
<name>_MM_CMPGE_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for greater-than-or-equal, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] &amp;gt;= b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>843</id>
<name>_MM_CMPGT_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>845</id>
<name>_MM_CMPGT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>851</id>
<name>_MM_CMPGT_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>853</id>
<name>_MM_CMPGT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>859</id>
<name>_MM_CMPGT_EPI64</name>
<cpuid>SSE4_2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>861</id>
<name>_MM_CMPGT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>867</id>
<name>_MM_CMPGT_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>869</id>
<name>_MM_CMPGT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>875</id>
<name>_MM_CMPGT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>881</id>
<name>_MM_CMPGT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>887</id>
<name>_MM_CMPGT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>893</id>
<name>_MM_CMPGT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>899</id>
<name>_MM_CMPGT_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] &amp;gt; b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>900</id>
<name>_MM_CMPGT_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPGTW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>901</id>
<name>_MM_CMPGT_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>902</id>
<name>_MM_CMPGT_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PCMPGTB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>903</id>
<name>_MM_CMPGT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>904</id>
<name>_MM_CMPGT_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for greater-than, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] &amp;gt; b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>905</id>
<name>_MM_CMPGT_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for greater-than, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] &amp;gt; b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>906</id>
<name>_MM_CMPISTRA</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRI</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and returns 1 if b did not contain a null character and the resulting mask was zero, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF a[m+size-1:m] == 0
			aInvalid := 1
		FI
		IF b[n+size-1:n] == 0
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
bInvalid := 0
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF b[n+size-1:n] == 0
				bInvalid := 1
			FI
			IF bInvalid // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
dst := (IntRes2 == 0) AND bInvalid</oper>
</intrinsic>
<intrinsic>
<id>907</id>
<name>_MM_CMPISTRC</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRI</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and returns 1 if the resulting mask was non-zero, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF a[m+size-1:m] == 0
			aInvalid := 1
		FI
		IF b[n+size-1:n] == 0
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
bInvalid := 0
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF b[n+size-1:n] == 0
				bInvalid := 1
			FI
			IF bInvalid // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
dst := (IntRes2 != 0)</oper>
</intrinsic>
<intrinsic>
<id>908</id>
<name>_MM_CMPISTRI</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRI</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and store the generated index in dst.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF a[m+size-1:m] == 0
			aInvalid := 1
		FI
		IF b[n+size-1:n] == 0
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
bInvalid := 0
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF b[n+size-1:n] == 0
				bInvalid := 1
			FI
			IF bInvalid // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
IF imm8[6] // most significant bit
	tmp := UpperBound
	dst := tmp
	DO WHILE ((tmp &amp;gt;= 0) AND a[tmp] = 0)
		tmp := tmp - 1
		dst := tmp
	OD
ELSE // least significant bit
	tmp := 0
	dst := tmp
	DO WHILE ((tmp &amp;lt;= UpperBound) AND a[tmp] = 0)
		tmp := tmp + 1
		dst := tmp
	OD
FI</oper>
</intrinsic>
<intrinsic>
<id>909</id>
<name>_MM_CMPISTRM</name>
<cpuid>SSE4_2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRM</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and store the generated mask in dst.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF a[m+size-1:m] == 0
			aInvalid := 1
		FI
		IF b[n+size-1:n] == 0
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
bInvalid := 0
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF b[n+size-1:n] == 0
				bInvalid := 1
			FI
			IF bInvalid // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
IF imm8[6] // byte / word mask
	FOR i := 0 to UpperBound
		j := i*size
		IF IntRes2[i]
			dst[j+size-1:j] := (imm8[0] ? 0xFF : 0xFFFF)
		ELSE
			dst[j+size-1:j] := 0
		FI
	ENDFOR
ELSE // bit mask
	dst[UpperBound:0] := IntRes[UpperBound:0]
	dst[127:UpperBound+1] := 0
FI</oper>
</intrinsic>
<intrinsic>
<id>910</id>
<name>_MM_CMPISTRO</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRI</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and returns bit 0 of the resulting bit mask.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

// compare all characters
aInvalid := 0
bInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	FOR j := 0 to UpperBound
		n := j*size
		BoolRes[i][j] := (a[m+size-1:m] == b[n+size-1:n])
		
		// invalidate characters after EOS
		IF a[m+size-1:m] == 0
			aInvalid := 1
		FI
		IF b[n+size-1:n] == 0
			bInvalid := 1
		FI
		
		// override comparisons for invalid characters
		CASE (imm8[3:2]) OF
		0:  // equal any
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		1:  // ranges
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			FI
		2:  // equal each
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 0
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		3:  // equal ordered
			IF (!aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 0
			ELSE IF (aInvalid &amp;&amp; !bInvalid)
				BoolRes[i][j] := 1
			ELSE If (aInvalid &amp;&amp; bInvalid)
				BoolRes[i][j] := 1
			FI
		ESAC
	ENDFOR
ENDFOR

// aggregate results
CASE (imm8[3:2]) OF
0:  // equal any
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound
			IntRes1[i] := IntRes1[i] OR BoolRes[i][j]
		ENDFOR
	ENDFOR
1:  // ranges
	IntRes1 := 0
	FOR i := 0 to UpperBound
		FOR j := 0 to UpperBound, j += 2
			IntRes1[i] := IntRes1[i] OR (BoolRes[i][j] AND BoolRes[i][j+1])
		ENDFOR
	ENDFOR
2:  // equal each
	IntRes1 := 0
	FOR i := 0 to UpperBound
		IntRes1[i] := BoolRes[i][i]
	ENDFOR
3:  // equal ordered
	IntRes1 := (imm8[0] ? 0xFF : 0xFFFF)
	FOR i := 0 to UpperBound
		k := i
		FOR j := 0 to UpperBound-i
			IntRes1[i] := IntRes1[i] AND BoolRes[k][j]
			k++
		ENDFOR
	ENDFOR
ESAC

// optionally negate results
bInvalid := 0
FOR i := 0 to UpperBound
	IF imm8[4]
		IF imm8[5] // only negate valid
			IF b[n+size-1:n] == 0
				bInvalid := 1
			FI
			IF bInvalid // invalid, don't negate
				IntRes2[i] := IntRes1[i]
			ELSE // valid, negate
				IntRes2[i] := -1 XOR IntRes1[i]
			FI
		ELSE // negate all
			IntRes2[i] := -1 XOR IntRes1[i]
		FI
	ELSE // don't negate
		IntRes2[i] := IntRes1[i]
	FI
ENDFOR

// output
dst := IntRes2[0]</oper>
</intrinsic>
<intrinsic>
<id>911</id>
<name>_MM_CMPISTRS</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRI</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and returns 1 if any character in a was null, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

aInvalid := 0
FOR i := 0 to UpperBound
	m := i*size
	IF b[m+size-1:m] == 0
		aInvalid := 1
	FI
ENDFOR

dst := aInvalid</oper>
</intrinsic>
<intrinsic>
<id>912</id>
<name>_MM_CMPISTRZ</name>
<cpuid>SSE4_2</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>PCMPISTRI</instr>
<desc>Compare packed strings with implicit lengths in a and b using the control in imm8, and returns 1 if any character in b was null, and 0 otherwise.
	imm can be a combination of:    _SIDD_UBYTE_OPS                // unsigned 8-bit characters
    _SIDD_UWORD_OPS                // unsigned 16-bit characters
    _SIDD_SBYTE_OPS                // signed 8-bit characters
    _SIDD_SWORD_OPS                // signed 16-bit characters
    _SIDD_CMP_EQUAL_ANY            // compare equal any
    _SIDD_CMP_RANGES               // compare ranges
    _SIDD_CMP_EQUAL_EACH           // compare equal each
    _SIDD_CMP_EQUAL_ORDERED        // compare equal ordered
    _SIDD_NEGATIVE_POLARITY        // negate results
    _SIDD_MASKED_NEGATIVE_POLARITY // negate results only before end of string
    _SIDD_LEAST_SIGNIFICANT        // index only: return last significant bit
    _SIDD_MOST_SIGNIFICANT         // index only: return most significant bit
    _SIDD_BIT_MASK                 // mask only: return bit mask
    _SIDD_UNIT_MASK                // mask only: return byte/word mask
	</desc>
<oper>size := (imm8[0] ? 16 : 8) // 8 or 16-bit characters
UpperBound := (128 / size) - 1

bInvalid := 0
FOR j := 0 to UpperBound
	n := j*size
	IF b[n+size-1:n] == 0
		bInvalid := 1
	FI
ENDFOR

dst := bInvalid</oper>
</intrinsic>
<intrinsic>
<id>913</id>
<name>_MM_CMPLE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>919</id>
<name>_MM_CMPLE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>925</id>
<name>_MM_CMPLE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>931</id>
<name>_MM_CMPLE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>937</id>
<name>_MM_CMPLE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>943</id>
<name>_MM_CMPLE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>949</id>
<name>_MM_CMPLE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>955</id>
<name>_MM_CMPLE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>961</id>
<name>_MM_CMPLE_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for less-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] &amp;lt;= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>964</id>
<name>_MM_CMPLE_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for less-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>967</id>
<name>_MM_CMPLE_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for less-than-or-equal, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] &amp;lt;= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>968</id>
<name>_MM_CMPLE_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for less-than-or-equal, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] &amp;lt;= b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>969</id>
<name>_MM_CMPLT_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in dst. Note: This intrinsic emits the pcmpgtw instruction with the order of the operands switched.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 0xFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>970</id>
<name>_MM_CMPLT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>976</id>
<name>_MM_CMPLT_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in dst. Note: This intrinsic emits the pcmpgtd instruction with the order of the operands switched.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>977</id>
<name>_MM_CMPLT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>985</id>
<name>_MM_CMPLT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>991</id>
<name>_MM_CMPLT_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PCMPGTB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in dst. Note: This intrinsic emits the pcmpgtb instruction with the order of the operands switched.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 0xFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>992</id>
<name>_MM_CMPLT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>998</id>
<name>_MM_CMPLT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1004</id>
<name>_MM_CMPLT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1010</id>
<name>_MM_CMPLT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>1016</id>
<name>_MM_CMPLT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1022</id>
<name>_MM_CMPLT_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for less-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] &amp;lt; b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1025</id>
<name>_MM_CMPLT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for less-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1028</id>
<name>_MM_CMPLT_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for less-than, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] &amp;lt; b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1029</id>
<name>_MM_CMPLT_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for less-than, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] &amp;lt; b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1030</id>
<name>_MM_CMPNEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1036</id>
<name>_MM_CMPNEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1042</id>
<name>_MM_CMPNEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>1048</id>
<name>_MM_CMPNEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1054</id>
<name>_MM_CMPNEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1060</id>
<name>_MM_CMPNEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1066</id>
<name>_MM_CMPNEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>1072</id>
<name>_MM_CMPNEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1078</id>
<name>_MM_CMPNEQ_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] != b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1081</id>
<name>_MM_CMPNEQ_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] != b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1084</id>
<name>_MM_CMPNEQ_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for not-equal, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] != b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1085</id>
<name>_MM_CMPNEQ_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for not-equal, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] != b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1086</id>
<name>_MM_CMPNGE_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-greater-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := !(a[i+63:i] &amp;gt;= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1087</id>
<name>_MM_CMPNGE_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-greater-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := !( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1088</id>
<name>_MM_CMPNGE_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for not-greater-than-or-equal, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := !(a[63:0] &amp;gt;= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1089</id>
<name>_MM_CMPNGE_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for not-greater-than-or-equal, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := !( a[31:0] &amp;gt;= b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1090</id>
<name>_MM_CMPNGT_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := !(a[i+63:i] &amp;gt; b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1091</id>
<name>_MM_CMPNGT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := !( a[i+31:i] &amp;gt; b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1092</id>
<name>_MM_CMPNGT_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for not-greater-than, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := !(a[63:0] &amp;gt; b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1093</id>
<name>_MM_CMPNGT_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for not-greater-than, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := !( a[31:0] &amp;gt; b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1094</id>
<name>_MM_CMPNLE_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-less-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := !(a[i+63:i] &amp;lt;= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1097</id>
<name>_MM_CMPNLE_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-less-than-or-equal, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := !( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1100</id>
<name>_MM_CMPNLE_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for not-less-than-or-equal, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := !(a[63:0] &amp;lt;= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1101</id>
<name>_MM_CMPNLE_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for not-less-than-or-equal, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := !( a[31:0] &amp;lt;= b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1102</id>
<name>_MM_CMPNLT_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-less-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := !(a[i+63:i] &amp;lt; b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1105</id>
<name>_MM_CMPNLT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-less-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := !( a[i+31:i] &amp;lt; b[i+31:i] ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1108</id>
<name>_MM_CMPNLT_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b for not-less-than, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := !(a[63:0] &amp;lt; b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1109</id>
<name>_MM_CMPNLT_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b for not-less-than, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := !( a[31:0] &amp;lt; b[31:0] ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1110</id>
<name>_MM_CMPORD_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b to see if neither is NaN, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] != NaN AND b[i+63:i] != NaN) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1113</id>
<name>_MM_CMPORD_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b to see if neither is NaN, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] != NaN AND b[i+31:i] != NaN ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1116</id>
<name>_MM_CMPORD_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b to see if neither is NaN, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] != NaN AND b[63:0] != NaN) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1117</id>
<name>_MM_CMPORD_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b to see if neither is NaN, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] != NaN AND b[31:0] != NaN ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1118</id>
<name>_MM_CMPUNORD_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b to see if either is NaN, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] == NaN OR b[i+63:i] == NaN) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1121</id>
<name>_MM_CMPUNORD_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b to see if either is NaN, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ( a[i+31:i] == NaN OR b[i+31:i] == NaN ) ? 0xffffffff : 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1124</id>
<name>_MM_CMPUNORD_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>CMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b to see if either is NaN, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] == NaN OR b[63:0] == NaN) ? 0xFFFFFFFFFFFFFFFF : 0
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>1125</id>
<name>_MM_CMPUNORD_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>CMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b to see if either is NaN, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := ( a[31:0] == NaN OR b[31:0] == NaN ) ? 0xffffffff : 0
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1126</id>
<name>_MM_COMI_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b based on the comparison operand specified by imm8, and return the boolean result (0 or 1).
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

RETURN ( a[63:0] OP b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1127</id>
<name>_MM_COMI_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b based on the comparison operand specified by imm8, and return the boolean result (0 or 1).
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

RETURN ( a[31:0] OP b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1128</id>
<name>_MM_COMIEQ_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>COMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for equality, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[63:0] == b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1129</id>
<name>_MM_COMIEQ_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>COMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for equality, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[31:0] == b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1130</id>
<name>_MM_COMIGE_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>COMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for greater-than-or-equal, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[63:0] &amp;gt;= b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1131</id>
<name>_MM_COMIGE_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>COMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for greater-than-or-equal, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[31:0] &amp;gt;= b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1132</id>
<name>_MM_COMIGT_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>COMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for greater-than, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[63:0] &amp;gt; b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1133</id>
<name>_MM_COMIGT_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>COMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for greater-than, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[31:0] &amp;gt; b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1134</id>
<name>_MM_COMILE_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>COMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for less-than-or-equal, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[63:0] &amp;lt;= b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1135</id>
<name>_MM_COMILE_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>COMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for less-than-or-equal, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[31:0] &amp;lt;= b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1136</id>
<name>_MM_COMILT_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>COMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for less-than, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[63:0] &amp;lt; b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1137</id>
<name>_MM_COMILT_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>COMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for less-than, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[31:0] &amp;lt; b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1138</id>
<name>_MM_COMINEQ_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>COMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for not-equal, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[63:0] != b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1139</id>
<name>_MM_COMINEQ_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>COMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for not-equal, and return the boolean result (0 or 1).</desc>
<oper>RETURN ( a[31:0] != b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>1176</id>
<name>_MM_CONFLICT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit. Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	FOR k := 0 to j-1
		m := k*32
		dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
	ENDFOR
	dst[i+31:i+j] := 0
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1185</id>
<name>_MM_CONFLICT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit. Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	FOR k := 0 to j-1
		m := k*64
		dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
	ENDFOR
	dst[i+63:i+j] := 0
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1194</id>
<name>_MM_COS_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := COS(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1198</id>
<name>_MM_COS_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := COS(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1202</id>
<name>_MM_COSD_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := COSD(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1206</id>
<name>_MM_COSD_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := COSD(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1210</id>
<name>_MM_COSH_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := COSH(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1214</id>
<name>_MM_COSH_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := COSH(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1218</id>
<name>_MM_COUNTBITS_32</name>
<cpuid>KNCNI</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT r1</sign>
<instr>POPCNT</instr>
<desc>Counts the number of set bits in 32-bit unsigned integer r1, returning the results in dst.</desc>
<oper>dst[31:0] := PopCount(r1[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1219</id>
<name>_MM_COUNTBITS_64</name>
<cpuid>KNCNI</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 r1</sign>
<instr>POPCNT</instr>
<desc>Counts the number of set bits in double-precision (32-bit) unsigned integer r1, returning the results in dst.</desc>
<oper>dst[63:0] := PopCount(r1[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1220</id>
<name>_MM_CRC32_U16</name>
<cpuid>SSE4_2</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT crc,UNSIGNED_SHORT v</sign>
<instr>CRC32</instr>
<desc>Starting with the initial value in crc, accumulates a CRC32 value for unsigned 16-bit integer v, and stores the result in dst.</desc>
<oper>tmp1[15:0] := v[0:15] // bit reflection
tmp2[31:0] := crc[0:31] // bit reflection
tmp3[47:0] := tmp1[15:0] &amp;lt;&amp;lt; 32
tmp4[47:0] := tmp2[31:0] &amp;lt;&amp;lt; 16
tmp5[47:0] := tmp3[47:0] XOR tmp4[47:0]
tmp6[31:0] := tmp5[47:0] MOD2 0x11EDC6F41
dst[31:0] := tmp6[0:31] // bit reflection</oper>
</intrinsic>
<intrinsic>
<id>1221</id>
<name>_MM_CRC32_U32</name>
<cpuid>SSE4_2</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT crc,UNSIGNED_INT v</sign>
<instr>CRC32</instr>
<desc>Starting with the initial value in crc, accumulates a CRC32 value for unsigned 32-bit integer v, and stores the result in dst.</desc>
<oper>tmp1[31:0] := v[0:31] // bit reflection
tmp2[31:0] := crc[0:31] // bit reflection
tmp3[63:0] := tmp1[31:0] &amp;lt;&amp;lt; 32
tmp4[63:0] := tmp2[31:0] &amp;lt;&amp;lt; 32
tmp5[63:0] := tmp3[63:0] XOR tmp4[63:0]
tmp6[31:0] := tmp5[63:0] MOD2 0x11EDC6F41
dst[31:0] := tmp6[0:31] // bit reflection</oper>
</intrinsic>
<intrinsic>
<id>1222</id>
<name>_MM_CRC32_U64</name>
<cpuid>SSE4_2</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 crc,UNSIGNED__INT64 v</sign>
<instr>CRC32</instr>
<desc>Starting with the initial value in crc, accumulates a CRC32 value for unsigned 64-bit integer v, and stores the result in dst.</desc>
<oper>tmp1[63:0] := v[0:63] // bit reflection
tmp2[31:0] := crc[0:31] // bit reflection
tmp3[95:0] := tmp1[31:0] &amp;lt;&amp;lt; 32
tmp4[95:0] := tmp2[63:0] &amp;lt;&amp;lt; 64
tmp5[95:0] := tmp3[95:0] XOR tmp4[95:0]
tmp6[31:0] := tmp5[95:0] MOD2 0x11EDC6F41
dst[31:0] := tmp6[0:31] // bit reflection</oper>
</intrinsic>
<intrinsic>
<id>1223</id>
<name>_MM_CRC32_U8</name>
<cpuid>SSE4_2</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT crc,UNSIGNED_CHAR v</sign>
<instr>CRC32</instr>
<desc>Starting with the initial value in crc, accumulates a CRC32 value for unsigned 8-bit integer v, and stores the result in dst.</desc>
<oper>tmp1[7:0] := v[0:7] // bit reflection
tmp2[31:0] := crc[0:31] // bit reflection
tmp3[39:0] := tmp1[7:0] &amp;lt;&amp;lt; 32 
tmp4[39:0] := tmp2[31:0] &amp;lt;&amp;lt; 8
tmp5[39:0] := tmp3[39:0] XOR tmp4[39:0]
tmp6[31:0] := tmp5[39:0] MOD2 0x11EDC6F41
dst[31:0] := tmp6[0:31] // bit reflection</oper>
</intrinsic>
<intrinsic>
<id>1224</id>
<name>_MM_CSQRT_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the square root of packed complex single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1226</id>
<name>_MM_CVT_PI2PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M64 b</sign>
<instr>CVTPI2PS</instr>
<desc>Convert packed 32-bit integers in b to packed single-precision (32-bit) floating-point elements, store the results in the lower 2 elements of dst, and copy the upper 2 packed elements from a to the upper elements of dst. </desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[63:32] := Convert_Int32_To_FP32(b[63:32])
dst[95:64] := a[95:64]
dst[127:96] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>1227</id>
<name>_MM_CVT_PS2PI</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M128 a</sign>
<instr>CVTPS2PI</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1246</id>
<name>_MM_CVT_ROUNDI32_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT b,INT rounding</sign>
<instr>VCVTSI2SS</instr>
<desc>Convert the 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1247</id>
<name>_MM_CVT_ROUNDI64_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__INT64 b,INT rounding</sign>
<instr>VCVTSI2SD</instr>
<desc>Convert the 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1248</id>
<name>_MM_CVT_ROUNDI64_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__INT64 b,INT rounding</sign>
<instr>VCVTSI2SS</instr>
<desc>Convert the 64-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1291</id>
<name>_MM_CVT_ROUNDSD_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1292</id>
<name>_MM_CVT_ROUNDSD_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1293</id>
<name>_MM_CVT_ROUNDSD_SI32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1294</id>
<name>_MM_CVT_ROUNDSD_SI64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1295</id>
<name>_MM_CVT_ROUNDSD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128D b,INT rounding</sign>
<instr>VCVTSD2SS</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_FP32(b[63:0])
dst[127:32] := a[127:31]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1298</id>
<name>_MM_CVT_ROUNDSD_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 32-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1299</id>
<name>_MM_CVT_ROUNDSD_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 64-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1300</id>
<name>_MM_CVT_ROUNDSI32_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT b,INT rounding</sign>
<instr>VCVTSI2SS</instr>
<desc>Convert the 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1301</id>
<name>_MM_CVT_ROUNDSI64_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__INT64 b,INT rounding</sign>
<instr>VCVTSI2SD</instr>
<desc>Convert the 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1302</id>
<name>_MM_CVT_ROUNDSI64_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__INT64 b,INT rounding</sign>
<instr>VCVTSI2SS</instr>
<desc>Convert the 64-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1303</id>
<name>_MM_CVT_ROUNDSS_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1304</id>
<name>_MM_CVT_ROUNDSS_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1305</id>
<name>_MM_CVT_ROUNDSS_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128 b,INT rounding</sign>
<instr>VCVTSS2SD</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1308</id>
<name>_MM_CVT_ROUNDSS_SI32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1309</id>
<name>_MM_CVT_ROUNDSS_SI64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1310</id>
<name>_MM_CVT_ROUNDSS_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 32-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1311</id>
<name>_MM_CVT_ROUNDSS_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 64-bit integer, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1312</id>
<name>_MM_CVT_ROUNDU32_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,UNSIGNED_INT b,INT rounding</sign>
<instr>VCVTUSI2SS</instr>
<desc>Convert the unsigned 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1313</id>
<name>_MM_CVT_ROUNDU64_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,UNSIGNED__INT64 b,INT rounding</sign>
<instr>VCVTUSI2SD</instr>
<desc>Convert the unsigned 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1314</id>
<name>_MM_CVT_ROUNDU64_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,UNSIGNED__INT64 b,INT rounding</sign>
<instr>VCVTUSI2SS</instr>
<desc>Convert the unsigned 64-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1315</id>
<name>_MM_CVT_SI2SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT b</sign>
<instr>CVTSI2SS</instr>
<desc>Convert the 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1316</id>
<name>_MM_CVT_SS2SI</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>CVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1317</id>
<name>_MM_CVTEPI16_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 16*j
	dst[i+31:i] := SignExtend(a[k+15:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1326</id>
<name>_MM_CVTEPI16_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 16*j
	dst[i+63:i] := SignExtend(a[k+15:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1335</id>
<name>_MM_CVTEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1347</id>
<name>_MM_CVTEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 16*j
	dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1356</id>
<name>_MM_CVTEPI32_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 32*j
	dst[i+63:i] := SignExtend(a[k+31:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1365</id>
<name>_MM_CVTEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 8*j
	dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1374</id>
<name>_MM_CVTEPI32_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>CVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1383</id>
<name>_MM_CVTEPI32_PS</name>
<cpuid>SSE2</cpuid>
<ret>__m128</ret>
<sign>__M128I a</sign>
<instr>CVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1400</id>
<name>_MM_CVTEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 16*j
	dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1409</id>
<name>_MM_CVTEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 32*j
	dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1418</id>
<name>_MM_CVTEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 8*j
	dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1427</id>
<name>_MM_CVTEPI64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1436</id>
<name>_MM_CVTEPI64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1454</id>
<name>_MM_CVTEPI8_EPI16</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	l := j*16
	dst[l+15:l] := SignExtend(a[i+7:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1463</id>
<name>_MM_CVTEPI8_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 8*j
	dst[i+31:i] := SignExtend(a[k+7:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1472</id>
<name>_MM_CVTEPI8_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 8*j
	dst[i+63:i] := SignExtend(a[k+7:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1481</id>
<name>_MM_CVTEPU16_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 16*j
	dst[i+31:i] := ZeroExtend(a[k+15:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1490</id>
<name>_MM_CVTEPU16_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 16*j
	dst[i+63:i] := ZeroExtend(a[k+15:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1499</id>
<name>_MM_CVTEPU32_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 32*j
	dst[i+63:i] := ZeroExtend(a[k+31:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1508</id>
<name>_MM_CVTEPU32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1522</id>
<name>_MM_CVTEPU64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1531</id>
<name>_MM_CVTEPU64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1540</id>
<name>_MM_CVTEPU8_EPI16</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	l := j*16
	dst[l+15:l] := ZeroExtend(a[i+7:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1549</id>
<name>_MM_CVTEPU8_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 8*j
	dst[i+31:i] := ZeroExtend(a[k+7:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1558</id>
<name>_MM_CVTEPU8_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 byte sof a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 8*j
	dst[i+63:i] := ZeroExtend(a[k+7:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1576</id>
<name>_MM_CVTI32_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,INT b</sign>
<instr>VCVTSI2SD</instr>
<desc>Convert the 32-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := Convert_Int32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1577</id>
<name>_MM_CVTI32_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT b</sign>
<instr>VCVTSI2SS</instr>
<desc>Convert the 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1578</id>
<name>_MM_CVTI64_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__INT64 b</sign>
<instr>VCVTSI2SD</instr>
<desc>Convert the 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1579</id>
<name>_MM_CVTI64_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__INT64 b</sign>
<instr>VCVTSI2SS</instr>
<desc>Convert the 64-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1580</id>
<name>_MM_CVTM64_SI64</name>
<cpuid>MMX</cpuid>
<ret>__int64</ret>
<sign>__M64 a</sign>
<instr>MOVQ</instr>
<desc>Copy 64-bit integer a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>1581</id>
<name>_MM_CVTPD_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>CVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1590</id>
<name>_MM_CVTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1599</id>
<name>_MM_CVTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1608</id>
<name>_MM_CVTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1617</id>
<name>_MM_CVTPD_PI32</name>
<cpuid>SSE2</cpuid>
<ret>__m64</ret>
<sign>__M128D a</sign>
<instr>CVTPD2PI</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1618</id>
<name>_MM_CVTPD_PS</name>
<cpuid>SSE2</cpuid>
<ret>__m128</ret>
<sign>__M128D a</sign>
<instr>CVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1629</id>
<name>_MM_CVTPH_PS</name>
<cpuid>FP16C</cpuid>
<ret>__m128</ret>
<sign>__M128I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*16
	dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1638</id>
<name>_MM_CVTPI16_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M64 a</sign>
<instr>NONE</instr>
<desc>Convert packed 16-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	m := j*32
	dst[m+31:m] := Convert_Int16_To_FP32(a[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1639</id>
<name>_MM_CVTPI32_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M64 a</sign>
<instr>CVTPI2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1640</id>
<name>_MM_CVTPI32_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M64 b</sign>
<instr>CVTPI2PS</instr>
<desc>Convert packed 32-bit integers in b to packed single-precision (32-bit) floating-point elements, store the results in the lower 2 elements of dst, and copy the upper 2 packed elements from a to the upper elements of dst. </desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[63:32] := Convert_Int32_To_FP32(b[63:32])
dst[95:64] := a[95:64]
dst[127:96] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>1641</id>
<name>_MM_CVTPI32X2_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M64 a,__M64 b</sign>
<instr>NONE</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, store the results in the lower 2 elements of dst, then covert the packed 32-bit integers in a to single-precision (32-bit) floating-point element, and store the results in the upper 2 elements of dst. </desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(a[31:0])
dst[63:32] := Convert_Int32_To_FP32(a[63:32])
dst[95:64] := Convert_Int32_To_FP32(b[31:0])
dst[127:96] := Convert_Int32_To_FP32(b[63:32])</oper>
</intrinsic>
<intrinsic>
<id>1642</id>
<name>_MM_CVTPI8_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M64 a</sign>
<instr>NONE</instr>
<desc>Convert the lower packed 8-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*8
	m := j*32
	dst[m+31:m] := Convert_Int8_To_FP32(a[i+7:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1643</id>
<name>_MM_CVTPS_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>CVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1652</id>
<name>_MM_CVTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1661</id>
<name>_MM_CVTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1670</id>
<name>_MM_CVTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1679</id>
<name>_MM_CVTPS_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>CVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 32*j
	dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1684</id>
<name>_MM_CVTPS_PH</name>
<cpuid>FP16C</cpuid>
<ret>__m128d</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := 16*j
	l := 32*j
	dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1693</id>
<name>_MM_CVTPS_PI16</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m64</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 16*j
	k := 32*j
	dst[i+15:i] := Convert_FP32_To_Int16(a[k+31:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1694</id>
<name>_MM_CVTPS_PI32</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M128 a</sign>
<instr>CVTPS2PI</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1695</id>
<name>_MM_CVTPS_PI8</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m64</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 8-bit integers, and store the results in lower 4 elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := 8*j
	k := 32*j
	dst[i+7:i] := Convert_FP32_To_Int8(a[k+31:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1698</id>
<name>_MM_CVTPU16_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M64 a</sign>
<instr>NONE</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	m := j*32
	dst[m+31:m] := Convert_UnsignedInt16_To_FP32(a[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1699</id>
<name>_MM_CVTPU8_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M64 a</sign>
<instr>NONE</instr>
<desc>Convert the lower packed unsigned 8-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*8
	m := j*32
	dst[m+31:m] := Convert_UnsignedInt8_To_FP32(a[i+7:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1700</id>
<name>_MM_CVTSD_F64</name>
<cpuid>SSE2</cpuid>
<ret>double</ret>
<sign>__M128D a</sign>
<instr>MOVSD</instr>
<desc>Copy the lower double-precision (64-bit) floating-point element of a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>1701</id>
<name>_MM_CVTSD_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a</sign>
<instr>VCVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1702</id>
<name>_MM_CVTSD_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128D a</sign>
<instr>VCVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1703</id>
<name>_MM_CVTSD_SI32</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a</sign>
<instr>CVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1704</id>
<name>_MM_CVTSD_SI64</name>
<cpuid>SSE2</cpuid>
<ret>__int64</ret>
<sign>__M128D a</sign>
<instr>CVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1705</id>
<name>_MM_CVTSD_SI64X</name>
<cpuid>SSE2</cpuid>
<ret>__int64</ret>
<sign>__M128D a</sign>
<instr>CVTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1706</id>
<name>_MM_CVTSD_SS</name>
<cpuid>SSE2</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128D b</sign>
<instr>CVTSD2SS</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
	</desc>
<oper>dst[31:0] := Convert_FP64_To_FP32(b[63:0])
dst[127:32] := a[127:31]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1709</id>
<name>_MM_CVTSD_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128D a</sign>
<instr>VCVTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1710</id>
<name>_MM_CVTSD_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128D a</sign>
<instr>VCVTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1711</id>
<name>_MM_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1723</id>
<name>_MM_CVTSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 16*j
	dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1732</id>
<name>_MM_CVTSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 8*j
	dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1747</id>
<name>_MM_CVTSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 16*j
	dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1756</id>
<name>_MM_CVTSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 32*j
	dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1765</id>
<name>_MM_CVTSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 8*j
	dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1784</id>
<name>_MM_CVTSI128_SI32</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128I a</sign>
<instr>MOVD</instr>
<desc>Copy the lower 32-bit integer in a to dst.</desc>
<oper>dst[31:0] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>1785</id>
<name>_MM_CVTSI128_SI64</name>
<cpuid>SSE2</cpuid>
<ret>__int64</ret>
<sign>__M128I a</sign>
<instr>MOVQ</instr>
<desc>Copy the lower 64-bit integer in a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>1786</id>
<name>_MM_CVTSI128_SI64X</name>
<cpuid>SSE2</cpuid>
<ret>__int64</ret>
<sign>__M128I a</sign>
<instr>MOVQ</instr>
<desc>Copy the lower 64-bit integer in a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>1787</id>
<name>_MM_CVTSI32_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,INT b</sign>
<instr>CVTSI2SD</instr>
<desc>Convert the 32-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := Convert_Int32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1788</id>
<name>_MM_CVTSI32_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>INT a</sign>
<instr>MOVD</instr>
<desc>Copy 32-bit integer a to the lower elements of dst, and zero the upper elements of dst.</desc>
<oper>dst[31:0] := a[31:0]
dst[127:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1789</id>
<name>_MM_CVTSI32_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>INT a</sign>
<instr>MOVD</instr>
<desc>Copy 32-bit integer a to the lower elements of dst, and zero the upper element of dst.</desc>
<oper>dst[31:0] := a[31:0]
dst[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1790</id>
<name>_MM_CVTSI32_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT b</sign>
<instr>CVTSI2SS</instr>
<desc>Convert the 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>1791</id>
<name>_MM_CVTSI64_M64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__INT64 a</sign>
<instr>MOVQ</instr>
<desc>Copy 64-bit integer a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>1792</id>
<name>_MM_CVTSI64_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__INT64 b</sign>
<instr>CVTSI2SD</instr>
<desc>Convert the 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1793</id>
<name>_MM_CVTSI64_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__INT64 a</sign>
<instr>MOVQ</instr>
<desc>Copy 64-bit integer a to the lower element of dst, and zero the upper element.</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1794</id>
<name>_MM_CVTSI64_SI32</name>
<cpuid>MMX</cpuid>
<ret>int</ret>
<sign>__M64 a</sign>
<instr>MOVD</instr>
<desc>Copy the lower 32-bit integer in a to dst.</desc>
<oper>dst[31:0] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>1795</id>
<name>_MM_CVTSI64_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__INT64 b</sign>
<instr>CVTSI2SS</instr>
<desc>Convert the 64-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1796</id>
<name>_MM_CVTSI64X_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__INT64 b</sign>
<instr>CVTSI2SD</instr>
<desc>Convert the 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1797</id>
<name>_MM_CVTSI64X_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__INT64 a</sign>
<instr>MOVQ</instr>
<desc>Copy 64-bit integer a to the lower element of dst, and zero the upper element.</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1798</id>
<name>_MM_CVTSS_F32</name>
<cpuid>SSE</cpuid>
<ret>float</ret>
<sign>__M128 a</sign>
<instr>MOVSS</instr>
<desc>Copy the lower single-precision (32-bit) floating-point element of a to dst.</desc>
<oper>dst[31:0] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>1799</id>
<name>_MM_CVTSS_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>VCVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1800</id>
<name>_MM_CVTSS_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128 a</sign>
<instr>VCVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1801</id>
<name>_MM_CVTSS_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128 b</sign>
<instr>CVTSS2SD</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
	</desc>
<oper>dst[63:0] := Convert_FP32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1805</id>
<name>_MM_CVTSS_SI32</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>CVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1806</id>
<name>_MM_CVTSS_SI64</name>
<cpuid>SSE</cpuid>
<ret>__int64</ret>
<sign>__M128 a</sign>
<instr>CVTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1807</id>
<name>_MM_CVTSS_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128 a</sign>
<instr>VCVTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 32-bit integer, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1808</id>
<name>_MM_CVTSS_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128 a</sign>
<instr>VCVTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 64-bit integer, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1809</id>
<name>_MM_CVTT_PS2PI</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M128 a</sign>
<instr>CVTTPS2PI</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1834</id>
<name>_MM_CVTT_ROUNDSD_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1835</id>
<name>_MM_CVTT_ROUNDSD_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1836</id>
<name>_MM_CVTT_ROUNDSD_SI32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1837</id>
<name>_MM_CVTT_ROUNDSD_SI64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1838</id>
<name>_MM_CVTT_ROUNDSD_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 32-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1839</id>
<name>_MM_CVTT_ROUNDSD_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128D a,INT rounding</sign>
<instr>VCVTTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 64-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1840</id>
<name>_MM_CVTT_ROUNDSS_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1841</id>
<name>_MM_CVTT_ROUNDSS_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1842</id>
<name>_MM_CVTT_ROUNDSS_SI32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1843</id>
<name>_MM_CVTT_ROUNDSS_SI64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1844</id>
<name>_MM_CVTT_ROUNDSS_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 32-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1845</id>
<name>_MM_CVTT_ROUNDSS_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128 a,INT rounding</sign>
<instr>VCVTTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 64-bit integer with truncation, and store the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1846</id>
<name>_MM_CVTT_SS2SI</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>CVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1847</id>
<name>_MM_CVTTPD_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>CVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1856</id>
<name>_MM_CVTTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1865</id>
<name>_MM_CVTTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1874</id>
<name>_MM_CVTTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1883</id>
<name>_MM_CVTTPD_PI32</name>
<cpuid>SSE2</cpuid>
<ret>__m64</ret>
<sign>__M128D a</sign>
<instr>CVTTPD2PI</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1884</id>
<name>_MM_CVTTPS_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>CVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1893</id>
<name>_MM_CVTTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1902</id>
<name>_MM_CVTTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1911</id>
<name>_MM_CVTTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1920</id>
<name>_MM_CVTTPS_PI32</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M128 a</sign>
<instr>CVTTPS2PI</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1921</id>
<name>_MM_CVTTSD_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128D a</sign>
<instr>VCVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1922</id>
<name>_MM_CVTTSD_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128D a</sign>
<instr>VCVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1923</id>
<name>_MM_CVTTSD_SI32</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a</sign>
<instr>CVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1924</id>
<name>_MM_CVTTSD_SI64</name>
<cpuid>SSE2</cpuid>
<ret>__int64</ret>
<sign>__M128D a</sign>
<instr>CVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1925</id>
<name>_MM_CVTTSD_SI64X</name>
<cpuid>SSE2</cpuid>
<ret>__int64</ret>
<sign>__M128D a</sign>
<instr>CVTTSD2SI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1926</id>
<name>_MM_CVTTSD_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128D a</sign>
<instr>VCVTTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1927</id>
<name>_MM_CVTTSD_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128D a</sign>
<instr>VCVTTSD2USI</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in a to an unsigned 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])</oper>
</intrinsic>
<intrinsic>
<id>1928</id>
<name>_MM_CVTTSS_I32</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>VCVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1929</id>
<name>_MM_CVTTSS_I64</name>
<cpuid>AVX512F</cpuid>
<ret>__int64</ret>
<sign>__M128 a</sign>
<instr>VCVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1930</id>
<name>_MM_CVTTSS_SI32</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>CVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1931</id>
<name>_MM_CVTTSS_SI64</name>
<cpuid>SSE</cpuid>
<ret>__int64</ret>
<sign>__M128 a</sign>
<instr>CVTTSS2SI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to a 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP64_To_Int32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1932</id>
<name>_MM_CVTTSS_U32</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned int</ret>
<sign>__M128 a</sign>
<instr>VCVTTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 32-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1933</id>
<name>_MM_CVTTSS_U64</name>
<cpuid>AVX512F</cpuid>
<ret>unsigned __int64</ret>
<sign>__M128 a</sign>
<instr>VCVTTSS2USI</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in a to an unsigned 64-bit integer with truncation, and store the result in dst.</desc>
<oper>dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])</oper>
</intrinsic>
<intrinsic>
<id>1934</id>
<name>_MM_CVTU32_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,UNSIGNED_INT b</sign>
<instr>VCVTUSI2SD</instr>
<desc>Convert the unsigned 32-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := Convert_UnsignedInt32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1935</id>
<name>_MM_CVTU32_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,UNSIGNED_INT b</sign>
<instr>VCVTUSI2SS</instr>
<desc>Convert the unsigned 32-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1936</id>
<name>_MM_CVTU64_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,UNSIGNED__INT64 b</sign>
<instr>VCVTUSI2SD</instr>
<desc>Convert the unsigned 64-bit integer b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1937</id>
<name>_MM_CVTU64_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,UNSIGNED__INT64 b</sign>
<instr>VCVTUSI2SS</instr>
<desc>Convert the unsigned 64-bit integer b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1938</id>
<name>_MM_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1950</id>
<name>_MM_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 16*j
	dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1959</id>
<name>_MM_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 8*j
	dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1974</id>
<name>_MM_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 16*j
	dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1983</id>
<name>_MM_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 32*j
	dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1992</id>
<name>_MM_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	k := 8*j
	dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2010</id>
<name>_MM_DBSAD_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst.
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>tmp[31:0] := select(b[127:0], imm8[1:0])
tmp[63:32] := select(b[127:0], imm8[3:2])
tmp[95:64] := select(b[127:0], imm8[5:4])
tmp[127:96] := select(b[127:0], imm8[7:6])

FOR j := 0 to 1
	i := j*64
	dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2019</id>
<name>_MM_DELAY_32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT r1</sign>
<instr>DELAY</instr>
<desc>Stalls a thread without blocking other threads for 32-bit unsigned integer r1 clock cycles.</desc>
<oper>BlockThread(r1)</oper>
</intrinsic>
<intrinsic>
<id>2020</id>
<name>_MM_DELAY_64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>UNSIGNED__INT64 r1</sign>
<instr>DELAY</instr>
<desc>Stalls a thread without blocking other threads for 64-bit unsigned integer r1 clock cycles.</desc>
<oper>BlockThread(r1)</oper>
</intrinsic>
<intrinsic>
<id>2021</id>
<name>_MM_DIV_EPI16</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 16-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2024</id>
<name>_MM_DIV_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2028</id>
<name>_MM_DIV_EPI64</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 64-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2031</id>
<name>_MM_DIV_EPI8</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 8-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 8*j
	dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2034</id>
<name>_MM_DIV_EPU16</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 16-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2037</id>
<name>_MM_DIV_EPU32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2041</id>
<name>_MM_DIV_EPU64</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 64-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2044</id>
<name>_MM_DIV_EPU8</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 8-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 8*j
	dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2047</id>
<name>_MM_DIV_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>DIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	dst[i+63:i] := a[i+63:i] / b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2056</id>
<name>_MM_DIV_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>DIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := a[i+31:i] / b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2071</id>
<name>_MM_DIV_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VDIVSD</instr>
<desc>Divide the lower double-precision (64-bit) floating-point element in a by the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>dst[63:0] := a[63:0] / b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2074</id>
<name>_MM_DIV_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VDIVSS</instr>
<desc>Divide the lower single-precision (32-bit) floating-point element in a by the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>dst[31:0] := a[31:0] / b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2077</id>
<name>_MM_DIV_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>DIVSD</instr>
<desc>Divide the lower double-precision (64-bit) floating-point element in a by the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. </desc>
<oper>dst[63:0] := a[63:0] 0 b[63:0]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>2080</id>
<name>_MM_DIV_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>DIVSS</instr>
<desc>Divide the lower single-precision (32-bit) floating-point element in a by the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. </desc>
<oper>dst[31:0] := a[31:0] / b[31:0]
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>2083</id>
<name>_MM_DP_PD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>DPPD</instr>
<desc>Conditionally multiply the packed double-precision (64-bit) floating-point elements in a and b using the high 4 bits in imm8, sum the four products, and conditionally store the sum in dst using the low 4 bits of imm8.</desc>
<oper>DP(a[127:0], b[127:0], imm8[7:0]) {
	FOR j := 0 to 1
		i := j*64
		IF imm8[(4+j)%8]]
			temp[i+63:i] := a[i+63:i] * b[i+63:i]
		ELSE
			temp[i+63:i] := 0
		FI
	ENDFOR
	
	sum[63:0] := temp[127:64] + temp[63:0]
	
	FOR j := 0 to 1
		i := j*64
		IF imm8[j%8]
			tmpdst[i+63:i] := sum[63:0]
		ELSE
			tmpdst[i+63:i] := 0
		FI
	ENDFOR
	RETURN tmpdst[127:0]
}

dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])</oper>
</intrinsic>
<intrinsic>
<id>2084</id>
<name>_MM_DP_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>DPPS</instr>
<desc>Conditionally multiply the packed single-precision (32-bit) floating-point elements in a and b using the high 4 bits in imm8, sum the four products, and conditionally store the sum in dst using the low 4 bits of imm8.</desc>
<oper>DP(a[127:0], b[127:0], imm8[7:0]) {
	FOR j := 0 to 3
		i := j*32
		IF imm8[(4+j)%8]
			temp[i+31:i] := a[i+31:i] * b[i+31:i]
		ELSE
			temp[i+31:i] := 0
		FI
	ENDFOR
	
	sum[31:0] := (temp[127:96] + temp[95:64]) + (temp[63:32] + temp[31:0])
	
	FOR j := 0 to 3
		i := j*32
		IF imm8[j%8]
			tmpdst[i+31:i] := sum[31:0]
		ELSE
			tmpdst[i+31:i] := 0
		FI
	ENDFOR
	RETURN tmpdst[127:0]
}

dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])</oper>
</intrinsic>
<intrinsic>
<id>2087</id>
<name>_MM_EMPTY</name>
<cpuid>MMX</cpuid>
<ret>void</ret>
<sign></sign>
<instr>EMMS</instr>
<desc>Empty the MMX state, which marks the x87 FPU registers as available for use by x87 instructions. This instruction must be used at the end of all MMX technology procedures.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>2088</id>
<name>_MM_ERF_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ERF(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2092</id>
<name>_MM_ERF_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ERF(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2096</id>
<name>_MM_ERFC_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := 1.0 - ERF(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2100</id>
<name>_MM_ERFC_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := 1.0 - ERF(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2104</id>
<name>_MM_ERFCINV_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2108</id>
<name>_MM_ERFCINV_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2112</id>
<name>_MM_ERFINV_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := 1.0 / ERF(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2116</id>
<name>_MM_ERFINV_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := 1.0 / ERF(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2120</id>
<name>_MM_EXP_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := e^(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2124</id>
<name>_MM_EXP_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := e^(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2128</id>
<name>_MM_EXP10_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := 10^(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2132</id>
<name>_MM_EXP10_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := 10^(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2136</id>
<name>_MM_EXP2_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := 2^(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2140</id>
<name>_MM_EXP2_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := 2^(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2206</id>
<name>_MM_EXPM1_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, subtract one from each element, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := e^(a[i+63:i]) - 1.0
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2210</id>
<name>_MM_EXPM1_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, subtract one from each element, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := e^(a[i+31:i]) - 1.0
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2254</id>
<name>_MM_EXTRACT_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PEXTRW</instr>
<desc>Extract a 16-bit integer from a, selected with imm8, and store the result in the lower element of dst.</desc>
<oper>dst[15:0] := (a[127:0] &amp;gt;&amp;gt; (imm8[2:0] * 16))[15:0]
dst[31:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2256</id>
<name>_MM_EXTRACT_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>PEXTRD</instr>
<desc>Extract a 32-bit integer from a, selected with imm8, and store the result in dst.</desc>
<oper>dst[31:0] := (a[127:0] &amp;gt;&amp;gt; (imm8[1:0] * 32))[31:0]</oper>
</intrinsic>
<intrinsic>
<id>2258</id>
<name>_MM_EXTRACT_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__int64</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>PEXTRQ</instr>
<desc>Extract a 64-bit integer from a, selected with imm8, and store the result in dst.</desc>
<oper>dst[63:0] := (a[127:0] &amp;gt;&amp;gt; (imm8[0] * 64))[63:0]</oper>
</intrinsic>
<intrinsic>
<id>2260</id>
<name>_MM_EXTRACT_EPI8</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>PEXTRB</instr>
<desc>Extract an 8-bit integer from a, selected with imm8, and store the result in the lower element of dst.</desc>
<oper>dst[7:0] := (a[127:0] &amp;gt;&amp;gt; (imm8[3:0] * 8))[7:0]
dst[31:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2262</id>
<name>_MM_EXTRACT_PI16</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PEXTRW</instr>
<desc>Extract a 16-bit integer from a, selected with imm8, and store the result in the lower element of dst.</desc>
<oper>dst[15:0] := (a[63:0] &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
dst[31:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2263</id>
<name>_MM_EXTRACT_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128 a,CONST_INT imm8</sign>
<instr>EXTRACTPS</instr>
<desc>Extract a single-precision (32-bit) floating-point element from a, selected with imm8, and store the result in dst.</desc>
<oper>dst[31:0] := (a[127:0] &amp;gt;&amp;gt; (imm8[1:0] * 32))[31:0]</oper>
</intrinsic>
<intrinsic>
<id>2312</id>
<name>_MM_FIXUPIMM_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2321</id>
<name>_MM_FIXUPIMM_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2336</id>
<name>_MM_FIXUPIMM_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMSD</instr>
<desc>Fix up the lower double-precision (64-bit) floating-point elements in a and b using the lower 64-bit integer in c, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2339</id>
<name>_MM_FIXUPIMM_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMSS</instr>
<desc>Fix up the lower single-precision (32-bit) floating-point elements in a and b using the lower 32-bit integer in c, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2342</id>
<name>_MM_FIXUPIMM_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMSD</instr>
<desc>Fix up the lower double-precision (64-bit) floating-point elements in a and b using the lower 64-bit integer in c, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2345</id>
<name>_MM_FIXUPIMM_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMSS</instr>
<desc>Fix up the lower single-precision (32-bit) floating-point elements in a and b using the lower 32-bit integer in c, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2352</id>
<name>_MM_FLOOR_PD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>ROUNDPD</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a down to an integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := FLOOR(a[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2356</id>
<name>_MM_FLOOR_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>ROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a down to an integer value, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := FLOOR(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2360</id>
<name>_MM_FLOOR_SD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ROUNDSD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in b down to an integer value, store the result as a double-precision floating-point element in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := FLOOR(b[63:0])
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>2361</id>
<name>_MM_FLOOR_SS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ROUNDSS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in b down to an integer value, store the result as a single-precision floating-point element in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := FLOOR(b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>2365</id>
<name>_MM_FMADD_PD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2377</id>
<name>_MM_FMADD_PS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2403</id>
<name>_MM_FMADD_SD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
	</desc>
<oper>dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2407</id>
<name>_MM_FMADD_SS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
	</desc>
<oper>dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2417</id>
<name>_MM_FMADDSUB_PD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2429</id>
<name>_MM_FMADDSUB_PS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2449</id>
<name>_MM_FMSUB_PD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2461</id>
<name>_MM_FMSUB_PS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2487</id>
<name>_MM_FMSUB_SD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2491</id>
<name>_MM_FMSUB_SS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2495</id>
<name>_MM_FMSUBADD_PD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2507</id>
<name>_MM_FMSUBADD_PS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2527</id>
<name>_MM_FNMADD_PD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2539</id>
<name>_MM_FNMADD_PS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2565</id>
<name>_MM_FNMADD_SD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMADD132SD, VFNMADD213SD, VFNMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2569</id>
<name>_MM_FNMADD_SS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2573</id>
<name>_MM_FNMSUB_PD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2585</id>
<name>_MM_FNMSUB_PS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2611</id>
<name>_MM_FNMSUB_SD</name>
<cpuid>FMA</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2615</id>
<name>_MM_FNMSUB_SS</name>
<cpuid>FMA</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2619</id>
<name>_MM_FPCLASS_PD_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128D a,INT imm8</sign>
<instr>VFPCLASSPD</instr>
<desc>Test packed double-precision (64-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2625</id>
<name>_MM_FPCLASS_PS_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128 a,INT imm8</sign>
<instr>VFPCLASSPS</instr>
<desc>Test packed single-precision (32-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2631</id>
<name>_MM_FPCLASS_SD_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__M128D a,INT imm8</sign>
<instr>VFPCLASSSD</instr>
<desc>Test the lower double-precision (64-bit) floating-point element in a for special categories specified by imm8, and store the result in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>k[0] := CheckFPClass_FP64(a[63:0], imm8[7:0])
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>2633</id>
<name>_MM_FPCLASS_SS_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__M128 a,INT imm8</sign>
<instr>VFPCLASSSS</instr>
<desc>Test the lower single-precision (32-bit) floating-point element in a for special categories specified by imm8, and store the result in mask vector k.
	imm" can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>k[0] := CheckFPClass_FP32(a[31:0], imm8[7:0])
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>2635</id>
<name>_MM_FREE</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr</sign>
<instr></instr>
<desc>Free aligned memory that was allocated with _mm_malloc.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>2642</id>
<name>_MM_GET_EXCEPTION_MASK</name>
<cpuid>SSE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr></instr>
<desc>Macro: Get the exception mask bits from the MXCSR control and status register. The exception mask may contain any of the following flags: _MM_MASK_INVALID, _MM_MASK_DIV_ZERO, _MM_MASK_DENORM, _MM_MASK_OVERFLOW, _MM_MASK_UNDERFLOW, _MM_MASK_INEXACT</desc>
<oper>dst[31:0] := MXCSR &amp; _MM_MASK_MASK</oper>
</intrinsic>
<intrinsic>
<id>2643</id>
<name>_MM_GET_EXCEPTION_STATE</name>
<cpuid>SSE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr></instr>
<desc>Macro: Get the exception state bits from the MXCSR control and status register. The exception state may contain any of the following flags: _MM_EXCEPT_INVALID, _MM_EXCEPT_DIV_ZERO, _MM_EXCEPT_DENORM, _MM_EXCEPT_OVERFLOW, _MM_EXCEPT_UNDERFLOW, _MM_EXCEPT_INEXACT</desc>
<oper>dst[31:0] := MXCSR &amp; _MM_EXCEPT_MASK</oper>
</intrinsic>
<intrinsic>
<id>2644</id>
<name>_MM_GET_FLUSH_ZERO_MODE</name>
<cpuid>SSE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr></instr>
<desc>Macro: Get the flush zero bits from the MXCSR control and status register. The flush zero may contain any of the following flags: _MM_FLUSH_ZERO_ON or _MM_FLUSH_ZERO_OFF</desc>
<oper>dst[31:0] := MXCSR &amp; _MM_FLUSH_MASK</oper>
</intrinsic>
<intrinsic>
<id>2645</id>
<name>_MM_GET_ROUNDING_MODE</name>
<cpuid>SSE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr></instr>
<desc>Macro: Get the rounding mode bits from the MXCSR control and status register. The rounding mode may contain any of the following flags: _MM_ROUND_NEAREST, _MM_ROUND_DOWN, _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO</desc>
<oper>dst[31:0] := MXCSR &amp; _MM_ROUND_MASK</oper>
</intrinsic>
<intrinsic>
<id>2646</id>
<name>_MM_GETCSR</name>
<cpuid>SSE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr>STMXCSR</instr>
<desc>Get the unsigned 32-bit value of the MXCSR control and status register.</desc>
<oper>dst[31:0] := MXCSR</oper>
</intrinsic>
<intrinsic>
<id>2647</id>
<name>_MM_GETEXP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2656</id>
<name>_MM_GETEXP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2671</id>
<name>_MM_GETEXP_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VGETEXPSD</instr>
<desc>Convert the exponent of the lower double-precision (64-bit) floating-point element in b to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := ConvertExpFP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2674</id>
<name>_MM_GETEXP_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VGETEXPSS</instr>
<desc>Convert the exponent of the lower single-precision (32-bit) floating-point element in b to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := ConvertExpFP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2677</id>
<name>_MM_GETEXP_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VGETEXPSD</instr>
<desc>Convert the exponent of the lower double-precision (64-bit) floating-point element in b to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.</desc>
<oper>dst[63:0] := ConvertExpFP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2680</id>
<name>_MM_GETEXP_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VGETEXPSS</instr>
<desc>Convert the exponent of the lower single-precision (32-bit) floating-point element in b to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.</desc>
<oper>dst[31:0] := ConvertExpFP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2683</id>
<name>_MM_GETMANT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2692</id>
<name>_MM_GETMANT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2707</id>
<name>_MM_GETMANT_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTSD</instr>
<desc>Normalize the mantissas of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2710</id>
<name>_MM_GETMANT_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTSS</instr>
<desc>Normalize the mantissas of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2713</id>
<name>_MM_GETMANT_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTSD</instr>
<desc>Normalize the mantissas of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2716</id>
<name>_MM_GETMANT_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTSS</instr>
<desc>Normalize the mantissas of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2729</id>
<name>_MM_HADD_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PHADDW</instr>
<desc>Horizontally add adjacent pairs of 16-bit integers in a and b, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0] := a[31:16] + a[15:0]
dst[31:16] := a[63:48] + a[47:32]
dst[47:32] := a[95:80] + a[79:64]
dst[63:48] := a[127:112] + a[111:96]
dst[79:64] := b[31:16] + b[15:0]
dst[95:80] := b[63:48] + b[47:32]
dst[111:96] := b[95:80] + b[79:64]
dst[127:112] := b[127:112] + b[111:96]</oper>
</intrinsic>
<intrinsic>
<id>2731</id>
<name>_MM_HADD_EPI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PHADDD</instr>
<desc>Horizontally add adjacent pairs of 32-bit integers in a and b, and pack the signed 32-bit results in dst.</desc>
<oper>dst[31:0] := a[63:32] + a[31:0]
dst[63:32] := a[127:96] + a[95:64]
dst[95:64] := b[63:32] + b[31:0]
dst[127:96] := b[127:96] + b[95:64]</oper>
</intrinsic>
<intrinsic>
<id>2733</id>
<name>_MM_HADD_PD</name>
<cpuid>SSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>HADDPD</instr>
<desc>Horizontally add adjacent pairs of double-precision (64-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[63:0] := a[127:64] + a[63:0]
dst[127:64] := b[127:64] + b[63:0]</oper>
</intrinsic>
<intrinsic>
<id>2735</id>
<name>_MM_HADD_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PHADDW</instr>
<desc>Horizontally add adjacent pairs of 16-bit integers in a and b, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0] := a[31:16] + a[15:0]
dst[31:16] := a[63:48] + a[47:32]
dst[47:32] := b[31:16] + b[15:0]
dst[63:48] := b[63:48] + b[47:32]</oper>
</intrinsic>
<intrinsic>
<id>2736</id>
<name>_MM_HADD_PI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PHADDW</instr>
<desc>Horizontally add adjacent pairs of 32-bit integers in a and b, and pack the signed 32-bit results in dst.</desc>
<oper>dst[31:0] := a[63:32] + a[31:0]
dst[63:32] := b[63:32] + b[31:0]</oper>
</intrinsic>
<intrinsic>
<id>2737</id>
<name>_MM_HADD_PS</name>
<cpuid>SSE3</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>HADDPS</instr>
<desc>Horizontally add adjacent pairs of single-precision (32-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[31:0] := a[63:32] + a[31:0]
dst[63:32] := a[127:96] + a[95:64]
dst[95:64] := b[63:32] + b[31:0]
dst[127:96] := b[127:96] + b[95:64]</oper>
</intrinsic>
<intrinsic>
<id>2739</id>
<name>_MM_HADDS_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PHADDSW</instr>
<desc>Horizontally add adjacent pairs of 16-bit integers in a and b using saturation, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0]= Saturate_To_Int16(a[31:16] + a[15:0])
dst[31:16] = Saturate_To_Int16(a[63:48] + a[47:32])
dst[47:32] = Saturate_To_Int16(a[95:80] + a[79:64])
dst[63:48] = Saturate_To_Int16(a[127:112] + a[111:96])
dst[79:64] = Saturate_To_Int16(b[31:16] + b[15:0])
dst[95:80] = Saturate_To_Int16(b[63:48] + b[47:32])
dst[111:96] = Saturate_To_Int16(b[95:80] + b[79:64])
dst[127:112] = Saturate_To_Int16(b[127:112] + b[111:96])</oper>
</intrinsic>
<intrinsic>
<id>2741</id>
<name>_MM_HADDS_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PHADDSW</instr>
<desc>Horizontally add adjacent pairs of 16-bit integers in a and b using saturation, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0]= Saturate_To_Int16(a[31:16] + a[15:0])
dst[31:16] = Saturate_To_Int16(a[63:48] + a[47:32])
dst[47:32] = Saturate_To_Int16(b[31:16] + b[15:0])
dst[63:48] = Saturate_To_Int16(b[63:48] + b[47:32])</oper>
</intrinsic>
<intrinsic>
<id>2742</id>
<name>_MM_HSUB_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PHSUBW</instr>
<desc>Horizontally subtract adjacent pairs of 16-bit integers in a and b, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0] := a[15:0] - a[31:16]
dst[31:16] := a[47:32] - a[63:48]
dst[47:32] := a[79:64] - a[95:80]
dst[63:48] := a[111:96] - a[127:112]
dst[79:64] := b[15:0] - b[31:16]
dst[95:80] := b[47:32] - b[63:48]
dst[111:96] := b[79:64] - b[95:80]
dst[127:112] := b[111:96] - b[127:112]</oper>
</intrinsic>
<intrinsic>
<id>2744</id>
<name>_MM_HSUB_EPI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PHSUBD</instr>
<desc>Horizontally subtract adjacent pairs of 32-bit integers in a and b, and pack the signed 32-bit results in dst.</desc>
<oper>dst[31:0] := a[31:0] - a[63:32]
dst[63:32] := a[95:64] - a[127:96]
dst[95:64] := b[31:0] - b[63:32]
dst[127:96] := b[95:64] - b[127:96]</oper>
</intrinsic>
<intrinsic>
<id>2746</id>
<name>_MM_HSUB_PD</name>
<cpuid>SSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>HSUBPD</instr>
<desc>Horizontally subtract adjacent pairs of double-precision (64-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[63:0] := a[63:0] - a[127:64]
dst[127:64] := b[63:0] - b[127:64]</oper>
</intrinsic>
<intrinsic>
<id>2748</id>
<name>_MM_HSUB_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PHSUBW</instr>
<desc>Horizontally subtract adjacent pairs of 16-bit integers in a and b, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0] := a[15:0] - a[31:16]
dst[31:16] := a[47:32] - a[63:48]
dst[47:32] := b[15:0] - b[31:16]
dst[63:48] := b[47:32] - b[63:48]</oper>
</intrinsic>
<intrinsic>
<id>2749</id>
<name>_MM_HSUB_PI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PHSUBD</instr>
<desc>Horizontally subtract adjacent pairs of 32-bit integers in a and b, and pack the signed 32-bit results in dst.</desc>
<oper>dst[31:0] := a[31:0] - a[63:32]
dst[63:32] := b[31:0] - b[63:32]</oper>
</intrinsic>
<intrinsic>
<id>2750</id>
<name>_MM_HSUB_PS</name>
<cpuid>SSE3</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>HSUBPS</instr>
<desc>Horizontally add adjacent pairs of single-precision (32-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[31:0] := a[31:0] - a[63:32]
dst[63:32] := a[95:64] - a[127:96]
dst[95:64] := b[31:0] - b[63:32]
dst[127:96] := b[95:64] - b[127:96]</oper>
</intrinsic>
<intrinsic>
<id>2752</id>
<name>_MM_HSUBS_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PHSUBSW</instr>
<desc>Horizontally subtract adjacent pairs of 16-bit integers in a and b using saturation, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0]= Saturate_To_Int16(a[15:0] - a[31:16])
dst[31:16] = Saturate_To_Int16(a[47:32] - a[63:48])
dst[47:32] = Saturate_To_Int16(a[79:64] - a[95:80])
dst[63:48] = Saturate_To_Int16(a[111:96] - a[127:112])
dst[79:64] = Saturate_To_Int16(b[15:0] - b[31:16])
dst[95:80] = Saturate_To_Int16(b[47:32] - b[63:48])
dst[111:96] = Saturate_To_Int16(b[79:64] - b[95:80])
dst[127:112] = Saturate_To_Int16(b[111:96] - b[127:112])</oper>
</intrinsic>
<intrinsic>
<id>2754</id>
<name>_MM_HSUBS_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PHSUBSW</instr>
<desc>Horizontally subtract adjacent pairs of 16-bit integers in a and b using saturation, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0]= Saturate_To_Int16(a[15:0] - a[31:16])
dst[31:16] = Saturate_To_Int16(a[47:32] - a[63:48])
dst[47:32] = Saturate_To_Int16(b[15:0] - b[31:16])
dst[63:48] = Saturate_To_Int16(b[47:32] - b[63:48])</oper>
</intrinsic>
<intrinsic>
<id>2755</id>
<name>_MM_HYPOT_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2759</id>
<name>_MM_HYPOT_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2771</id>
<name>_MM_I32GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>INT_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2779</id>
<name>_MM_I32GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__INT64_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	m := j*32
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2787</id>
<name>_MM_I32GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	m := j*32
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2795</id>
<name>_MM_I32GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2819</id>
<name>_MM_I32SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERDD</instr>
<desc>Scatter 32-bit integers from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2825</id>
<name>_MM_I32SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Scatter 64-bit integers from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2831</id>
<name>_MM_I32SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128D a,CONST_INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2837</id>
<name>_MM_I32SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128 a,CONST_INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2859</id>
<name>_MM_I64GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>INT_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2869</id>
<name>_MM_I64GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__INT64_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2877</id>
<name>_MM_I64GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2885</id>
<name>_MM_I64GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2895</id>
<name>_MM_I64SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERQD</instr>
<desc>Scatter 32-bit integers from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2903</id>
<name>_MM_I64SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERQQ</instr>
<desc>Scatter 64-bit integers from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2909</id>
<name>_MM_I64SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128D a,CONST_INT scale</sign>
<instr>VSCATTERQPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2915</id>
<name>_MM_I64SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M128 a,CONST_INT scale</sign>
<instr>VSCATTERQPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2923</id>
<name>_MM_IDIV_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2925</id>
<name>_MM_IDIVREM_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I_PTR mem_addr,__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, store the truncated results in dst, and store the remainders as packed 32-bit integers into memory at mem_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
	MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2927</id>
<name>_MM_INSERT_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT i,INT imm8</sign>
<instr>PINSRW</instr>
<desc>Copy a to dst, and insert the 16-bit integer i into dst at the location specified by imm8. </desc>
<oper>dst[127:0] := a[127:0]
sel := imm8[2:0]*16
dst[sel+15:sel] := i[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2929</id>
<name>_MM_INSERT_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT i,CONST_INT imm8</sign>
<instr>PINSRD</instr>
<desc>Copy a to dst, and insert the 32-bit integer i into dst at the location specified by imm8. </desc>
<oper>dst[127:0] := a[127:0]
sel := imm8[1:0]*32
dst[sel+31:sel] := i[31:0]</oper>
</intrinsic>
<intrinsic>
<id>2931</id>
<name>_MM_INSERT_EPI64</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__INT64 i,CONST_INT imm8</sign>
<instr>PINSRQ</instr>
<desc>Copy a to dst, and insert the 64-bit integer i into dst at the location specified by imm8. </desc>
<oper>dst[127:0] := a[127:0]
sel := imm8[0]*64
dst[sel+63:sel] := i[63:0]</oper>
</intrinsic>
<intrinsic>
<id>2933</id>
<name>_MM_INSERT_EPI8</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT i,CONST_INT imm8</sign>
<instr>PINSRB</instr>
<desc>Copy a to dst, and insert the lower 8-bit integer from i into dst at the location specified by imm8. </desc>
<oper>dst[127:0] := a[127:0]
sel := imm8[3:0]*8
dst[sel+7:sel] := i[7:0]</oper>
</intrinsic>
<intrinsic>
<id>2935</id>
<name>_MM_INSERT_PI16</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT i,INT imm8</sign>
<instr>PINSRW</instr>
<desc>Copy a to dst, and insert the 16-bit integer i into dst at the location specified by imm8. </desc>
<oper>dst[63:0] := a[63:0]
sel := imm8[1:0]*16
dst[sel+15:sel] := i[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2936</id>
<name>_MM_INSERT_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>INSERTPS</instr>
<desc>Copy a to tmp, then insert a single-precision (32-bit) floating-point element from b into tmp using the control in imm8. Store tmp to dst using the mask in imm8 (elements are zeroed out when the corresponding bit is set). </desc>
<oper>tmp2[127:0] := a[127:0]
CASE (imm8[7:6]) of
0: tmp1[31:0] := b[31:0]
1: tmp1[31:0] := b[63:32]
2: tmp1[31:0] := b[95:64]
3: tmp1[31:0] := b[127:96]
ESAC
CASE (imm8[5:4]) of
0: tmp2[31:0] := tmp1[31:0]
1: tmp2[63:32] := tmp1[31:0]
2: tmp2[95:64] := tmp1[31:0]
3: tmp2[127:96] := tmp1[31:0]
ESAC
FOR j := 0 to 3
	i := j*32
	IF imm8[j%8]
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := tmp2[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2979</id>
<name>_MM_INVCBRT_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cube root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := InvCubeRoot(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2981</id>
<name>_MM_INVCBRT_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cube root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := InvCubeRoot(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2984</id>
<name>_MM_INVSQRT_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := InvSQRT(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2988</id>
<name>_MM_INVSQRT_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := InvSQRT(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2992</id>
<name>_MM_IREM_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3023</id>
<name>_MM_LDDQU_SI128</name>
<cpuid>SSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I_CONST_PTR mem_addr</sign>
<instr>LDDQU</instr>
<desc>Load 128-bits of integer data from unaligned memory into dst. This intrinsic may perform better than _mm_loadu_si128 when the data crosses a cache line boundary.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3025</id>
<name>_MM_LFENCE</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign></sign>
<instr>LFENCE</instr>
<desc>Perform a serializing operation on all load-from-memory instructions that were issued prior to this instruction. Guarantees that every load instruction that precedes, in program order, is globally visible before any load instruction which follows the fence in program order.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>3040</id>
<name>_MM_LOAD_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>MOVAPD</instr>
<desc>Load 128-bits (composed of 2 packed double-precision (64-bit) floating-point elements) from memory into dst.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3049</id>
<name>_MM_LOAD_PD1</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into both elements of dst.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[127:64] := MEM[mem_addr+63:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3050</id>
<name>_MM_LOAD_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>MOVAPS</instr>
<desc>Load 128-bits (composed of 4 packed single-precision (32-bit) floating-point elements) from memory into dst.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3059</id>
<name>_MM_LOAD_PS1</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load a single-precision (32-bit) floating-point element from memory into all elements of dst.</desc>
<oper>dst[31:0] := MEM[mem_addr+31:mem_addr]
dst[63:32] := MEM[mem_addr+31:mem_addr]
dst[95:64] := MEM[mem_addr+31:mem_addr]
dst[127:96] := MEM[mem_addr+31:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3060</id>
<name>_MM_LOAD_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>MOVSD</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into the lower of dst, and zero the upper element. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[127:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3063</id>
<name>_MM_LOAD_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I_CONST_PTR mem_addr</sign>
<instr>MOVDQA</instr>
<desc>Load 128-bits of integer data from memory into dst. 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3066</id>
<name>_MM_LOAD_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>MOVSS</instr>
<desc>Load a single-precision (32-bit) floating-point element from memory into the lower of dst, and zero the upper 3 elements. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[31:0] := MEM[mem_addr+31:mem_addr]
dst[127:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3069</id>
<name>_MM_LOAD1_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into both elements of dst.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[127:64] := MEM[mem_addr+63:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3070</id>
<name>_MM_LOAD1_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load a single-precision (32-bit) floating-point element from memory into all elements of dst.</desc>
<oper>dst[31:0] := MEM[mem_addr+31:mem_addr]
dst[63:32] := MEM[mem_addr+31:mem_addr]
dst[95:64] := MEM[mem_addr+31:mem_addr]
dst[127:96] := MEM[mem_addr+31:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3074</id>
<name>_MM_LOADDUP_PD</name>
<cpuid>SSE3</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>MOVDDUP</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into both elements of dst.
	</desc>
<oper>tmp[63:0] := MEM[mem_addr+63:mem_addr]
tmp[127:64] := MEM[mem_addr+63:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3075</id>
<name>_MM_LOADH_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,DOUBLE_CONST_PTR mem_addr</sign>
<instr>MOVHPD</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into the upper element of dst, and copy the lower element from a to dst. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := MEM[mem_addr+63:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3076</id>
<name>_MM_LOADH_PI</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M64_CONST_PTR mem_addr</sign>
<instr>MOVHPS</instr>
<desc>Load 2 single-precision (32-bit) floating-point elements from memory into the upper 2 elements of dst, and copy the lower 2 elements from a to dst. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[31:0] := a[31:0]
dst[63:32] := a[63:32]
dst[95:64] := MEM[mem_addr+31:mem_addr]
dst[127:96] := MEM[mem_addr+63:mem_addr+32]</oper>
</intrinsic>
<intrinsic>
<id>3077</id>
<name>_MM_LOADL_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I_CONST_PTR mem_addr</sign>
<instr>MOVQ</instr>
<desc>Load 64-bit integer from memory into the first element of dst.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3078</id>
<name>_MM_LOADL_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,DOUBLE_CONST_PTR mem_addr</sign>
<instr>MOVLPD</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into the lower element of dst, and copy the upper element from a to dst. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>3079</id>
<name>_MM_LOADL_PI</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M64_CONST_PTR mem_addr</sign>
<instr>MOVLPS</instr>
<desc>Load 2 single-precision (32-bit) floating-point elements from memory into the lower 2 elements of dst, and copy the upper 2 elements from a to dst. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[31:0] := MEM[mem_addr+31:mem_addr]
dst[63:32] := MEM[mem_addr+63:mem_addr+32]
dst[95:64] := a[95:64]
dst[127:96] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>3080</id>
<name>_MM_LOADR_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load 2 double-precision (64-bit) floating-point elements from memory into dst in reverse order. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[63:0] := MEM[mem_addr+127:mem_addr+64]
dst[127:64] := MEM[mem_addr+63:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3081</id>
<name>_MM_LOADR_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load 4 single-precision (32-bit) floating-point elements from memory into dst in reverse order. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[31:0] := MEM[mem_addr+127:mem_addr+96]
dst[63:32] := MEM[mem_addr+95:mem_addr+64]
dst[95:64] := MEM[mem_addr+63:mem_addr+32]
dst[127:96] := MEM[mem_addr+31:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3106</id>
<name>_MM_LOADU_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>MOVUPD</instr>
<desc>Load 128-bits (composed of 2 packed double-precision (64-bit) floating-point elements) from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3115</id>
<name>_MM_LOADU_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>MOVUPS</instr>
<desc>Load 128-bits (composed of 4 packed single-precision (32-bit) floating-point elements) from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3124</id>
<name>_MM_LOADU_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I_CONST_PTR mem_addr</sign>
<instr>MOVDQU</instr>
<desc>Load 128-bits of integer data from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>3125</id>
<name>_MM_LOADU_SI16</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>NONE</instr>
<desc>Load unaligned 16-bit integer from memory into the first element of dst.</desc>
<oper>dst[15:0] := MEM[mem_addr+15:mem_addr]
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3126</id>
<name>_MM_LOADU_SI16</name>
<cpuid></cpuid>
<ret>__m128d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>MOVZWL+MOVD</instr>
<desc>Load unaligned 16-bit integer from memory into the first element of dst.</desc>
<oper>dst[15:0] := MEM[mem_addr+15:mem_addr]
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3128</id>
<name>_MM_LOADU_SI32</name>
<cpuid>SSE</cpuid>
<ret>__m128d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>MOVD</instr>
<desc>Load unaligned 32-bit integer from memory into the first element of dst.</desc>
<oper>dst[31:0] := MEM[mem_addr+31:mem_addr]
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3129</id>
<name>_MM_LOADU_SI32</name>
<cpuid></cpuid>
<ret>__m128d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>MOVD</instr>
<desc>Load unaligned 32-bit integer from memory into the first element of dst.</desc>
<oper>dst[31:0] := MEM[mem_addr+31:mem_addr]
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3131</id>
<name>_MM_LOADU_SI64</name>
<cpuid></cpuid>
<ret>__m128d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>MOVQ</instr>
<desc>Load unaligned 64-bit integer from memory into the first element of dst.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3132</id>
<name>_MM_LOADU_SI64</name>
<cpuid>SSE</cpuid>
<ret>__m128d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>MOVQ</instr>
<desc>Load unaligned 64-bit integer from memory into the first element of dst.</desc>
<oper>dst[63:0] := MEM[mem_addr+63:mem_addr]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3152</id>
<name>_MM_LOG_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ln(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3156</id>
<name>_MM_LOG_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ln(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3160</id>
<name>_MM_LOG10_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := log10(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3164</id>
<name>_MM_LOG10_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := log10(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3168</id>
<name>_MM_LOG1P_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ln(1.0 + a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3172</id>
<name>_MM_LOG1P_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ln(1.0 + a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3176</id>
<name>_MM_LOG2_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the base-2 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := log2(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3180</id>
<name>_MM_LOG2_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the base-2 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := log2(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3186</id>
<name>_MM_LOGB_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3190</id>
<name>_MM_LOGB_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3196</id>
<name>_MM_LZCNT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	tmp := 31
	dst[i+31:i] := 0
	DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
		tmp := tmp - 1
		dst[i+31:i] := dst[i+31:i] + 1
	OD
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3205</id>
<name>_MM_LZCNT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	tmp := 63
	dst[i+63:i] := 0
	DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
		tmp := tmp - 1
		dst[i+63:i] := dst[i+63:i] + 1
	OD
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3216</id>
<name>_MM_MADD_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMADDWD</instr>
<desc>Multiply packed signed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3225</id>
<name>_MM_MADD_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3226</id>
<name>_MM_MADD52HI_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,__M128I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
	dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3235</id>
<name>_MM_MADD52LO_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,__M128I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
	dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3244</id>
<name>_MM_MADDUBS_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMADDUBSW</instr>
<desc>Vertically multiply each unsigned 8-bit integer from a with the corresponding signed 8-bit integer from b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3253</id>
<name>_MM_MADDUBS_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMADDUBSW</instr>
<desc>Vertically multiply each unsigned 8-bit integer from a with the corresponding signed 8-bit integer from b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3254</id>
<name>_MM_MALLOC</name>
<cpuid>SSE</cpuid>
<ret>void *</ret>
<sign>SIZE_T size,SIZE_T align</sign>
<instr></instr>
<desc>Allocate size bytes of memory, aligned to the alignment specified in align, and return a pointer to the allocated memory. _mm_free should be used to free memory that is allocated with _mm_malloc.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>1</id>
<name>_MM_MASK_ABS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ABS(a[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>10</id>
<name>_MM_MASK_ABS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>19</id>
<name>_MM_MASK_ABS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>28</id>
<name>_MM_MASK_ABS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := ABS(a[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>62</id>
<name>_MM_MASK_ADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] + b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>71</id>
<name>_MM_MASK_ADD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>80</id>
<name>_MM_MASK_ADD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>89</id>
<name>_MM_MASK_ADD_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] + b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>98</id>
<name>_MM_MASK_ADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>110</id>
<name>_MM_MASK_ADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>125</id>
<name>_MM_MASK_ADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VADDSD</instr>
<desc>Add the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] + b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>128</id>
<name>_MM_MASK_ADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VADDSS</instr>
<desc>Add the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] + b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>131</id>
<name>_MM_MASK_ADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VADDSD</instr>
<desc>Add the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. </desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] + b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>135</id>
<name>_MM_MASK_ADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VADDSS</instr>
<desc>Add the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. </desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] + b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>150</id>
<name>_MM_MASK_ADDS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>159</id>
<name>_MM_MASK_ADDS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>168</id>
<name>_MM_MASK_ADDS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>177</id>
<name>_MM_MASK_ADDS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>208</id>
<name>_MM_MASK_ALIGNR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 32-byte immediate result, shift the result right by count 32-bit elements, and store the low 16 bytes (4 elements) in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &amp;gt;&amp;gt; (32*count)
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := temp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>217</id>
<name>_MM_MASK_ALIGNR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 32-byte immediate result, shift the result right by count 64-bit elements, and store the low 16 bytes (2 elements) in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &amp;gt;&amp;gt; (64*count)
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := temp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>226</id>
<name>_MM_MASK_ALIGNR_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[255:0] := ((a[127:0] &amp;lt;&amp;lt; 128) OR b[127:0]) &amp;gt;&amp;gt; (count[7:0]*8)

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>236</id>
<name>_MM_MASK_AND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] AND b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>243</id>
<name>_MM_MASK_AND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] AND b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>251</id>
<name>_MM_MASK_AND_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>260</id>
<name>_MM_MASK_AND_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>272</id>
<name>_MM_MASK_ANDNOT_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>279</id>
<name>_MM_MASK_ANDNOT_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>287</id>
<name>_MM_MASK_ANDNOT_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>296</id>
<name>_MM_MASK_ANDNOT_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>349</id>
<name>_MM_MASK_AVG_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>358</id>
<name>_MM_MASK_AVG_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>385</id>
<name>_MM_MASK_BLEND_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPBLENDMW</instr>
<desc>Blend packed 16-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := b[i+15:i]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>390</id>
<name>_MM_MASK_BLEND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPBLENDMD</instr>
<desc>Blend packed 32-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>394</id>
<name>_MM_MASK_BLEND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPBLENDMQ</instr>
<desc>Blend packed 64-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>397</id>
<name>_MM_MASK_BLEND_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPBLENDMB</instr>
<desc>Blend packed 8-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := b[i+7:i]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>401</id>
<name>_MM_MASK_BLEND_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VBLENDMPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>406</id>
<name>_MM_MASK_BLEND_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VBLENDMPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>457</id>
<name>_MM_MASK_BROADCAST_I32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[n+31:n]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>489</id>
<name>_MM_MASK_BROADCASTB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>498</id>
<name>_MM_MASK_BROADCASTD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>513</id>
<name>_MM_MASK_BROADCASTQ_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>530</id>
<name>_MM_MASK_BROADCASTSS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>539</id>
<name>_MM_MASK_BROADCASTW_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>640</id>
<name>_MM_MASK_CMP_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>646</id>
<name>_MM_MASK_CMP_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>652</id>
<name>_MM_MASK_CMP_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>658</id>
<name>_MM_MASK_CMP_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>664</id>
<name>_MM_MASK_CMP_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>670</id>
<name>_MM_MASK_CMP_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>676</id>
<name>_MM_MASK_CMP_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>682</id>
<name>_MM_MASK_CMP_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>690</id>
<name>_MM_MASK_CMP_PD_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>698</id>
<name>_MM_MASK_CMP_PS_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>708</id>
<name>_MM_MASK_CMP_ROUND_SD_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128D a,__M128D b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k using zeromask k1 (the element is zeroed out when mask bit 0 is not set).
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

IF k1[0]
	k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
ELSE
	k[0] := 0
FI
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>710</id>
<name>_MM_MASK_CMP_ROUND_SS_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128 a,__M128 b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k using zeromask k1 (the element is zeroed out when mask bit 0 is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

IF k1[0]
	k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
ELSE
	k[0] := 0
FI
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>713</id>
<name>_MM_MASK_CMP_SD_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VCMPSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k using zeromask k1 (the element is zeroed out when mask bit 0 is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

IF k1[0]
	k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
ELSE
	k[0] := 0
FI
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>716</id>
<name>_MM_MASK_CMP_SS_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VCMPSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b based on the comparison operand specified by imm8, and store the result in mask vector k using zeromask k1 (the element is zeroed out when mask bit 0 is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC

IF k1[0]
	k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
ELSE
	k[0] := 0
FI
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>720</id>
<name>_MM_MASK_CMPEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>728</id>
<name>_MM_MASK_CMPEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>736</id>
<name>_MM_MASK_CMPEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>744</id>
<name>_MM_MASK_CMPEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>750</id>
<name>_MM_MASK_CMPEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>756</id>
<name>_MM_MASK_CMPEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>762</id>
<name>_MM_MASK_CMPEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>768</id>
<name>_MM_MASK_CMPEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>792</id>
<name>_MM_MASK_CMPGE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>798</id>
<name>_MM_MASK_CMPGE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>804</id>
<name>_MM_MASK_CMPGE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>810</id>
<name>_MM_MASK_CMPGE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>816</id>
<name>_MM_MASK_CMPGE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>822</id>
<name>_MM_MASK_CMPGE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>828</id>
<name>_MM_MASK_CMPGE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>834</id>
<name>_MM_MASK_CMPGE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>846</id>
<name>_MM_MASK_CMPGT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;== b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>854</id>
<name>_MM_MASK_CMPGT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>862</id>
<name>_MM_MASK_CMPGT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>870</id>
<name>_MM_MASK_CMPGT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>876</id>
<name>_MM_MASK_CMPGT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;== b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>882</id>
<name>_MM_MASK_CMPGT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>888</id>
<name>_MM_MASK_CMPGT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>894</id>
<name>_MM_MASK_CMPGT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>914</id>
<name>_MM_MASK_CMPLE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>920</id>
<name>_MM_MASK_CMPLE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>926</id>
<name>_MM_MASK_CMPLE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>932</id>
<name>_MM_MASK_CMPLE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>938</id>
<name>_MM_MASK_CMPLE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>944</id>
<name>_MM_MASK_CMPLE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>950</id>
<name>_MM_MASK_CMPLE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>956</id>
<name>_MM_MASK_CMPLE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>971</id>
<name>_MM_MASK_CMPLT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>978</id>
<name>_MM_MASK_CMPLT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>986</id>
<name>_MM_MASK_CMPLT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>993</id>
<name>_MM_MASK_CMPLT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>999</id>
<name>_MM_MASK_CMPLT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1005</id>
<name>_MM_MASK_CMPLT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1011</id>
<name>_MM_MASK_CMPLT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>1017</id>
<name>_MM_MASK_CMPLT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1031</id>
<name>_MM_MASK_CMPNEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1037</id>
<name>_MM_MASK_CMPNEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1043</id>
<name>_MM_MASK_CMPNEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>1049</id>
<name>_MM_MASK_CMPNEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1055</id>
<name>_MM_MASK_CMPNEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1061</id>
<name>_MM_MASK_CMPNEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1067</id>
<name>_MM_MASK_CMPNEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>1073</id>
<name>_MM_MASK_CMPNEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1140</id>
<name>_MM_MASK_COMPRESS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1146</id>
<name>_MM_MASK_COMPRESS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1152</id>
<name>_MM_MASK_COMPRESS_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1158</id>
<name>_MM_MASK_COMPRESS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1164</id>
<name>_MM_MASK_COMPRESSSTOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 32
m := base_addr
FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1167</id>
<name>_MM_MASK_COMPRESSSTOREU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 64
m := base_addr
FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1170</id>
<name>_MM_MASK_COMPRESSSTOREU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 64
m := base_addr
FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1173</id>
<name>_MM_MASK_COMPRESSSTOREU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 32
m := base_addr
FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1177</id>
<name>_MM_MASK_CONFLICT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit using writemask k (elements are copied from src when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[i]
		FOR l := 0 to j-1
			m := l*32
			dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
		ENDFOR
		dst[i+31:i+j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1186</id>
<name>_MM_MASK_CONFLICT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit using writemask k (elements are copied from src when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		FOR l := 0 to j-1
			m := l*64
			dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
		ENDFOR
		dst[i+63:i+j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1284</id>
<name>_MM_MASK_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1296</id>
<name>_MM_MASK_CVT_ROUNDSD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128D b,INT rounding</sign>
<instr>VCVTSD2SS</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:31]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1306</id>
<name>_MM_MASK_CVT_ROUNDSS_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128 b,INT rounding</sign>
<instr>VCVTSS2SD</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1318</id>
<name>_MM_MASK_CVTEPI16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*16
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1327</id>
<name>_MM_MASK_CVTEPI16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1336</id>
<name>_MM_MASK_CVTEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1344</id>
<name>_MM_MASK_CVTEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1348</id>
<name>_MM_MASK_CVTEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1357</id>
<name>_MM_MASK_CVTEPI32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1366</id>
<name>_MM_MASK_CVTEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1375</id>
<name>_MM_MASK_CVTEPI32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	IF k[j]
		dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
	ELSE
		dst[m+63:m] := src[m+63:m]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1384</id>
<name>_MM_MASK_CVTEPI32_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1392</id>
<name>_MM_MASK_CVTEPI32_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1395</id>
<name>_MM_MASK_CVTEPI32_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1401</id>
<name>_MM_MASK_CVTEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1410</id>
<name>_MM_MASK_CVTEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1419</id>
<name>_MM_MASK_CVTEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1428</id>
<name>_MM_MASK_CVTEPI64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1437</id>
<name>_MM_MASK_CVTEPI64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1445</id>
<name>_MM_MASK_CVTEPI64_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1448</id>
<name>_MM_MASK_CVTEPI64_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1451</id>
<name>_MM_MASK_CVTEPI64_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1455</id>
<name>_MM_MASK_CVTEPI8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := SignExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1464</id>
<name>_MM_MASK_CVTEPI8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in the low 4 bytes of a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1473</id>
<name>_MM_MASK_CVTEPI8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 2 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1482</id>
<name>_MM_MASK_CVTEPU16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1491</id>
<name>_MM_MASK_CVTEPU16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1500</id>
<name>_MM_MASK_CVTEPU32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1509</id>
<name>_MM_MASK_CVTEPU32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1523</id>
<name>_MM_MASK_CVTEPU64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1532</id>
<name>_MM_MASK_CVTEPU64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1541</id>
<name>_MM_MASK_CVTEPU8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := ZeroExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1550</id>
<name>_MM_MASK_CVTEPU8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 4 bytes of a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1559</id>
<name>_MM_MASK_CVTEPU8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 2 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1582</id>
<name>_MM_MASK_CVTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1591</id>
<name>_MM_MASK_CVTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1600</id>
<name>_MM_MASK_CVTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1609</id>
<name>_MM_MASK_CVTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1619</id>
<name>_MM_MASK_CVTPD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1630</id>
<name>_MM_MASK_CVTPH_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1644</id>
<name>_MM_MASK_CVTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1653</id>
<name>_MM_MASK_CVTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1662</id>
<name>_MM_MASK_CVTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1671</id>
<name>_MM_MASK_CVTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1685</id>
<name>_MM_MASK_CVTPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1707</id>
<name>_MM_MASK_CVTSD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128D b</sign>
<instr>VCVTSD2SS</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	</desc>
<oper>IF k[0]
	dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:31]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1712</id>
<name>_MM_MASK_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1720</id>
<name>_MM_MASK_CVTSEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1724</id>
<name>_MM_MASK_CVTSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1733</id>
<name>_MM_MASK_CVTSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1741</id>
<name>_MM_MASK_CVTSEPI32_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1744</id>
<name>_MM_MASK_CVTSEPI32_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1748</id>
<name>_MM_MASK_CVTSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1757</id>
<name>_MM_MASK_CVTSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1766</id>
<name>_MM_MASK_CVTSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1774</id>
<name>_MM_MASK_CVTSEPI64_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1777</id>
<name>_MM_MASK_CVTSEPI64_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1780</id>
<name>_MM_MASK_CVTSEPI64_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
	FI
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1802</id>
<name>_MM_MASK_CVTSS_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128 b</sign>
<instr>VCVTSS2SD</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	</desc>
<oper>IF k[0]
	dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1848</id>
<name>_MM_MASK_CVTTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1857</id>
<name>_MM_MASK_CVTTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1866</id>
<name>_MM_MASK_CVTTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1875</id>
<name>_MM_MASK_CVTTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1885</id>
<name>_MM_MASK_CVTTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1894</id>
<name>_MM_MASK_CVTTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1903</id>
<name>_MM_MASK_CVTTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed double-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1912</id>
<name>_MM_MASK_CVTTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1939</id>
<name>_MM_MASK_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1947</id>
<name>_MM_MASK_CVTUSEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1951</id>
<name>_MM_MASK_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1960</id>
<name>_MM_MASK_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1968</id>
<name>_MM_MASK_CVTUSEPI32_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1971</id>
<name>_MM_MASK_CVTUSEPI32_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1975</id>
<name>_MM_MASK_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1984</id>
<name>_MM_MASK_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1993</id>
<name>_MM_MASK_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2001</id>
<name>_MM_MASK_CVTUSEPI64_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>2004</id>
<name>_MM_MASK_CVTUSEPI64_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2007</id>
<name>_MM_MASK_CVTUSEPI64_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	FI
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2011</id>
<name>_MM_MASK_DBSAD_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>tmp[31:0] := select(b[127:0], imm8[1:0])
tmp[63:32] := select(b[127:0], imm8[3:2])
tmp[95:64] := select(b[127:0], imm8[5:4])
tmp[127:96] := select(b[127:0], imm8[7:6])

FOR j := 0 to 1
	i := j*64
	tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2048</id>
<name>_MM_MASK_DIV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2057</id>
<name>_MM_MASK_DIV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2072</id>
<name>_MM_MASK_DIV_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VDIVSD</instr>
<desc>Divide the lower double-precision (64-bit) floating-point element in a by the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. 
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] / b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2075</id>
<name>_MM_MASK_DIV_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VDIVSS</instr>
<desc>Divide the lower single-precision (32-bit) floating-point element in a by the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] / b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2078</id>
<name>_MM_MASK_DIV_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VDIVSD</instr>
<desc>Divide the lower double-precision (64-bit) floating-point element in a by the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. </desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] / b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2081</id>
<name>_MM_MASK_DIV_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VDIVSS</instr>
<desc>Divide the lower single-precision (32-bit) floating-point element in a by the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. </desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] / b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2158</id>
<name>_MM_MASK_EXPAND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2164</id>
<name>_MM_MASK_EXPAND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2170</id>
<name>_MM_MASK_EXPAND_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2176</id>
<name>_MM_MASK_EXPAND_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2182</id>
<name>_MM_MASK_EXPANDLOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2188</id>
<name>_MM_MASK_EXPANDLOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2194</id>
<name>_MM_MASK_EXPANDLOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2200</id>
<name>_MM_MASK_EXPANDLOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2313</id>
<name>_MM_MASK_FIXUPIMM_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2322</id>
<name>_MM_MASK_FIXUPIMM_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2337</id>
<name>_MM_MASK_FIXUPIMM_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMSD</instr>
<desc>Fix up the lower double-precision (64-bit) floating-point elements in a and b using the lower 64-bit integer in c, store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

IF k[0]
	dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2340</id>
<name>_MM_MASK_FIXUPIMM_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMSS</instr>
<desc>Fix up the lower single-precision (32-bit) floating-point elements in a and b using the lower 32-bit integer in c, store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

IF k[0]
	dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2343</id>
<name>_MM_MASK_FIXUPIMM_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMSD</instr>
<desc>Fix up the lower double-precision (64-bit) floating-point elements in a and b using the lower 64-bit integer in c, store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

IF k[0]
	dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2346</id>
<name>_MM_MASK_FIXUPIMM_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMSS</instr>
<desc>Fix up the lower single-precision (32-bit) floating-point elements in a and b using the lower 32-bit integer in c, store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

IF k[0]
	dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2366</id>
<name>_MM_MASK_FMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2378</id>
<name>_MM_MASK_FMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2397</id>
<name>_MM_MASK_FMADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c,INT rounding</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2400</id>
<name>_MM_MASK_FMADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c,INT rounding</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2404</id>
<name>_MM_MASK_FMADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2408</id>
<name>_MM_MASK_FMADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2418</id>
<name>_MM_MASK_FMADDSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2430</id>
<name>_MM_MASK_FMADDSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2450</id>
<name>_MM_MASK_FMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2462</id>
<name>_MM_MASK_FMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2481</id>
<name>_MM_MASK_FMSUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c,INT rounding</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2484</id>
<name>_MM_MASK_FMSUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c,INT rounding</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2488</id>
<name>_MM_MASK_FMSUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2492</id>
<name>_MM_MASK_FMSUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2496</id>
<name>_MM_MASK_FMSUBADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1 
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2508</id>
<name>_MM_MASK_FMSUBADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2528</id>
<name>_MM_MASK_FNMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2540</id>
<name>_MM_MASK_FNMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2559</id>
<name>_MM_MASK_FNMADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c,INT rounding</sign>
<instr>VFNMADD132SD, VFNMADD213SD, VFNMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2562</id>
<name>_MM_MASK_FNMADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c,INT rounding</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2566</id>
<name>_MM_MASK_FNMADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFNMADD132SD, VFNMADD213SD, VFNMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2570</id>
<name>_MM_MASK_FNMADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from a when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2574</id>
<name>_MM_MASK_FNMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2586</id>
<name>_MM_MASK_FNMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2605</id>
<name>_MM_MASK_FNMSUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c,INT rounding</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2608</id>
<name>_MM_MASK_FNMSUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c,INT rounding</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2612</id>
<name>_MM_MASK_FNMSUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128D b,__M128D c</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2616</id>
<name>_MM_MASK_FNMSUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128 b,__M128 c</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2620</id>
<name>_MM_MASK_FPCLASS_PD_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128D a,INT imm8</sign>
<instr>VFPCLASSPD</instr>
<desc>Test packed double-precision (64-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2626</id>
<name>_MM_MASK_FPCLASS_PS_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128 a,INT imm8</sign>
<instr>VFPCLASSPS</instr>
<desc>Test packed single-precision (32-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2632</id>
<name>_MM_MASK_FPCLASS_SD_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128D a,INT imm8</sign>
<instr>VFPCLASSSD</instr>
<desc>Test the lower double-precision (64-bit) floating-point element in a for special categories specified by imm8, and store the result in mask vector k using zeromask k1 (the element is zeroed out when mask bit 0 is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>IF k1[0]
	k[0] := CheckFPClass_FP64(a[63:0], imm8[7:0])
ELSE
	k[0] := 0
FI
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>2634</id>
<name>_MM_MASK_FPCLASS_SS_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128 a,INT imm8</sign>
<instr>VFPCLASSSS</instr>
<desc>Test the lower single-precision (32-bit) floating-point element in a for special categories specified by imm8, and store the result in mask vector k using zeromask k1 (the element is zeroed out when mask bit 0 is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>IF k1[0]
	k[0] := CheckFPClass_FP32(a[31:0], imm8[7:0])
ELSE
	k[0] := 0
FI
k[MAX:1] := 0</oper>
</intrinsic>
<intrinsic>
<id>2648</id>
<name>_MM_MASK_GETEXP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2657</id>
<name>_MM_MASK_GETEXP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2672</id>
<name>_MM_MASK_GETEXP_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VGETEXPSD</instr>
<desc>Convert the exponent of the lower double-precision (64-bit) floating-point element in b to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2675</id>
<name>_MM_MASK_GETEXP_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VGETEXPSS</instr>
<desc>Convert the exponent of the lower single-precision (32-bit) floating-point element in b to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2678</id>
<name>_MM_MASK_GETEXP_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VGETEXPSD</instr>
<desc>Convert the exponent of the lower double-precision (64-bit) floating-point element in b to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.</desc>
<oper>IF k[0]
	dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2681</id>
<name>_MM_MASK_GETEXP_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VGETEXPSS</instr>
<desc>Convert the exponent of the lower single-precision (32-bit) floating-point element in b to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.</desc>
<oper>IF k[0]
	dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2684</id>
<name>_MM_MASK_GETMANT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2693</id>
<name>_MM_MASK_GETMANT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2708</id>
<name>_MM_MASK_GETMANT_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTSD</instr>
<desc>Normalize the mantissas of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2711</id>
<name>_MM_MASK_GETMANT_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTSS</instr>
<desc>Normalize the mantissas of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2714</id>
<name>_MM_MASK_GETMANT_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTSD</instr>
<desc>Normalize the mantissas of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>IF k[0]
	dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2717</id>
<name>_MM_MASK_GETMANT_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTSS</instr>
<desc>Normalize the mantissas of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>IF k[0]
	dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2772</id>
<name>_MM_MASK_I32GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,INT_CONST_PTR base_addr,__M128I vindex,__M128I mask,CONST_INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2780</id>
<name>_MM_MASK_I32GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__INT64_CONST_PTR base_addr,__M128I vindex,__M128I mask,CONST_INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>
	Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	m := j*32
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2788</id>
<name>_MM_MASK_I32GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,DOUBLE_CONST_PTR base_addr,__M128I vindex,__M128D mask,CONST_INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	m := j*32
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2796</id>
<name>_MM_MASK_I32GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>__M128 src,FLOAT_CONST_PTR base_addr,__M128I vindex,__M128 mask,CONST_INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2820</id>
<name>_MM_MASK_I32SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERDD</instr>
<desc>Scatter 32-bit integers from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2826</id>
<name>_MM_MASK_I32SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Scatter 64-bit integers from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2832</id>
<name>_MM_MASK_I32SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128D a,CONST_INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2838</id>
<name>_MM_MASK_I32SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128 a,CONST_INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2860</id>
<name>_MM_MASK_I64GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,INT_CONST_PTR base_addr,__M128I vindex,__M128I mask,CONST_INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>
	Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:64] := 0
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2870</id>
<name>_MM_MASK_I64GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__INT64_CONST_PTR base_addr,__M128I vindex,__M128I mask,CONST_INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>
	Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2878</id>
<name>_MM_MASK_I64GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,DOUBLE_CONST_PTR base_addr,__M128I vindex,__M128D mask,CONST_INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2886</id>
<name>_MM_MASK_I64GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>__M128 src,FLOAT_CONST_PTR base_addr,__M128I vindex,__M128 mask,CONST_INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:64] := 0
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2896</id>
<name>_MM_MASK_I64SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERQD</instr>
<desc>Scatter 32-bit integers from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2904</id>
<name>_MM_MASK_I64SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERQQ</instr>
<desc>Scatter 64-bit integers from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2910</id>
<name>_MM_MASK_I64SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128D a,CONST_INT scale</sign>
<instr>VSCATTERQPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>2916</id>
<name>_MM_MASK_I64SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M128 a,CONST_INT scale</sign>
<instr>VSCATTERQPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>3026</id>
<name>_MM_MASK_LOAD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3033</id>
<name>_MM_MASK_LOAD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load packed 64-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3041</id>
<name>_MM_MASK_LOAD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3051</id>
<name>_MM_MASK_LOAD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3061</id>
<name>_MM_MASK_LOAD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,DOUBLE_CONST_PTR mem_addr</sign>
<instr>VMOVSD</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and set the upper element of dst to zero. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>IF k[0]
	dst[63:0] := MEM[mem_addr+63:mem_addr]
ELSE
	dst[63:0] := src[63:0]
FI
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3067</id>
<name>_MM_MASK_LOAD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,FLOAT_CONST_PTR mem_addr</sign>
<instr>VMOVSS</instr>
<desc>Load a single-precision (32-bit) floating-point element from memory into the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and set the upper elements of dst to zero. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>IF k[0]
	dst[31:0] := MEM[mem_addr+31:mem_addr]
ELSE
	dst[31:0] := src[31:0]
FI
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3082</id>
<name>_MM_MASK_LOADU_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU16</instr>
<desc>Load packed 16-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3088</id>
<name>_MM_MASK_LOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3094</id>
<name>_MM_MASK_LOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU64</instr>
<desc>Load packed 64-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3100</id>
<name>_MM_MASK_LOADU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU8</instr>
<desc>Load packed 8-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3107</id>
<name>_MM_MASK_LOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memoy into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3116</id>
<name>_MM_MASK_LOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3197</id>
<name>_MM_MASK_LZCNT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		tmp := 31
		dst[i+31:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+31:i] := dst[i+31:i] + 1
		OD
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3206</id>
<name>_MM_MASK_LZCNT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp := 63
		dst[i+63:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+63:i] := dst[i+63:i] + 1
		OD
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3217</id>
<name>_MM_MASK_MADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3227</id>
<name>_MM_MASK_MADD52HI_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__MMASK8 k,__M128I b,__M128I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3236</id>
<name>_MM_MASK_MADD52LO_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__MMASK8 k,__M128I b,__M128I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3245</id>
<name>_MM_MASK_MADDUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Multiply packed unsigned 8-bit integers in a by packed signed 8-bit integers in b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3276</id>
<name>_MM_MASK_MAX_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3285</id>
<name>_MM_MASK_MAX_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3294</id>
<name>_MM_MASK_MAX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3303</id>
<name>_MM_MASK_MAX_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3312</id>
<name>_MM_MASK_MAX_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3321</id>
<name>_MM_MASK_MAX_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3330</id>
<name>_MM_MASK_MAX_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3339</id>
<name>_MM_MASK_MAX_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3348</id>
<name>_MM_MASK_MAX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3358</id>
<name>_MM_MASK_MAX_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3374</id>
<name>_MM_MASK_MAX_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT sae</sign>
<instr>VMAXSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3377</id>
<name>_MM_MASK_MAX_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT sae</sign>
<instr>VMAXSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3380</id>
<name>_MM_MASK_MAX_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMAXSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3383</id>
<name>_MM_MASK_MAX_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMAXSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3390</id>
<name>_MM_MASK_MIN_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3399</id>
<name>_MM_MASK_MIN_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3408</id>
<name>_MM_MASK_MIN_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3417</id>
<name>_MM_MASK_MIN_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3426</id>
<name>_MM_MASK_MIN_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3435</id>
<name>_MM_MASK_MIN_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3444</id>
<name>_MM_MASK_MIN_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3453</id>
<name>_MM_MASK_MIN_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3462</id>
<name>_MM_MASK_MIN_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3472</id>
<name>_MM_MASK_MIN_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3488</id>
<name>_MM_MASK_MIN_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT sae</sign>
<instr>VMINSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3491</id>
<name>_MM_MASK_MIN_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT sae</sign>
<instr>VMINSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3494</id>
<name>_MM_MASK_MIN_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMINSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3497</id>
<name>_MM_MASK_MIN_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMINSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3502</id>
<name>_MM_MASK_MOV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQU16</instr>
<desc>Move packed 16-bit integers from a into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3508</id>
<name>_MM_MASK_MOV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQA32</instr>
<desc>Move packed 32-bit integers from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3514</id>
<name>_MM_MASK_MOV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQA64</instr>
<desc>Move packed 64-bit integers from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3520</id>
<name>_MM_MASK_MOV_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a</sign>
<instr>VMOVDQU8</instr>
<desc>Move packed 8-bit integers from a into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3526</id>
<name>_MM_MASK_MOV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VMOVAPD</instr>
<desc>Move packed double-precision (64-bit) floating-point elements from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3532</id>
<name>_MM_MASK_MOV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VMOVAPS</instr>
<desc>Move packed single-precision (32-bit) floating-point elements from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3539</id>
<name>_MM_MASK_MOVE_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMOVSD</instr>
<desc>Move the lower double-precision (64-bit) floating-point element from b to the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3542</id>
<name>_MM_MASK_MOVE_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMOVSS</instr>
<desc>Move the lower single-precision (32-bit) floating-point element from b to the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3545</id>
<name>_MM_MASK_MOVEDUP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3554</id>
<name>_MM_MASK_MOVEHDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3564</id>
<name>_MM_MASK_MOVELDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3609</id>
<name>_MM_MASK_MUL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3618</id>
<name>_MM_MASK_MUL_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3627</id>
<name>_MM_MASK_MUL_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3636</id>
<name>_MM_MASK_MUL_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  RM.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3651</id>
<name>_MM_MASK_MUL_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VMULSD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] * b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3654</id>
<name>_MM_MASK_MUL_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VMULSS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] * b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3657</id>
<name>_MM_MASK_MUL_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMULSD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] * b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3660</id>
<name>_MM_MASK_MUL_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMULSS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] * b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3664</id>
<name>_MM_MASK_MULHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3675</id>
<name>_MM_MASK_MULHI_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3688</id>
<name>_MM_MASK_MULHRS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
		dst[i+15:i] := tmp[16:1]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3698</id>
<name>_MM_MASK_MULLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3707</id>
<name>_MM_MASK_MULLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		tmp[63:0] := a[i+31:i] * b[i+31:i]
		dst[i+31:i] := tmp[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3716</id>
<name>_MM_MASK_MULLO_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp[127:0] := a[i+63:i] * b[i+63:i]
		dst[i+63:i] := tmp[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3728</id>
<name>_MM_MASK_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR i := 0 to 1
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		IF k[i*8+j]
			dst[q+j*8+7:q+j*8] := tmp8[7:0]
		ELSE
			dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
		FI
	ENDFOR
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3742</id>
<name>_MM_MASK_OR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] OR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3749</id>
<name>_MM_MASK_OR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] OR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3756</id>
<name>_MM_MASK_OR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3765</id>
<name>_MM_MASK_OR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3778</id>
<name>_MM_MASK_PACKS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3787</id>
<name>_MM_MASK_PACKS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3817</id>
<name>_MM_MASK_PACKUS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3826</id>
<name>_MM_MASK_PACKUS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3856</id>
<name>_MM_MASK_PERMUTE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3865</id>
<name>_MM_MASK_PERMUTE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3886</id>
<name>_MM_MASK_PERMUTEVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a using the control in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3895</id>
<name>_MM_MASK_PERMUTEVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3918</id>
<name>_MM_MASK_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__MMASK8 k,__M128I idx,__M128I b</sign>
<instr>VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		off := 16*idx[i+2:i]
		dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3930</id>
<name>_MM_MASK_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__MMASK8 k,__M128I idx,__M128I b</sign>
<instr>VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3942</id>
<name>_MM_MASK_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__MMASK8 k,__M128I idx,__M128I b</sign>
<instr>VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	IF k[j]
		dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3954</id>
<name>_MM_MASK_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__MMASK16 k,__M128I idx,__M128I b</sign>
<instr>VPERMT2B</instr>
<desc>Shuffle 8-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		off := 8*idx[i+3:i]
		dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3966</id>
<name>_MM_MASK_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__MMASK8 k,__M128I idx,__M128D b</sign>
<instr>VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	IF k[j]
		dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3978</id>
<name>_MM_MASK_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__MMASK8 k,__M128I idx,__M128 b</sign>
<instr>VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3990</id>
<name>_MM_MASK_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I idx,__M128I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	id := idx[i+2:i]*16
	IF k[j]
		dst[i+15:i] := a[id+15:id]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4011</id>
<name>_MM_MASK_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I idx,__M128I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	id := idx[i+3:i]*8
	IF k[j]
		dst[i+7:i] := a[id+7:id]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4113</id>
<name>_MM_MASK_RANGE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4122</id>
<name>_MM_MASK_RANGE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4137</id>
<name>_MM_MASK_RANGE_ROUND_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT imm8,INT rounding</sign>
<instr>VRANGESD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4140</id>
<name>_MM_MASK_RANGE_ROUND_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT imm8,INT rounding</sign>
<instr>VRANGESS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[31:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4143</id>
<name>_MM_MASK_RANGE_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT imm8</sign>
<instr>VRANGESD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4145</id>
<name>_MM_MASK_RANGE_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT imm8</sign>
<instr>VRANGESS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[31:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4150</id>
<name>_MM_MASK_RCP14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4159</id>
<name>_MM_MASK_RCP14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4168</id>
<name>_MM_MASK_RCP14_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRCP14SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[63:0] := APPROXIMATE(1.0/b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4171</id>
<name>_MM_MASK_RCP14_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRCP14SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[31:0] := APPROXIMATE(1.0/b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4188</id>
<name>_MM_MASK_RCP28_ROUND_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VRCP28SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[63:0] := RCP_28_DP(1.0/b[63:0];
ELSE
	dst[63:0] := src[63:0];
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4191</id>
<name>_MM_MASK_RCP28_ROUND_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VRCP28SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[31:0] := RCP_28_DP(1.0/b[31:0];
ELSE
	dst[31:0] := src[31:0];
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4194</id>
<name>_MM_MASK_RCP28_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRCP28SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[63:0] := RCP_28_DP(1.0/b[63:0];
ELSE
	dst[63:0] := src[63:0];
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4197</id>
<name>_MM_MASK_RCP28_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRCP28SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[31:0] := RCP_28_DP(1.0/b[31:0];
ELSE
	dst[31:0] := src[31:0];
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4273</id>
<name>_MM_MASK_REDUCE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4282</id>
<name>_MM_MASK_REDUCE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4297</id>
<name>_MM_MASK_REDUCE_ROUND_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT imm8,INT rounding</sign>
<instr>VREDUCESD</instr>
<desc>Extract the reduced argument of the lower double-precision (64-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

IF k[0]
	dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4300</id>
<name>_MM_MASK_REDUCE_ROUND_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT imm8,INT rounding</sign>
<instr>VREDUCESS</instr>
<desc>Extract the reduced argument of the lower single-precision (32-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}

IF k[0]
	dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:64] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4303</id>
<name>_MM_MASK_REDUCE_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT imm8</sign>
<instr>VREDUCESD</instr>
<desc>Extract the reduced argument of the lower double-precision (64-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

IF k[0]
	dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4306</id>
<name>_MM_MASK_REDUCE_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT imm8</sign>
<instr>VREDUCESS</instr>
<desc>Extract the reduced argument of the lower single-precision (32-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}

IF k[0]
	dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:64] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4339</id>
<name>_MM_MASK_ROL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4348</id>
<name>_MM_MASK_ROL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4357</id>
<name>_MM_MASK_ROLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4366</id>
<name>_MM_MASK_ROLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4375</id>
<name>_MM_MASK_ROR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4384</id>
<name>_MM_MASK_ROR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4393</id>
<name>_MM_MASK_RORV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4402</id>
<name>_MM_MASK_RORV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4427</id>
<name>_MM_MASK_ROUNDSCALE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4436</id>
<name>_MM_MASK_ROUNDSCALE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4451</id>
<name>_MM_MASK_ROUNDSCALE_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,CONST_INT imm8,CONST_INT rounding</sign>
<instr>VRNDSCALESD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}		

IF k[0]
	dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4454</id>
<name>_MM_MASK_ROUNDSCALE_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,CONST_INT imm8,CONST_INT rounding</sign>
<instr>VRNDSCALESS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}

IF k[0]
	dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4457</id>
<name>_MM_MASK_ROUNDSCALE_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VRNDSCALESD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}		

IF k[0]
	dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4460</id>
<name>_MM_MASK_ROUNDSCALE_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VRNDSCALESS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}

IF k[0]
	dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4466</id>
<name>_MM_MASK_RSQRT14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4473</id>
<name>_MM_MASK_RSQRT14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4480</id>
<name>_MM_MASK_RSQRT14_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRSQRT14SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4483</id>
<name>_MM_MASK_RSQRT14_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRSQRT14SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4500</id>
<name>_MM_MASK_RSQRT28_ROUND_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VRSQRT28SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[63:0] := (1.0/SQRT(b[63:0]));
ELSE
	dst[63:0] := src[63:0];
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4503</id>
<name>_MM_MASK_RSQRT28_ROUND_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VRSQRT28SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[31:0] := (1.0/SQRT(b[31:0]));
ELSE
	dst[31:0] := src[31:0];
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4506</id>
<name>_MM_MASK_RSQRT28_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRSQRT28SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[63:0] := (1.0/SQRT(b[63:0]));
ELSE
	dst[63:0] := src[63:0];
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4509</id>
<name>_MM_MASK_RSQRT28_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRSQRT28SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[31:0] := (1.0/SQRT(b[31:0]));
ELSE
	dst[31:0] := src[31:0];
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4524</id>
<name>_MM_MASK_SCALEF_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4533</id>
<name>_MM_MASK_SCALEF_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4548</id>
<name>_MM_MASK_SCALEF_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VSCALEFSD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4551</id>
<name>_MM_MASK_SCALEF_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VSCALEFSS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4554</id>
<name>_MM_MASK_SCALEF_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSCALEFSD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4557</id>
<name>_MM_MASK_SCALEF_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSCALEFSS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4591</id>
<name>_MM_MASK_SET1_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,SHORT a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4600</id>
<name>_MM_MASK_SET1_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4609</id>
<name>_MM_MASK_SET1_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4619</id>
<name>_MM_MASK_SET1_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,CHAR a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast 8-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4688</id>
<name>_MM_MASK_SHUFFLE_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4697</id>
<name>_MM_MASK_SHUFFLE_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF b[i+7] == 1
			dst[i+7:i] := 0
		ELSE
			index[3:0] := b[i+3:i]
			dst[i+7:i] := a[index*8+7:index*8]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4730</id>
<name>_MM_MASK_SHUFFLE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4741</id>
<name>_MM_MASK_SHUFFLE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4750</id>
<name>_MM_MASK_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of a using the control in imm8. Store the results in the high 64 bits of dst, with the low 64 bits being copied from from a to dst, using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4759</id>
<name>_MM_MASK_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of a using the control in imm8. Store the results in the low 64 bits of dst, with the high 64 bits being copied from from a to dst, using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4809</id>
<name>_MM_MASK_SLL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4818</id>
<name>_MM_MASK_SLL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4827</id>
<name>_MM_MASK_SLL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4839</id>
<name>_MM_MASK_SLLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4848</id>
<name>_MM_MASK_SLLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4857</id>
<name>_MM_MASK_SLLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4871</id>
<name>_MM_MASK_SLLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4880</id>
<name>_MM_MASK_SLLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4889</id>
<name>_MM_MASK_SLLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4900</id>
<name>_MM_MASK_SQRT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4909</id>
<name>_MM_MASK_SQRT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4924</id>
<name>_MM_MASK_SQRT_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VSQRTSD</instr>
<desc>Compute the square root of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := SQRT(a[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4927</id>
<name>_MM_MASK_SQRT_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VSQRTSS</instr>
<desc>Compute the square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := SQRT(a[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4930</id>
<name>_MM_MASK_SQRT_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSQRTSD</instr>
<desc>Compute the square root of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := SQRT(a[63:0])
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4933</id>
<name>_MM_MASK_SQRT_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSQRTSS</instr>
<desc>Compute the square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := SQRT(a[31:0])
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4936</id>
<name>_MM_MASK_SRA_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4945</id>
<name>_MM_MASK_SRA_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4954</id>
<name>_MM_MASK_SRA_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4965</id>
<name>_MM_MASK_SRAI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4974</id>
<name>_MM_MASK_SRAI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4983</id>
<name>_MM_MASK_SRAI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4994</id>
<name>_MM_MASK_SRAV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5003</id>
<name>_MM_MASK_SRAV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5012</id>
<name>_MM_MASK_SRAV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5021</id>
<name>_MM_MASK_SRL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5030</id>
<name>_MM_MASK_SRL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5039</id>
<name>_MM_MASK_SRL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5051</id>
<name>_MM_MASK_SRLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5060</id>
<name>_MM_MASK_SRLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5069</id>
<name>_MM_MASK_SRLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5083</id>
<name>_MM_MASK_SRLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5092</id>
<name>_MM_MASK_SRLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5101</id>
<name>_MM_MASK_SRLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5110</id>
<name>_MM_MASK_STORE_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQA32</instr>
<desc>Store packed 32-bit integers from a into memory using writemask k.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5114</id>
<name>_MM_MASK_STORE_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQA64</instr>
<desc>Store packed 64-bit integers from a into memory using writemask k.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5118</id>
<name>_MM_MASK_STORE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128D a</sign>
<instr>VMOVAPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using writemask k.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5125</id>
<name>_MM_MASK_STORE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128 a</sign>
<instr>VMOVAPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using writemask k.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5132</id>
<name>_MM_MASK_STORE_SD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__MMASK8 k,__M128D a</sign>
<instr>VMOVSD</instr>
<desc>Store the lower double-precision (64-bit) floating-point element from a into memory using writemask k.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>IF k[0]
	MEM[mem_addr+63:mem_addr] := a[63:0]
FI</oper>
</intrinsic>
<intrinsic>
<id>5137</id>
<name>_MM_MASK_STORE_SS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__MMASK8 k,__M128 a</sign>
<instr>VMOVSS</instr>
<desc>Store the lower single-precision (32-bit) floating-point element from a into memory using writemask k.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>IF k[0]
	MEM[mem_addr+31:mem_addr] := a[31:0]
FI</oper>
</intrinsic>
<intrinsic>
<id>5155</id>
<name>_MM_MASK_STOREU_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQU16</instr>
<desc>Store packed 16-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5158</id>
<name>_MM_MASK_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQU32</instr>
<desc>Store packed 32-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5161</id>
<name>_MM_MASK_STOREU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128I a</sign>
<instr>VMOVDQU64</instr>
<desc>Store packed 64-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5164</id>
<name>_MM_MASK_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK16 k,__M128I a</sign>
<instr>VMOVDQU8</instr>
<desc>Store packed 8-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5167</id>
<name>_MM_MASK_STOREU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128D a</sign>
<instr>VMOVUPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5173</id>
<name>_MM_MASK_STOREU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M128 a</sign>
<instr>VMOVUPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5206</id>
<name>_MM_MASK_SUB_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] - b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5215</id>
<name>_MM_MASK_SUB_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5224</id>
<name>_MM_MASK_SUB_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5233</id>
<name>_MM_MASK_SUB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] - b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5242</id>
<name>_MM_MASK_SUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5254</id>
<name>_MM_MASK_SUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5269</id>
<name>_MM_MASK_SUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VSUBSD</instr>
<desc>Subtract the lower double-precision (64-bit) floating-point element in b from the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] - b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5272</id>
<name>_MM_MASK_SUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VSUBSS</instr>
<desc>Subtract the lower single-precision (32-bit) floating-point element in b from the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] - b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5275</id>
<name>_MM_MASK_SUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSUBSD</instr>
<desc>Subtract the lower double-precision (64-bit) floating-point element in b from the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] - b[63:0]
ELSE
	dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5279</id>
<name>_MM_MASK_SUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSUBSS</instr>
<desc>Subtract the lower single-precision (32-bit) floating-point element in b from the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using writemask k (the element is copied from src when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] - b[31:0]
ELSE
	dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5296</id>
<name>_MM_MASK_SUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5305</id>
<name>_MM_MASK_SUBS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5314</id>
<name>_MM_MASK_SUBS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5323</id>
<name>_MM_MASK_SUBS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5388</id>
<name>_MM_MASK_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from src, a, and b are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using writemask k at 32-bit granularity (32-bit elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		FOR h := 0 to 31
			index[2:0] := (src[i+h] &amp;lt;&amp;lt; 2) OR (a[i+h] &amp;lt;&amp;lt; 1) OR b[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5397</id>
<name>_MM_MASK_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from src, a, and b are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using writemask k at 64-bit granularity (64-bit elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		FOR h := 0 to 63
			index[2:0] := (src[i+h] &amp;lt;&amp;lt; 2) OR (a[i+h] &amp;lt;&amp;lt; 1) OR b[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5408</id>
<name>_MM_MASK_TEST_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPTESTMW</instr>
<desc>Compute the bitwise AND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5414</id>
<name>_MM_MASK_TEST_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPTESTMD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5420</id>
<name>_MM_MASK_TEST_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPTESTMQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>5426</id>
<name>_MM_MASK_TEST_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPTESTMB</instr>
<desc>Compute the bitwise AND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5439</id>
<name>_MM_MASK_TESTN_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPTESTNMW</instr>
<desc>Compute the bitwise NAND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k1[j]
		k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5445</id>
<name>_MM_MASK_TESTN_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPTESTNMD</instr>
<desc>Compute the bitwise NAND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k1[j]
		k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5451</id>
<name>_MM_MASK_TESTN_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M128I a,__M128I b</sign>
<instr>VPTESTNMQ</instr>
<desc>Compute the bitwise NAND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k1[j]
		k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>5457</id>
<name>_MM_MASK_TESTN_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M128I a,__M128I b</sign>
<instr>VPTESTNMB</instr>
<desc>Compute the bitwise NAND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k1[j]
		k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5518</id>
<name>_MM_MASK_UNPACKHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5527</id>
<name>_MM_MASK_UNPACKHI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5536</id>
<name>_MM_MASK_UNPACKHI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5545</id>
<name>_MM_MASK_UNPACKHI_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5554</id>
<name>_MM_MASK_UNPACKHI_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5566</id>
<name>_MM_MASK_UNPACKHI_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5575</id>
<name>_MM_MASK_UNPACKLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5584</id>
<name>_MM_MASK_UNPACKLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5593</id>
<name>_MM_MASK_UNPACKLO_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5602</id>
<name>_MM_MASK_UNPACKLO_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5611</id>
<name>_MM_MASK_UNPACKLO_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5623</id>
<name>_MM_MASK_UNPACKLO_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5642</id>
<name>_MM_MASK_XOR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5649</id>
<name>_MM_MASK_XOR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5656</id>
<name>_MM_MASK_XOR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5665</id>
<name>_MM_MASK_XOR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3919</id>
<name>_MM_MASK2_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__MMASK8 k,__M128I b</sign>
<instr>VPERMI2W</instr>
<desc>Shuffle 16-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		off := 16*idx[i+2:i]
		dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := idx[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3931</id>
<name>_MM_MASK2_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__MMASK8 k,__M128I b</sign>
<instr>VPERMI2D</instr>
<desc>Shuffle 32-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := idx[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3943</id>
<name>_MM_MASK2_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__MMASK8 k,__M128I b</sign>
<instr>VPERMI2Q</instr>
<desc>Shuffle 64-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	IF k[j]
		dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := idx[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3955</id>
<name>_MM_MASK2_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__MMASK16 k,__M128I b</sign>
<instr>VPERMI2B</instr>
<desc>Shuffle 8-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		off := 8*idx[i+3:i]
		dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3967</id>
<name>_MM_MASK2_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128I idx,__MMASK8 k,__M128D b</sign>
<instr>VPERMI2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set)</desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	IF k[j]
		dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := idx[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3979</id>
<name>_MM_MASK2_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128I idx,__MMASK8 k,__M128 b</sign>
<instr>VPERMI2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := idx[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2367</id>
<name>_MM_MASK3_FMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2379</id>
<name>_MM_MASK3_FMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2398</id>
<name>_MM_MASK3_FMADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k,INT rounding</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2401</id>
<name>_MM_MASK3_FMADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k,INT rounding</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2405</id>
<name>_MM_MASK3_FMADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2409</id>
<name>_MM_MASK3_FMADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2419</id>
<name>_MM_MASK3_FMADDSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2431</id>
<name>_MM_MASK3_FMADDSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2451</id>
<name>_MM_MASK3_FMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2463</id>
<name>_MM_MASK3_FMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2482</id>
<name>_MM_MASK3_FMSUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k,INT rounding</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2485</id>
<name>_MM_MASK3_FMSUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k,INT rounding</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2489</id>
<name>_MM_MASK3_FMSUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2493</id>
<name>_MM_MASK3_FMSUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2497</id>
<name>_MM_MASK3_FMSUBADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2509</id>
<name>_MM_MASK3_FMSUBADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2529</id>
<name>_MM_MASK3_FNMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2541</id>
<name>_MM_MASK3_FNMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2560</id>
<name>_MM_MASK3_FNMADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k,INT rounding</sign>
<instr>VFNMADD132SD, VFNMADD213SD, VFNMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2563</id>
<name>_MM_MASK3_FNMADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k,INT rounding</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2567</id>
<name>_MM_MASK3_FNMADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFNMADD132SD, VFNMADD213SD, VFNMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2571</id>
<name>_MM_MASK3_FNMADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2575</id>
<name>_MM_MASK3_FNMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2587</id>
<name>_MM_MASK3_FNMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2606</id>
<name>_MM_MASK3_FNMSUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k,INT rounding</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2609</id>
<name>_MM_MASK3_FNMSUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k,INT rounding</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, subtract the lower element in c from the negated intermediate result, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2613</id>
<name>_MM_MASK3_FNMSUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,__M128D c,__MMASK8 k</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using writemask k (the element is copied from c when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := c[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2617</id>
<name>_MM_MASK3_FNMSUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,__M128 c,__MMASK8 k</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst, and copy the upper element from a to the upper element of dst using writemask k (elements are copied from c when the corresponding mask bit is not set).</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := c[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3257</id>
<name>_MM_MASKLOAD_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>INT_CONST_PTR mem_addr,__M128I mask</sign>
<instr>VPMASKMOVD</instr>
<desc>Load packed 32-bit integers from memory into dst using mask (elements are zeroed out when the highest bit is not set in the corresponding element).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3259</id>
<name>_MM_MASKLOAD_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__INT64_CONST_PTR mem_addr,__M128I mask</sign>
<instr>VPMASKMOVQ</instr>
<desc>Load packed 64-bit integers from memory into dst using mask (elements are zeroed out when the highest bit is not set in the corresponding element).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3261</id>
<name>_MM_MASKLOAD_PD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE_CONST_PTR mem_addr,__M128I mask</sign>
<instr>VMASKMOVPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using mask (elements are zeroed out when the high bit of the corresponding element is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3263</id>
<name>_MM_MASKLOAD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR mem_addr,__M128I mask</sign>
<instr>VMASKMOVPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using mask (elements are zeroed out when the high bit of the corresponding element is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3265</id>
<name>_MM_MASKMOVE_SI64</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>__M64 a,__M64 mask,CHAR_PTR mem_addr</sign>
<instr>MASKMOVQ</instr>
<desc>Conditionally store 8-bit integer elements from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element) and a non-temporal memory hint.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF mask[i+7]
		MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3266</id>
<name>_MM_MASKMOVEU_SI128</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>__M128I a,__M128I mask,CHAR_PTR mem_addr</sign>
<instr>MASKMOVDQU</instr>
<desc>Conditionally store 8-bit integer elements from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element) and a non-temporal memory hint. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF mask[i+7]
		MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3268</id>
<name>_MM_MASKSTORE_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>void</ret>
<sign>INT_PTR mem_addr,__M128I mask,__M128I a</sign>
<instr>VPMASKMOVD</instr>
<desc>Store packed 32-bit integers from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element).
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3270</id>
<name>_MM_MASKSTORE_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>void</ret>
<sign>__INT64_PTR mem_addr,__M128I mask,__M128I a</sign>
<instr>VPMASKMOVQ</instr>
<desc>Store packed 64-bit integers from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element).
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3272</id>
<name>_MM_MASKSTORE_PD</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128I mask,__M128D a</sign>
<instr>VMASKMOVPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using mask.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF mask[i+63]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3274</id>
<name>_MM_MASKSTORE_PS</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128I mask,__M128 a</sign>
<instr>VMASKMOVPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using mask.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF mask[i+31]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2</id>
<name>_MM_MASKZ_ABS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ABS(a[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>11</id>
<name>_MM_MASKZ_ABS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>20</id>
<name>_MM_MASKZ_ABS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>29</id>
<name>_MM_MASKZ_ABS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := ABS(a[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>63</id>
<name>_MM_MASKZ_ADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] + b[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>72</id>
<name>_MM_MASKZ_ADD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>81</id>
<name>_MM_MASKZ_ADD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>90</id>
<name>_MM_MASKZ_ADD_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] + b[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>99</id>
<name>_MM_MASKZ_ADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>111</id>
<name>_MM_MASKZ_ADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>126</id>
<name>_MM_MASKZ_ADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VADDSD</instr>
<desc>Add the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] + b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>129</id>
<name>_MM_MASKZ_ADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VADDSS</instr>
<desc>Add the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] + b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>132</id>
<name>_MM_MASKZ_ADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VADDSD</instr>
<desc>Add the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] + b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>136</id>
<name>_MM_MASKZ_ADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VADDSS</instr>
<desc>Add the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] + b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>151</id>
<name>_MM_MASKZ_ADDS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>160</id>
<name>_MM_MASKZ_ADDS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>169</id>
<name>_MM_MASKZ_ADDS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>178</id>
<name>_MM_MASKZ_ADDS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>209</id>
<name>_MM_MASKZ_ALIGNR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 32-byte immediate result, shift the result right by count 32-bit elements, and store the low 16 bytes (4 elements) in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &amp;gt;&amp;gt; (32*count)
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := temp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>218</id>
<name>_MM_MASKZ_ALIGNR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 32-byte immediate result, shift the result right by count 64-bit elements, and store the low 16 bytes (2 elements) in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &amp;gt;&amp;gt; (64*count)
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := temp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>227</id>
<name>_MM_MASKZ_ALIGNR_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[255:0] := ((a[127:0] &amp;lt;&amp;lt; 128) OR b[127:0]) &amp;gt;&amp;gt; (count[7:0]*8)

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>237</id>
<name>_MM_MASKZ_AND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] AND b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>244</id>
<name>_MM_MASKZ_AND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] AND b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>252</id>
<name>_MM_MASKZ_AND_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>261</id>
<name>_MM_MASKZ_AND_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>273</id>
<name>_MM_MASKZ_ANDNOT_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>280</id>
<name>_MM_MASKZ_ANDNOT_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>288</id>
<name>_MM_MASKZ_ANDNOT_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>297</id>
<name>_MM_MASKZ_ANDNOT_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>350</id>
<name>_MM_MASKZ_AVG_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>359</id>
<name>_MM_MASKZ_AVG_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>458</id>
<name>_MM_MASKZ_BROADCAST_I32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>490</id>
<name>_MM_MASKZ_BROADCASTB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>499</id>
<name>_MM_MASKZ_BROADCASTD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>514</id>
<name>_MM_MASKZ_BROADCASTQ_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>531</id>
<name>_MM_MASKZ_BROADCASTSS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>540</id>
<name>_MM_MASKZ_BROADCASTW_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1141</id>
<name>_MM_MASKZ_COMPRESS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1147</id>
<name>_MM_MASKZ_COMPRESS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1153</id>
<name>_MM_MASKZ_COMPRESS_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1159</id>
<name>_MM_MASKZ_COMPRESS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1178</id>
<name>_MM_MASKZ_CONFLICT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[i]
		FOR l := 0 to j-1
			m := l*32
			dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
		ENDFOR
		dst[i+31:i+j] := 0
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1187</id>
<name>_MM_MASKZ_CONFLICT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		FOR l := 0 to j-1
			m := l*64
			dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
		ENDFOR
		dst[i+63:i+j] := 0
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1285</id>
<name>_MM_MASKZ_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1297</id>
<name>_MM_MASKZ_CVT_ROUNDSD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128D b,INT rounding</sign>
<instr>VCVTSD2SS</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:31]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1307</id>
<name>_MM_MASKZ_CVT_ROUNDSS_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128 b,INT rounding</sign>
<instr>VCVTSS2SD</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1319</id>
<name>_MM_MASKZ_CVTEPI16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1328</id>
<name>_MM_MASKZ_CVTEPI16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1337</id>
<name>_MM_MASKZ_CVTEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1349</id>
<name>_MM_MASKZ_CVTEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1358</id>
<name>_MM_MASKZ_CVTEPI32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1367</id>
<name>_MM_MASKZ_CVTEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1376</id>
<name>_MM_MASKZ_CVTEPI32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	IF k[j]
		dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
	ELSE
		dst[m+63:m] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1385</id>
<name>_MM_MASKZ_CVTEPI32_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1402</id>
<name>_MM_MASKZ_CVTEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1411</id>
<name>_MM_MASKZ_CVTEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1420</id>
<name>_MM_MASKZ_CVTEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1429</id>
<name>_MM_MASKZ_CVTEPI64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1438</id>
<name>_MM_MASKZ_CVTEPI64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1456</id>
<name>_MM_MASKZ_CVTEPI8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := SignExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1465</id>
<name>_MM_MASKZ_CVTEPI8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in the low 4 bytes of a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1474</id>
<name>_MM_MASKZ_CVTEPI8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 2 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1483</id>
<name>_MM_MASKZ_CVTEPU16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1492</id>
<name>_MM_MASKZ_CVTEPU16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1501</id>
<name>_MM_MASKZ_CVTEPU32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+31:l])
	ELSE 
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1510</id>
<name>_MM_MASKZ_CVTEPU32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1524</id>
<name>_MM_MASKZ_CVTEPU64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1533</id>
<name>_MM_MASKZ_CVTEPU64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1542</id>
<name>_MM_MASKZ_CVTEPU8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := ZeroExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1551</id>
<name>_MM_MASKZ_CVTEPU8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in th elow 4 bytes of a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1560</id>
<name>_MM_MASKZ_CVTEPU8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 2 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1583</id>
<name>_MM_MASKZ_CVTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1592</id>
<name>_MM_MASKZ_CVTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1601</id>
<name>_MM_MASKZ_CVTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1610</id>
<name>_MM_MASKZ_CVTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1620</id>
<name>_MM_MASKZ_CVTPD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1631</id>
<name>_MM_MASKZ_CVTPH_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1645</id>
<name>_MM_MASKZ_CVTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1654</id>
<name>_MM_MASKZ_CVTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1663</id>
<name>_MM_MASKZ_CVTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1672</id>
<name>_MM_MASKZ_CVTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1686</id>
<name>_MM_MASKZ_CVTPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1708</id>
<name>_MM_MASKZ_CVTSD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128D b</sign>
<instr>VCVTSD2SS</instr>
<desc>Convert the lower double-precision (64-bit) floating-point element in b to a single-precision (32-bit) floating-point element, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	</desc>
<oper>IF k[0]
	dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:31]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1713</id>
<name>_MM_MASKZ_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1725</id>
<name>_MM_MASKZ_CVTSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1734</id>
<name>_MM_MASKZ_CVTSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1749</id>
<name>_MM_MASKZ_CVTSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1758</id>
<name>_MM_MASKZ_CVTSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1767</id>
<name>_MM_MASKZ_CVTSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1803</id>
<name>_MM_MASKZ_CVTSS_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128 b</sign>
<instr>VCVTSS2SD</instr>
<desc>Convert the lower single-precision (32-bit) floating-point element in b to a double-precision (64-bit) floating-point element, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1849</id>
<name>_MM_MASKZ_CVTTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1858</id>
<name>_MM_MASKZ_CVTTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1867</id>
<name>_MM_MASKZ_CVTTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1876</id>
<name>_MM_MASKZ_CVTTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1886</id>
<name>_MM_MASKZ_CVTTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*i
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1895</id>
<name>_MM_MASKZ_CVTTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1904</id>
<name>_MM_MASKZ_CVTTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed double-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1913</id>
<name>_MM_MASKZ_CVTTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1940</id>
<name>_MM_MASKZ_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1952</id>
<name>_MM_MASKZ_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1961</id>
<name>_MM_MASKZ_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1976</id>
<name>_MM_MASKZ_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1985</id>
<name>_MM_MASKZ_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1994</id>
<name>_MM_MASKZ_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2012</id>
<name>_MM_MASKZ_DBSAD_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>tmp[31:0] := select(b[127:0], imm8[1:0])
tmp[63:32] := select(b[127:0], imm8[3:2])
tmp[95:64] := select(b[127:0], imm8[5:4])
tmp[127:96] := select(b[127:0], imm8[7:6])

FOR j := 0 to 1
	i := j*64
	tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2049</id>
<name>_MM_MASKZ_DIV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2058</id>
<name>_MM_MASKZ_DIV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2073</id>
<name>_MM_MASKZ_DIV_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VDIVSD</instr>
<desc>Divide the lower double-precision (64-bit) floating-point element in a by the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] / b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2076</id>
<name>_MM_MASKZ_DIV_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VDIVSS</instr>
<desc>Divide the lower single-precision (32-bit) floating-point element in a by the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] / b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2079</id>
<name>_MM_MASKZ_DIV_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VDIVSD</instr>
<desc>Divide the lower double-precision (64-bit) floating-point element in a by the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] / b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2082</id>
<name>_MM_MASKZ_DIV_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VDIVSS</instr>
<desc>Divide the lower single-precision (32-bit) floating-point element in a by the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] / b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2159</id>
<name>_MM_MASKZ_EXPAND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2165</id>
<name>_MM_MASKZ_EXPAND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2171</id>
<name>_MM_MASKZ_EXPAND_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2177</id>
<name>_MM_MASKZ_EXPAND_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2183</id>
<name>_MM_MASKZ_EXPANDLOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2189</id>
<name>_MM_MASKZ_EXPANDLOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2195</id>
<name>_MM_MASKZ_EXPANDLOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2201</id>
<name>_MM_MASKZ_EXPANDLOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2314</id>
<name>_MM_MASKZ_FIXUPIMM_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2323</id>
<name>_MM_MASKZ_FIXUPIMM_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2338</id>
<name>_MM_MASKZ_FIXUPIMM_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMSD</instr>
<desc>Fix up the lower double-precision (64-bit) floating-point elements in a and b using the lower 64-bit integer in c, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

IF k[0]
	dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2341</id>
<name>_MM_MASKZ_FIXUPIMM_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMSS</instr>
<desc>Fix up the lower single-precision (32-bit) floating-point elements in a and b using the lower 32-bit integer in c, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

IF k[0]
	dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2344</id>
<name>_MM_MASKZ_FIXUPIMM_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMSD</instr>
<desc>Fix up the lower double-precision (64-bit) floating-point elements in a and b using the lower 64-bit integer in c, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

IF k[0]
	dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2347</id>
<name>_MM_MASKZ_FIXUPIMM_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128I c,INT imm8</sign>
<instr>VFIXUPIMMSS</instr>
<desc>Fix up the lower single-precision (32-bit) floating-point elements in a and b using the lower 32-bit integer in c, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

IF k[0]
	dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2368</id>
<name>_MM_MASKZ_FMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2380</id>
<name>_MM_MASKZ_FMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2399</id>
<name>_MM_MASKZ_FMADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c,INT rounding</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2402</id>
<name>_MM_MASKZ_FMADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c,INT rounding</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2406</id>
<name>_MM_MASKZ_FMADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFMADD132SD, VFMADD213SD, VFMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2410</id>
<name>_MM_MASKZ_FMADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFMADD132SS, VFMADD213SS, VFMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2420</id>
<name>_MM_MASKZ_FMADDSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2432</id>
<name>_MM_MASKZ_FMADDSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2452</id>
<name>_MM_MASKZ_FMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2464</id>
<name>_MM_MASKZ_FMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2483</id>
<name>_MM_MASKZ_FMSUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c,INT rounding</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2486</id>
<name>_MM_MASKZ_FMSUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c,INT rounding</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2490</id>
<name>_MM_MASKZ_FMSUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFMSUB132SD, VFMSUB213SD, VFMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2494</id>
<name>_MM_MASKZ_FMSUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFMSUB132SS, VFMSUB213SS, VFMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the intermediate result. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2498</id>
<name>_MM_MASKZ_FMSUBADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2510</id>
<name>_MM_MASKZ_FMSUBADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2530</id>
<name>_MM_MASKZ_FNMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2542</id>
<name>_MM_MASKZ_FNMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2561</id>
<name>_MM_MASKZ_FNMADD_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c,INT rounding</sign>
<instr>VFNMADD132SD, VFNMADD213SD, VFNMADD231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2564</id>
<name>_MM_MASKZ_FNMADD_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c,INT rounding</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2568</id>
<name>_MM_MASKZ_FNMADD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMADD213SD, VFNMADD231SD, VFNMADD132SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2572</id>
<name>_MM_MASKZ_FNMADD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMADD132SS, VFNMADD213SS, VFNMADD231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and add the negated intermediate result to the lower element in c. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2576</id>
<name>_MM_MASKZ_FNMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2588</id>
<name>_MM_MASKZ_FNMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2607</id>
<name>_MM_MASKZ_FNMSUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c,INT rounding</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2610</id>
<name>_MM_MASKZ_FNMSUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c,INT rounding</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2614</id>
<name>_MM_MASKZ_FNMSUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,__M128D c</sign>
<instr>VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2618</id>
<name>_MM_MASKZ_FNMSUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,__M128 c</sign>
<instr>VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point elements in a and b, and subtract the lower element in c from the negated intermediate result. Store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2649</id>
<name>_MM_MASKZ_GETEXP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2658</id>
<name>_MM_MASKZ_GETEXP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2673</id>
<name>_MM_MASKZ_GETEXP_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VGETEXPSD</instr>
<desc>Convert the exponent of the lower double-precision (64-bit) floating-point element in b to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2676</id>
<name>_MM_MASKZ_GETEXP_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VGETEXPSS</instr>
<desc>Convert the exponent of the lower single-precision (32-bit) floating-point element in b to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2679</id>
<name>_MM_MASKZ_GETEXP_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VGETEXPSD</instr>
<desc>Convert the exponent of the lower double-precision (64-bit) floating-point element in b to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.</desc>
<oper>IF k[0]
	dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2682</id>
<name>_MM_MASKZ_GETEXP_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VGETEXPSS</instr>
<desc>Convert the exponent of the lower single-precision (32-bit) floating-point element in b to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. This intrinsic essentially calculates floor(log2(x)) for the lower element.</desc>
<oper>IF k[0]
	dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2685</id>
<name>_MM_MASKZ_GETMANT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2694</id>
<name>_MM_MASKZ_GETMANT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2709</id>
<name>_MM_MASKZ_GETMANT_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTSD</instr>
<desc>Normalize the mantissas of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2712</id>
<name>_MM_MASKZ_GETMANT_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTSS</instr>
<desc>Normalize the mantissas of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0]
	dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2715</id>
<name>_MM_MASKZ_GETMANT_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTSD</instr>
<desc>Normalize the mantissas of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>IF k[0]
	dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2718</id>
<name>_MM_MASKZ_GETMANT_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTSS</instr>
<desc>Normalize the mantissas of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>IF k[0]
	dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3027</id>
<name>_MM_MASKZ_LOAD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3034</id>
<name>_MM_MASKZ_LOAD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load packed 64-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3042</id>
<name>_MM_MASKZ_LOAD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3052</id>
<name>_MM_MASKZ_LOAD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3062</id>
<name>_MM_MASKZ_LOAD_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,DOUBLE_CONST_PTR mem_addr</sign>
<instr>VMOVSD</instr>
<desc>Load a double-precision (64-bit) floating-point element from memory into the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and set the upper element of dst to zero. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>IF k[0]
	dst[63:0] := MEM[mem_addr+63:mem_addr]
ELSE
	dst[63:0] := 0
FI
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3068</id>
<name>_MM_MASKZ_LOAD_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,FLOAT_CONST_PTR mem_addr</sign>
<instr>VMOVSS</instr>
<desc>Load a single-precision (32-bit) floating-point element from memory into the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and set the upper elements of dst to zero. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>IF k[0]
	dst[31:0] := MEM[mem_addr+31:mem_addr]
ELSE
	dst[31:0] := 0
FI
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3083</id>
<name>_MM_MASKZ_LOADU_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU16</instr>
<desc>Load packed 16-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3089</id>
<name>_MM_MASKZ_LOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3095</id>
<name>_MM_MASKZ_LOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU64</instr>
<desc>Load packed 64-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3101</id>
<name>_MM_MASKZ_LOADU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU8</instr>
<desc>Load packed 8-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3108</id>
<name>_MM_MASKZ_LOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memoy into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3117</id>
<name>_MM_MASKZ_LOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3198</id>
<name>_MM_MASKZ_LZCNT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		tmp := 31
		dst[i+31:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+31:i] := dst[i+31:i] + 1
		OD
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3207</id>
<name>_MM_MASKZ_LZCNT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp := 63
		dst[i+63:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+63:i] := dst[i+63:i] + 1
		OD
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3218</id>
<name>_MM_MASKZ_MADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3228</id>
<name>_MM_MASKZ_MADD52HI_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,__M128I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3237</id>
<name>_MM_MASKZ_MADD52LO_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,__M128I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3246</id>
<name>_MM_MASKZ_MADDUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Multiply packed unsigned 8-bit integers in a by packed signed 8-bit integers in b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3277</id>
<name>_MM_MASKZ_MAX_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3286</id>
<name>_MM_MASKZ_MAX_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3295</id>
<name>_MM_MASKZ_MAX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3304</id>
<name>_MM_MASKZ_MAX_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3313</id>
<name>_MM_MASKZ_MAX_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3322</id>
<name>_MM_MASKZ_MAX_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3331</id>
<name>_MM_MASKZ_MAX_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3340</id>
<name>_MM_MASKZ_MAX_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3349</id>
<name>_MM_MASKZ_MAX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3359</id>
<name>_MM_MASKZ_MAX_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3375</id>
<name>_MM_MASKZ_MAX_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT sae</sign>
<instr>VMAXSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3378</id>
<name>_MM_MASKZ_MAX_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT sae</sign>
<instr>VMAXSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3381</id>
<name>_MM_MASKZ_MAX_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMAXSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3384</id>
<name>_MM_MASKZ_MAX_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMAXSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the maximum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3391</id>
<name>_MM_MASKZ_MIN_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3400</id>
<name>_MM_MASKZ_MIN_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3409</id>
<name>_MM_MASKZ_MIN_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3418</id>
<name>_MM_MASKZ_MIN_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3427</id>
<name>_MM_MASKZ_MIN_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3436</id>
<name>_MM_MASKZ_MIN_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3445</id>
<name>_MM_MASKZ_MIN_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3454</id>
<name>_MM_MASKZ_MIN_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3463</id>
<name>_MM_MASKZ_MIN_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3473</id>
<name>_MM_MASKZ_MIN_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3489</id>
<name>_MM_MASKZ_MIN_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT sae</sign>
<instr>VMINSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3492</id>
<name>_MM_MASKZ_MIN_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT sae</sign>
<instr>VMINSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>IF k[0]
	dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3495</id>
<name>_MM_MASKZ_MIN_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMINSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3498</id>
<name>_MM_MASKZ_MIN_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMINSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the minimum value in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3503</id>
<name>_MM_MASKZ_MOV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VMOVDQU16</instr>
<desc>Move packed 16-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3509</id>
<name>_MM_MASKZ_MOV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VMOVDQA32</instr>
<desc>Move packed 32-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3515</id>
<name>_MM_MASKZ_MOV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VMOVDQA64</instr>
<desc>Move packed 64-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3521</id>
<name>_MM_MASKZ_MOV_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VMOVDQU8</instr>
<desc>Move packed 8-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3527</id>
<name>_MM_MASKZ_MOV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VMOVAPD</instr>
<desc>Move packed double-precision (64-bit) floating-point elements from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3533</id>
<name>_MM_MASKZ_MOV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VMOVAPS</instr>
<desc>Move packed single-precision (32-bit) floating-point elements from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3540</id>
<name>_MM_MASKZ_MOVE_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMOVSD</instr>
<desc>Move the lower double-precision (64-bit) floating-point element from b to the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3543</id>
<name>_MM_MASKZ_MOVE_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMOVSS</instr>
<desc>Move the lower single-precision (32-bit) floating-point element from b to the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3546</id>
<name>_MM_MASKZ_MOVEDUP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3555</id>
<name>_MM_MASKZ_MOVEHDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3565</id>
<name>_MM_MASKZ_MOVELDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3610</id>
<name>_MM_MASKZ_MUL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3619</id>
<name>_MM_MASKZ_MUL_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3628</id>
<name>_MM_MASKZ_MUL_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3637</id>
<name>_MM_MASKZ_MUL_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3652</id>
<name>_MM_MASKZ_MUL_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VMULSD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] * b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3655</id>
<name>_MM_MASKZ_MUL_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VMULSS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] * b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3658</id>
<name>_MM_MASKZ_MUL_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VMULSD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] * b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3661</id>
<name>_MM_MASKZ_MUL_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VMULSS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] * b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3665</id>
<name>_MM_MASKZ_MULHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3676</id>
<name>_MM_MASKZ_MULHI_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := o
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3689</id>
<name>_MM_MASKZ_MULHRS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
		dst[i+15:i] := tmp[16:1]
	ELSE
		dst[i+15:i] := 9
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3699</id>
<name>_MM_MASKZ_MULLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3708</id>
<name>_MM_MASKZ_MULLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		tmp[63:0] := a[i+31:i] * b[i+31:i]
		dst[i+31:i] := tmp[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3717</id>
<name>_MM_MASKZ_MULLO_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		tmp[127:0] := a[i+63:i] * b[i+63:i]
		dst[i+63:i] := tmp[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3729</id>
<name>_MM_MASKZ_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR i := 0 to 1
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		IF k[i*8+j]
			dst[q+j*8+7:q+j*8] := tmp8[7:0]
		ELSE
			dst[q+j*8+7:q+j*8] := 0
		FI
	ENDFOR
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3743</id>
<name>_MM_MASKZ_OR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] OR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3750</id>
<name>_MM_MASKZ_OR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] OR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3757</id>
<name>_MM_MASKZ_OR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3766</id>
<name>_MM_MASKZ_OR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3779</id>
<name>_MM_MASKZ_PACKS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3788</id>
<name>_MM_MASKZ_PACKS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3818</id>
<name>_MM_MASKZ_PACKUS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3827</id>
<name>_MM_MASKZ_PACKUS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3857</id>
<name>_MM_MASKZ_PERMUTE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3866</id>
<name>_MM_MASKZ_PERMUTE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3887</id>
<name>_MM_MASKZ_PERMUTEVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a using the control in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3896</id>
<name>_MM_MASKZ_PERMUTEVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3920</id>
<name>_MM_MASKZ_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2W, VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		off := 16*idx[i+2:i]
		dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3932</id>
<name>_MM_MASKZ_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2D, VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	IF k[j]
		dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3944</id>
<name>_MM_MASKZ_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2Q, VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	IF k[j]
		dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3956</id>
<name>_MM_MASKZ_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2B, VPERMT2B</instr>
<desc>Shuffle 8-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		off := 8*idx[i+3:i]
		dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3968</id>
<name>_MM_MASKZ_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128I idx,__M128D b</sign>
<instr>VPERMI2PD, VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	IF k[j]
		dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3980</id>
<name>_MM_MASKZ_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128I idx,__M128 b</sign>
<instr>VPERMI2PS, VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	IF k[j]
		dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3991</id>
<name>_MM_MASKZ_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I idx,__M128I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	id := idx[i+2:i]*16
	IF k[j]
		dst[i+15:i] := a[id+15:id]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4012</id>
<name>_MM_MASKZ_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I idx,__M128I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	id := idx[i+3:i]*8
	IF k[j]
		dst[i+7:i] := a[id+7:id]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4114</id>
<name>_MM_MASKZ_RANGE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4123</id>
<name>_MM_MASKZ_RANGE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4138</id>
<name>_MM_MASKZ_RANGE_ROUND_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT imm8,INT rounding</sign>
<instr>VRANGESD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4141</id>
<name>_MM_MASKZ_RANGE_ROUND_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT imm8,INT rounding</sign>
<instr>VRANGESS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[31:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4144</id>
<name>_MM_MASKZ_RANGE_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT imm8</sign>
<instr>VRANGESD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4146</id>
<name>_MM_MASKZ_RANGE_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT imm8</sign>
<instr>VRANGESS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[31:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

IF k[0]
	dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4151</id>
<name>_MM_MASKZ_RCP14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4160</id>
<name>_MM_MASKZ_RCP14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4169</id>
<name>_MM_MASKZ_RCP14_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRCP14SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[63:0] := APPROXIMATE(1.0/b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4172</id>
<name>_MM_MASKZ_RCP14_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRCP14SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[31:0] := APPROXIMATE(1.0/b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4189</id>
<name>_MM_MASKZ_RCP28_ROUND_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VRCP28SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[63:0] := RCP_28_DP(1.0/b[63:0];
ELSE
	dst[63:0] := 0;
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4192</id>
<name>_MM_MASKZ_RCP28_ROUND_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VRCP28SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[31:0] := RCP_28_DP(1.0/b[31:0];
ELSE
	dst[31:0] := 0;
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4195</id>
<name>_MM_MASKZ_RCP28_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRCP28SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[63:0] := RCP_28_DP(1.0/b[63:0];
ELSE
	dst[63:0] := 0;
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4198</id>
<name>_MM_MASKZ_RCP28_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRCP28SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[31:0] := RCP_28_DP(1.0/b[31:0];
ELSE
	dst[31:0] := 0;
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4274</id>
<name>_MM_MASKZ_REDUCE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4283</id>
<name>_MM_MASKZ_REDUCE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4298</id>
<name>_MM_MASKZ_REDUCE_ROUND_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT imm8,INT rounding</sign>
<instr>VREDUCESD</instr>
<desc>Extract the reduced argument of the lower double-precision (64-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

IF k[0]
	dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4301</id>
<name>_MM_MASKZ_REDUCE_ROUND_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT imm8,INT rounding</sign>
<instr>VREDUCESS</instr>
<desc>Extract the reduced argument of the lower single-precision (32-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}

IF k[0]
	dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := 0
FI
dst[127:64] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4304</id>
<name>_MM_MASKZ_REDUCE_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT imm8</sign>
<instr>VREDUCESD</instr>
<desc>Extract the reduced argument of the lower double-precision (64-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

IF k[0]
	dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4307</id>
<name>_MM_MASKZ_REDUCE_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT imm8</sign>
<instr>VREDUCESS</instr>
<desc>Extract the reduced argument of the lower single-precision (32-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}

IF k[0]
	dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := 0
FI
dst[127:64] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4340</id>
<name>_MM_MASKZ_ROL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4349</id>
<name>_MM_MASKZ_ROL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4358</id>
<name>_MM_MASKZ_ROLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4367</id>
<name>_MM_MASKZ_ROLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4376</id>
<name>_MM_MASKZ_ROR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4385</id>
<name>_MM_MASKZ_ROR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,CONST_INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4394</id>
<name>_MM_MASKZ_RORV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4403</id>
<name>_MM_MASKZ_RORV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4428</id>
<name>_MM_MASKZ_ROUNDSCALE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4437</id>
<name>_MM_MASKZ_ROUNDSCALE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4452</id>
<name>_MM_MASKZ_ROUNDSCALE_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,CONST_INT imm8,CONST_INT rounding</sign>
<instr>VRNDSCALESD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}		

IF k[0]
	dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4455</id>
<name>_MM_MASKZ_ROUNDSCALE_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,CONST_INT imm8,CONST_INT rounding</sign>
<instr>VRNDSCALESS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}

IF k[0]
	dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4458</id>
<name>_MM_MASKZ_ROUNDSCALE_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VRNDSCALESD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}		

IF k[0]
	dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4461</id>
<name>_MM_MASKZ_ROUNDSCALE_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VRNDSCALESS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}

IF k[0]
	dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4467</id>
<name>_MM_MASKZ_RSQRT14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4474</id>
<name>_MM_MASKZ_RSQRT14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4481</id>
<name>_MM_MASKZ_RSQRT14_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRSQRT14SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4484</id>
<name>_MM_MASKZ_RSQRT14_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRSQRT14SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>IF k[0]
	dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4501</id>
<name>_MM_MASKZ_RSQRT28_ROUND_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VRSQRT28SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[63:0] := (1.0/SQRT(b[63:0]));
ELSE
	dst[63:0] := 0;
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4504</id>
<name>_MM_MASKZ_RSQRT28_ROUND_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VRSQRT28SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>IF k[0] THEN
	dst[31:0] := (1.0/SQRT(b[31:0]));
ELSE
	dst[31:0] := 0;
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4507</id>
<name>_MM_MASKZ_RSQRT28_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VRSQRT28SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[63:0] := (1.0/SQRT(b[63:0]));
ELSE
	dst[63:0] := 0;
FI
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4510</id>
<name>_MM_MASKZ_RSQRT28_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VRSQRT28SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>IF k[0] THEN
	dst[31:0] := (1.0/SQRT(b[31:0]));
ELSE
	dst[31:0] := 0;
FI
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4525</id>
<name>_MM_MASKZ_SCALEF_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4534</id>
<name>_MM_MASKZ_SCALEF_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4549</id>
<name>_MM_MASKZ_SCALEF_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VSCALEFSD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4552</id>
<name>_MM_MASKZ_SCALEF_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VSCALEFSS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4555</id>
<name>_MM_MASKZ_SCALEF_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSCALEFSD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4558</id>
<name>_MM_MASKZ_SCALEF_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSCALEFSS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[63:0]
}

IF k[0]
	dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4592</id>
<name>_MM_MASKZ_SET1_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,SHORT a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4601</id>
<name>_MM_MASKZ_SET1_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4610</id>
<name>_MM_MASKZ_SET1_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4620</id>
<name>_MM_MASKZ_SET1_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,CHAR a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast 8-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4689</id>
<name>_MM_MASKZ_SHUFFLE_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4698</id>
<name>_MM_MASKZ_SHUFFLE_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		IF b[i+7] == 1
			dst[i+7:i] := 0
		ELSE
			index[3:0] := b[i+3:i]
			dst[i+7:i] := a[index*8+7:index*8]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4731</id>
<name>_MM_MASKZ_SHUFFLE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4742</id>
<name>_MM_MASKZ_SHUFFLE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4751</id>
<name>_MM_MASKZ_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of a using the control in imm8. Store the results in the high 64 bits of dst, with the low 64 bits being copied from from a to dst, using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4760</id>
<name>_MM_MASKZ_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of a using the control in imm8. Store the results in the low 64 bits of dst, with the high 64 bits being copied from from a to dst, using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4810</id>
<name>_MM_MASKZ_SLL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4819</id>
<name>_MM_MASKZ_SLL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4828</id>
<name>_MM_MASKZ_SLL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4840</id>
<name>_MM_MASKZ_SLLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4849</id>
<name>_MM_MASKZ_SLLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4858</id>
<name>_MM_MASKZ_SLLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4872</id>
<name>_MM_MASKZ_SLLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4881</id>
<name>_MM_MASKZ_SLLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4890</id>
<name>_MM_MASKZ_SLLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4901</id>
<name>_MM_MASKZ_SQRT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4910</id>
<name>_MM_MASKZ_SQRT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4925</id>
<name>_MM_MASKZ_SQRT_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VSQRTSD</instr>
<desc>Compute the square root of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := SQRT(a[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4928</id>
<name>_MM_MASKZ_SQRT_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VSQRTSS</instr>
<desc>Compute the square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := SQRT(a[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4931</id>
<name>_MM_MASKZ_SQRT_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSQRTSD</instr>
<desc>Compute the square root of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from b to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := SQRT(a[63:0])
ELSE
	dst[63:0] := 0
FI
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4934</id>
<name>_MM_MASKZ_SQRT_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSQRTSS</instr>
<desc>Compute the square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := SQRT(a[31:0])
ELSE
	dst[31:0] := 0
FI
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4937</id>
<name>_MM_MASKZ_SRA_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4946</id>
<name>_MM_MASKZ_SRA_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4955</id>
<name>_MM_MASKZ_SRA_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4966</id>
<name>_MM_MASKZ_SRAI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4975</id>
<name>_MM_MASKZ_SRAI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4984</id>
<name>_MM_MASKZ_SRAI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4995</id>
<name>_MM_MASKZ_SRAV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5004</id>
<name>_MM_MASKZ_SRAV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5013</id>
<name>_MM_MASKZ_SRAV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5022</id>
<name>_MM_MASKZ_SRL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5031</id>
<name>_MM_MASKZ_SRL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5040</id>
<name>_MM_MASKZ_SRL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5052</id>
<name>_MM_MASKZ_SRLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5061</id>
<name>_MM_MASKZ_SRLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5070</id>
<name>_MM_MASKZ_SRLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5084</id>
<name>_MM_MASKZ_SRLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5093</id>
<name>_MM_MASKZ_SRLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5102</id>
<name>_MM_MASKZ_SRLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5207</id>
<name>_MM_MASKZ_SUB_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] - b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5216</id>
<name>_MM_MASKZ_SUB_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5225</id>
<name>_MM_MASKZ_SUB_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5234</id>
<name>_MM_MASKZ_SUB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] - b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5243</id>
<name>_MM_MASKZ_SUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5255</id>
<name>_MM_MASKZ_SUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5270</id>
<name>_MM_MASKZ_SUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b,INT rounding</sign>
<instr>VSUBSD</instr>
<desc>Subtract the lower double-precision (64-bit) floating-point element in b from the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] - b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5273</id>
<name>_MM_MASKZ_SUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b,INT rounding</sign>
<instr>VSUBSS</instr>
<desc>Subtract the lower single-precision (32-bit) floating-point element in b from the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] - b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5276</id>
<name>_MM_MASKZ_SUB_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VSUBSD</instr>
<desc>Subtract the lower double-precision (64-bit) floating-point element in b from the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper element from a to the upper element of dst.</desc>
<oper>IF k[0]
	dst[63:0] := a[63:0] - b[63:0]
ELSE
	dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5280</id>
<name>_MM_MASKZ_SUB_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VSUBSS</instr>
<desc>Subtract the lower single-precision (32-bit) floating-point element in b from the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst using zeromask k (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>IF k[0]
	dst[31:0] := a[31:0] - b[31:0]
ELSE
	dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5297</id>
<name>_MM_MASKZ_SUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5306</id>
<name>_MM_MASKZ_SUBS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5315</id>
<name>_MM_MASKZ_SUBS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5324</id>
<name>_MM_MASKZ_SUBS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5389</id>
<name>_MM_MASKZ_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,__M128I c,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using zeromask k at 32-bit granularity (32-bit elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		FOR h := 0 to 31
			index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5398</id>
<name>_MM_MASKZ_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b,__M128I c,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using zeromask k at 64-bit granularity (64-bit elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		FOR h := 0 to 63
			index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5519</id>
<name>_MM_MASKZ_UNPACKHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5528</id>
<name>_MM_MASKZ_UNPACKHI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5537</id>
<name>_MM_MASKZ_UNPACKHI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5546</id>
<name>_MM_MASKZ_UNPACKHI_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5555</id>
<name>_MM_MASKZ_UNPACKHI_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5567</id>
<name>_MM_MASKZ_UNPACKHI_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5576</id>
<name>_MM_MASKZ_UNPACKLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])

FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5585</id>
<name>_MM_MASKZ_UNPACKLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5594</id>
<name>_MM_MASKZ_UNPACKLO_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5603</id>
<name>_MM_MASKZ_UNPACKLO_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M128I a,__M128I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])

FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5612</id>
<name>_MM_MASKZ_UNPACKLO_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5624</id>
<name>_MM_MASKZ_UNPACKLO_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])

FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5643</id>
<name>_MM_MASKZ_XOR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5650</id>
<name>_MM_MASKZ_XOR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128I a,__M128I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5657</id>
<name>_MM_MASKZ_XOR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M128D a,__M128D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5666</id>
<name>_MM_MASKZ_XOR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M128 a,__M128 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3278</id>
<name>_MM_MAX_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3287</id>
<name>_MM_MAX_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF a[i+31:i] &amp;gt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3296</id>
<name>_MM_MAX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF a[i+63:i] &amp;gt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3305</id>
<name>_MM_MAX_EPI8</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3314</id>
<name>_MM_MAX_EPU16</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3323</id>
<name>_MM_MAX_EPU32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF a[i+31:i] &amp;gt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3332</id>
<name>_MM_MAX_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF a[i+63:i] &amp;gt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3341</id>
<name>_MM_MAX_EPU8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3350</id>
<name>_MM_MAX_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3357</id>
<name>_MM_MAX_PI16</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3360</id>
<name>_MM_MAX_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3367</id>
<name>_MM_MAX_PU8</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3376</id>
<name>_MM_MAX_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT sae</sign>
<instr>VMAXSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the maximum value in the lower element of dst, and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>dst[63:0] := MAX(a[63:0], b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3379</id>
<name>_MM_MAX_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT sae</sign>
<instr>VMAXSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the maximum value in the lower element of dst, and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>dst[31:0] := MAX(a[31:0], b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3382</id>
<name>_MM_MAX_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MAXSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the maximum value in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := MAX(a[63:0], b[63:0])
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>3385</id>
<name>_MM_MAX_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MAXSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the maximum value in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[31:0] := MAX(a[31:0], b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>3389</id>
<name>_MM_MFENCE</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign></sign>
<instr>MFENCE</instr>
<desc>Perform a serializing operation on all load-from-memory and store-to-memory instructions that were issued prior to this instruction. Guarantees that every memory access that precedes, in program order, the memory fence instruction is globally visible before any memory instruction which follows the fence in program order.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>3392</id>
<name>_MM_MIN_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3401</id>
<name>_MM_MIN_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF a[i+31:i] &amp;lt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3410</id>
<name>_MM_MIN_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF a[i+63:i] &amp;lt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3419</id>
<name>_MM_MIN_EPI8</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3428</id>
<name>_MM_MIN_EPU16</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3437</id>
<name>_MM_MIN_EPU32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF a[i+31:i] &amp;lt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3446</id>
<name>_MM_MIN_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF a[i+63:i] &amp;lt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3455</id>
<name>_MM_MIN_EPU8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3464</id>
<name>_MM_MIN_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3471</id>
<name>_MM_MIN_PI16</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3474</id>
<name>_MM_MIN_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3481</id>
<name>_MM_MIN_PU8</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3490</id>
<name>_MM_MIN_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT sae</sign>
<instr>VMINSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the minimum value in the lower element of dst , and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>dst[63:0] := MIN(a[63:0], b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3493</id>
<name>_MM_MIN_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT sae</sign>
<instr>VMINSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the minimum value in the lower element of dst, and copy the upper element from a to the upper element of dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>dst[31:0] := MIN(a[31:0], b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3496</id>
<name>_MM_MIN_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MINSD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point elements in a and b, store the minimum value in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := MIN(a[63:0], b[63:0])
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>3499</id>
<name>_MM_MIN_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MINSS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point elements in a and b, store the minimum value in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[31:0] := MIN(a[31:0], b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>3500</id>
<name>_MM_MINPOS_EPU16</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>PHMINPOSUW</instr>
<desc>Horizontally compute the minimum amongst the packed unsigned 16-bit integers in a, store the minimum and index in dst, and zero the remaining bits in dst.</desc>
<oper>index[2:0] := 0
min[15:0] := a[15:0]
FOR j := 0 to 7
	i := j*16
	IF a[i+15:i] &amp;lt; min[15:0]
		index[2:0] := j
		min[15:0] := a[i+15:i]
	FI
ENDFOR
dst[15:0] := min[15:0]
dst[18:16] := index[2:0]
dst[127:19] := 0</oper>
</intrinsic>
<intrinsic>
<id>2773</id>
<name>_MM_MMASK_I32GATHER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2781</id>
<name>_MM_MMASK_I32GATHER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	m := j*32
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:2] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2789</id>
<name>_MM_MMASK_I32GATHER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	m := j*32
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:2] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2797</id>
<name>_MM_MMASK_I32GATHER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2861</id>
<name>_MM_MMASK_I64GATHER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:2] := 0
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2871</id>
<name>_MM_MMASK_I64GATHER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:2] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2879</id>
<name>_MM_MMASK_I64GATHER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:2] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2887</id>
<name>_MM_MMASK_I64GATHER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 1
	i := j*32
	m := j*64
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:2] := 0
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3501</id>
<name>_MM_MONITOR</name>
<cpuid>MONITOR</cpuid>
<ret>void</ret>
<sign>VOID_CONST_PTR p,INT extensions,INT hints</sign>
<instr>MONITOR</instr>
<desc>Arm address monitoring hardware using the address specified in p. A store to an address within the specified address range triggers the monitoring hardware. Specify optional extensions in extensions, and optional hints in hints.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>3538</id>
<name>_MM_MOVE_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a</sign>
<instr>MOVQ</instr>
<desc>Copy the lower 64-bit integer in a to the lower element of dst, and zero the upper element.</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3541</id>
<name>_MM_MOVE_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MOVSD</instr>
<desc>Move the lower double-precision (64-bit) floating-point element from b to the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := b[63:0]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>3544</id>
<name>_MM_MOVE_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MOVSS</instr>
<desc>Move the lower single-precision (32-bit) floating-point element from b to the lower element of dst, and copy the upper 3 elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := b[31:0]
dst[63:32] := a[63:32]
dst[95:64] := a[95:64]
dst[127:96] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>3547</id>
<name>_MM_MOVEDUP_PD</name>
<cpuid>SSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>MOVDDUP</instr>
<desc>Duplicate the low double-precision (64-bit) floating-point element from a, and store the results in dst.
	</desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>3556</id>
<name>_MM_MOVEHDUP_PS</name>
<cpuid>SSE3</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>MOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[31:0] := a[63:32] 
dst[63:32] := a[63:32]
dst[95:64] := a[127:96] 
dst[127:96] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>3563</id>
<name>_MM_MOVEHL_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MOVHLPS</instr>
<desc>Move the upper 2 single-precision (32-bit) floating-point elements from b to the lower 2 elements of dst, and copy the upper 2 elements from a to the upper 2 elements of dst.</desc>
<oper>dst[31:0] := b[95:64]
dst[63:32] := b[127:96]
dst[95:64] := a[95:64]
dst[127:96] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>3566</id>
<name>_MM_MOVELDUP_PS</name>
<cpuid>SSE3</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>MOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[31:0] := a[31:0] 
dst[63:32] := a[31:0]
dst[95:64] := a[95:64] 
dst[127:96] := a[95:64]</oper>
</intrinsic>
<intrinsic>
<id>3573</id>
<name>_MM_MOVELH_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MOVLHPS</instr>
<desc>Move the lower 2 single-precision (32-bit) floating-point elements from b to the upper 2 elements of dst, and copy the lower 2 elements from a to the lower 2 elements of dst.</desc>
<oper>dst[31:0] := a[31:0]
dst[63:32] := a[63:32]
dst[95:64] := b[31:0]
dst[127:96] := b[63:32]</oper>
</intrinsic>
<intrinsic>
<id>3574</id>
<name>_MM_MOVEMASK_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128I a</sign>
<instr>PMOVMSKB</instr>
<desc>Create mask from the most significant bit of each 8-bit element in a, and store the result in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[j] := a[i+7]
ENDFOR
dst[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3576</id>
<name>_MM_MOVEMASK_PD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a</sign>
<instr>MOVMSKPD</instr>
<desc>Set each bit of mask dst based on the most significant bit of the corresponding packed double-precision (64-bit) floating-point element in a.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF a[i+63]
		dst[j] := 1
	ELSE
		dst[j] := 0
	FI
ENDFOR
dst[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>3578</id>
<name>_MM_MOVEMASK_PI8</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M64 a</sign>
<instr>PMOVMSKB</instr>
<desc>Create mask from the most significant bit of each 8-bit element in a, and store the result in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[j] := a[i+7]
ENDFOR
dst[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>3579</id>
<name>_MM_MOVEMASK_PS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a</sign>
<instr>MOVMSKPS</instr>
<desc>Set each bit of mask dst based on the most significant bit of the corresponding packed single-precision (32-bit) floating-point element in a.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF a[i+31]
		dst[j] := 1
	ELSE
		dst[j] := 0
	FI
ENDFOR
dst[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>3581</id>
<name>_MM_MOVEPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a</sign>
<instr>VPMOVW2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 16-bit integer in a.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF a[i+15]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>3584</id>
<name>_MM_MOVEPI32_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a</sign>
<instr>VPMOVD2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 32-bit integer in a.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF a[i+31]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>3587</id>
<name>_MM_MOVEPI64_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a</sign>
<instr>VPMOVQ2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 64-bit integer in a.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF a[i+63]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>3590</id>
<name>_MM_MOVEPI64_PI64</name>
<cpuid>SSE2</cpuid>
<ret>__m64</ret>
<sign>__M128I a</sign>
<instr>MOVDQ2Q</instr>
<desc>Copy the lower 64-bit integer in a to dst.</desc>
<oper>dst[63:0] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>3591</id>
<name>_MM_MOVEPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a</sign>
<instr>VPMOVB2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 8-bit integer in a.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF a[i+7]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3594</id>
<name>_MM_MOVM_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k</sign>
<instr>VPMOVM2W</instr>
<desc>Set each packed 16-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := 0xFFFF
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3597</id>
<name>_MM_MOVM_EPI32</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k</sign>
<instr>VPMOVM2D</instr>
<desc>Set each packed 32-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := 0xFFFFFFFF
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3600</id>
<name>_MM_MOVM_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k</sign>
<instr>VPMOVM2Q</instr>
<desc>Set each packed 64-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := 0xFFFFFFFFffffffff
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3603</id>
<name>_MM_MOVM_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k</sign>
<instr>VPMOVM2B</instr>
<desc>Set each packed 8-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF k[j]
		dst[i+7:i] := 0xFF
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3606</id>
<name>_MM_MOVPI64_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M64 a</sign>
<instr>MOVQ2DQ</instr>
<desc>Copy the 64-bit integer a to the lower element of dst, and zero the upper element.</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3607</id>
<name>_MM_MPSADBW_EPU8</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT imm8</sign>
<instr>MPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst.
	Eight SADs are performed using one quadruplet from b and eight quadruplets from a. One quadruplet is selected from b starting at on the offset specified in imm8. Eight quadruplets are formed from sequential 8-bit integers selected from a starting at the offset specified in imm8.</desc>
<oper>MPSADBW(a[127:0], b[127:0], imm8[2:0]) {
	a_offset := imm8[2]*32
	b_offset := imm8[1:0]*32
	FOR j := 0 to 7
		i := j*8
		k := a_offset+i
		l := b_offset
		tmp[i+15:i] := ABS(a[k+7:k] - b[l+7:l]) + ABS(a[k+15:k+8] - b[l+15:l+8]) + ABS(a[k+23:k+16] - b[l+23:l+16]) + ABS(a[k+31:k+24] - b[l+31:l+24])
	ENDFOR
	RETURN tmp[127:0]
}

dst[127:0] := MPSADBW(a[127:0], b[127:0], imm8[2:0])</oper>
</intrinsic>
<intrinsic>
<id>3611</id>
<name>_MM_MUL_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3620</id>
<name>_MM_MUL_EPU32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3629</id>
<name>_MM_MUL_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] * b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3638</id>
<name>_MM_MUL_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] * b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3653</id>
<name>_MM_MUL_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VMULSD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>dst[63:0] := a[63:0] * b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3656</id>
<name>_MM_MUL_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VMULSS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
		Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		</desc>
<oper>dst[31:0] := a[31:0] * b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3659</id>
<name>_MM_MUL_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>MULSD</instr>
<desc>Multiply the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := a[63:0] * b[63:0]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>3662</id>
<name>_MM_MUL_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>MULSS</instr>
<desc>Multiply the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := a[31:0] * b[31:0]
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>3663</id>
<name>_MM_MUL_SU32</name>
<cpuid>SSE2</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from a and b, and store the unsigned 64-bit result in dst. </desc>
<oper>dst[63:0] := a[31:0] * b[31:0]</oper>
</intrinsic>
<intrinsic>
<id>3666</id>
<name>_MM_MULHI_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3677</id>
<name>_MM_MULHI_EPU16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3686</id>
<name>_MM_MULHI_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3687</id>
<name>_MM_MULHI_PU16</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3690</id>
<name>_MM_MULHRS_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
	dst[i+15:i] := tmp[16:1]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3697</id>
<name>_MM_MULHRS_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
	dst[i+15:i] := tmp[16:1]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3700</id>
<name>_MM_MULLO_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[15:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3709</id>
<name>_MM_MULLO_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	tmp[63:0] := a[i+31:i] * b[i+31:i]
	dst[i+31:i] := tmp[31:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3718</id>
<name>_MM_MULLO_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	tmp[127:0] := a[i+63:i] * b[i+63:i]
	dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3725</id>
<name>_MM_MULLO_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[15:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3730</id>
<name>_MM_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst.</desc>
<oper>FOR i := 0 to 1
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		dst[q+j*8+7:q+j*8] := tmp8[7:0]
	ENDFOR
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3737</id>
<name>_MM_MWAIT</name>
<cpuid>MONITOR</cpuid>
<ret>void</ret>
<sign>INT extensions,INT hints</sign>
<instr>MWAIT</instr>
<desc>Hint to the processor that it can enter an implementation-dependent-optimized state while waiting for an event or store operation to the address range specified by MONITOR.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>3758</id>
<name>_MM_OR_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>ORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3767</id>
<name>_MM_OR_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>ORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3774</id>
<name>_MM_OR_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>POR</instr>
<desc>Compute the bitwise OR of 128 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[127:0] := (a[127:0] OR b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>3777</id>
<name>_MM_OR_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>POR</instr>
<desc>Compute the bitwise OR of 64 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[63:0] := (a[63:0] OR b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>3780</id>
<name>_MM_PACKS_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst.
	</desc>
<oper>dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])</oper>
</intrinsic>
<intrinsic>
<id>3789</id>
<name>_MM_PACKS_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])</oper>
</intrinsic>
<intrinsic>
<id>3796</id>
<name>_MM_PACKS_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst.
	</desc>
<oper>dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
dst[39:32] := Saturate_Int16_To_Int8 (b[15:0])
dst[47:40] := Saturate_Int16_To_Int8 (b[31:16])
dst[55:48] := Saturate_Int16_To_Int8 (b[47:32])
dst[63:56] := Saturate_Int16_To_Int8 (b[63:48])</oper>
</intrinsic>
<intrinsic>
<id>3797</id>
<name>_MM_PACKS_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
dst[47:32] := Saturate_Int32_To_Int16 (b[31:0])
dst[63:48] := Saturate_Int32_To_Int16 (b[63:32])</oper>
</intrinsic>
<intrinsic>
<id>3798</id>
<name>_MM_PACKS_PU16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
dst[39:32] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
dst[47:40] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
dst[55:48] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
dst[63:56] := Saturate_Int16_To_UnsignedInt8 (b[63:48])</oper>
</intrinsic>
<intrinsic>
<id>3819</id>
<name>_MM_PACKUS_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])</oper>
</intrinsic>
<intrinsic>
<id>3828</id>
<name>_MM_PACKUS_EPI32</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])</oper>
</intrinsic>
<intrinsic>
<id>3845</id>
<name>_MM_PAUSE</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign></sign>
<instr>PAUSE</instr>
<desc>Provide a hint to the processor that the code sequence is a spin-wait loop. This can help improve the performance and power consumption of spin-wait loops.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>3858</id>
<name>_MM_PERMUTE_PD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a using the control in imm8, and store the results in dst.</desc>
<oper>IF (imm8[0] == 0) dst[63:0] := a[63:0]
IF (imm8[0] == 1) dst[63:0] := a[127:64]
IF (imm8[1] == 0) dst[127:64] := a[63:0]
IF (imm8[1] == 1) dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3867</id>
<name>_MM_PERMUTE_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3888</id>
<name>_MM_PERMUTEVAR_PD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a using the control in b, and store the results in dst.</desc>
<oper>IF (b[1] == 0) dst[63:0] := a[63:0]
IF (b[1] == 1) dst[63:0] := a[127:64]
IF (b[65] == 0) dst[127:64] := a[63:0]
IF (b[65] == 1) dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3897</id>
<name>_MM_PERMUTEVAR_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], b[1:0])
dst[63:32] := SELECT4(a[127:0], b[33:32])
dst[95:64] := SELECT4(a[127:0], b[65:64])
dst[127:96] := SELECT4(a[127:0], b[97:96])
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3921</id>
<name>_MM_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2W, VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	off := 16*idx[i+2:i]
	dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3933</id>
<name>_MM_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2D, VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+2:i]*32
	dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3945</id>
<name>_MM_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2Q, VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3957</id>
<name>_MM_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I idx,__M128I b</sign>
<instr>VPERMI2B</instr>
<desc>Shuffle 8-bit integers in a and b using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	off := 8*idx[i+3:i]
	dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3969</id>
<name>_MM_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128I idx,__M128D b</sign>
<instr>VPERMI2PD, VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	off := idx[i]*64
	dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3981</id>
<name>_MM_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128I idx,__M128 b</sign>
<instr>VPERMI2PS, VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	off := idx[i+1:i]*32
	dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3992</id>
<name>_MM_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I idx,__M128I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	id := idx[i+2:i]*16
	dst[i+15:i] := a[id+15:id]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4013</id>
<name>_MM_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m128d</ret>
<sign>__M128I idx,__M128I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	id := idx[i+3:i]*8
	dst[i+7:i] := a[id+7:id]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4045</id>
<name>_MM_POPCNT_U32</name>
<cpuid>POPCNT</cpuid>
<ret>int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>POPCNT</instr>
<desc>
		Count the number of bits set to 1 in unsigned 32-bit integer a, and return that count in dst. 
	</desc>
<oper>dst := 0
FOR i := 0 to 31
	IF a[i]
		dst := dst + 1
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4046</id>
<name>_MM_POPCNT_U64</name>
<cpuid>POPCNT</cpuid>
<ret>__int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>POPCNT</instr>
<desc>
		Count the number of bits set to 1 in unsigned 64-bit integer a, and return that count in dst. 
	</desc>
<oper>dst := 0
FOR i := 0 to 63
	IF a[i]
		dst := dst + 1
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4050</id>
<name>_MM_POW_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed double-precision (64-bit) floating-point elements in a raised by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4054</id>
<name>_MM_POW_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed single-precision (32-bit) floating-point elements in a raised by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4058</id>
<name>_MM_PREFETCH</name>
<cpuid>PREFETCHWT1</cpuid>
<ret>void</ret>
<sign>CHAR_CONST_PTR p,INT i</sign>
<instr>PREFETCHWT1</instr>
<desc>Fetch the line of data from memory that contains address p to a location in the cache heirarchy specified by the locality hint i.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>4059</id>
<name>_MM_PREFETCH</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>CHAR_CONST_PTR p,INT i</sign>
<instr>VPREFETCH0, VPREFETCH1, VPREFETCH2, VPREFETCHNTA, VPREFETCHE0, VPREFETCHE1, VPREFETCHE2, VPREFETCHENTA</instr>
<desc>Fetch the line of data from memory that contains address p to a location in the cache heirarchy specified by the locality hint i.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>4060</id>
<name>_MM_PREFETCH</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>CHAR_CONST_PTR p,INT i</sign>
<instr>PREFETCHNTA, PREFETCHT0, PREFETCHT1, PREFETCHT2</instr>
<desc>Fetch the line of data from memory that contains address p to a location in the cache heirarchy specified by the locality hint i.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>4115</id>
<name>_MM_RANGE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4124</id>
<name>_MM_RANGE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4139</id>
<name>_MM_RANGE_ROUND_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT imm8,INT rounding</sign>
<instr>VRANGESD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower double-precision (64-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4142</id>
<name>_MM_RANGE_ROUND_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT imm8,INT rounding</sign>
<instr>VRANGESS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for the lower single-precision (32-bit) floating-point element in a and b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[31:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4147</id>
<name>_MM_RCP_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>RCPPS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 1.5*2^-12.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4149</id>
<name>_MM_RCP_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>RCPSS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 1.5*2^-12.</desc>
<oper>dst[31:0] := APPROXIMATE(1.0/a[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>4152</id>
<name>_MM_RCP14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4161</id>
<name>_MM_RCP14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4170</id>
<name>_MM_RCP14_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VRCP14SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>dst[63:0] := APPROXIMATE(1.0/b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4173</id>
<name>_MM_RCP14_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VRCP14SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>dst[31:0] := APPROXIMATE(1.0/b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4190</id>
<name>_MM_RCP28_ROUND_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VRCP28SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>dst[63:0] := RCP_28_DP(1.0/b[63:0];
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4193</id>
<name>_MM_RCP28_ROUND_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VRCP28SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst. The maximum relative error for this approximation is less than 2^-28, and copy the upper 3 packed elements from a to the upper elements of dst. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>dst[31:0] := RCP_28_DP(1.0/b[31:0];
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4196</id>
<name>_MM_RCP28_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VRCP28SD</instr>
<desc>Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>dst[63:0] := RCP_28_DP(1.0/b[63:0];
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4199</id>
<name>_MM_RCP28_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VRCP28SS</instr>
<desc>Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>dst[31:0] := RCP_28_DP(1.0/b[31:0];
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4275</id>
<name>_MM_REDUCE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst. </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4284</id>
<name>_MM_REDUCE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst.</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4299</id>
<name>_MM_REDUCE_ROUND_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT imm8,INT rounding</sign>
<instr>VREDUCESD</instr>
<desc>Extract the reduced argument of the lower double-precision (64-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4302</id>
<name>_MM_REDUCE_ROUND_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT imm8,INT rounding</sign>
<instr>VREDUCESS</instr>
<desc>Extract the reduced argument of the lower single-precision (32-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}

dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
dst[127:64] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4305</id>
<name>_MM_REDUCE_SD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT imm8</sign>
<instr>VREDUCESD</instr>
<desc>Extract the reduced argument of the lower double-precision (64-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4308</id>
<name>_MM_REDUCE_SS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT imm8</sign>
<instr>VREDUCESS</instr>
<desc>Extract the reduced argument of the lower single-precision (32-bit) floating-point element in a by the number of bits specified by imm8, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}

dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
dst[127:64] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4309</id>
<name>_MM_REM_EPI16</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 16-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4312</id>
<name>_MM_REM_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4316</id>
<name>_MM_REM_EPI64</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 64-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4319</id>
<name>_MM_REM_EPI8</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed 8-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 15
	i := 8*j
	dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4322</id>
<name>_MM_REM_EPU16</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 16-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4325</id>
<name>_MM_REM_EPU32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4329</id>
<name>_MM_REM_EPU64</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 64-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 1
	i := 64*j
	dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4332</id>
<name>_MM_REM_EPU8</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 8-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 15
	i := 8*j
	dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4341</id>
<name>_MM_ROL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4350</id>
<name>_MM_ROL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4359</id>
<name>_MM_ROLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4368</id>
<name>_MM_ROLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4377</id>
<name>_MM_ROR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4386</id>
<name>_MM_ROR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,CONST_INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4395</id>
<name>_MM_RORV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4404</id>
<name>_MM_RORV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4415</id>
<name>_MM_ROUND_PD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,INT rounding</sign>
<instr>ROUNDPD</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a using the rounding parameter, and store the results as packed double-precision floating-point elements in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ROUND(a[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4417</id>
<name>_MM_ROUND_PS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT rounding</sign>
<instr>ROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a using the rounding parameter, and store the results as packed single-precision floating-point elements in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ROUND(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4421</id>
<name>_MM_ROUND_SD</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>ROUNDSD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in b using the rounding parameter, store the result as a double-precision floating-point element in the lower element of dst, and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := ROUND(b[63:0])
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>4422</id>
<name>_MM_ROUND_SS</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>ROUNDSS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in b using the rounding parameter, store the result as a single-precision floating-point element in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := ROUND(b[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>4429</id>
<name>_MM_ROUNDSCALE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4438</id>
<name>_MM_ROUNDSCALE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4453</id>
<name>_MM_ROUNDSCALE_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8,CONST_INT rounding</sign>
<instr>VRNDSCALESD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}		

dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4456</id>
<name>_MM_ROUNDSCALE_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8,CONST_INT rounding</sign>
<instr>VRNDSCALESS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}

dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4459</id>
<name>_MM_ROUNDSCALE_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,CONST_INT imm8</sign>
<instr>VRNDSCALESD</instr>
<desc>Round the lower double-precision (64-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}		

dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4462</id>
<name>_MM_ROUNDSCALE_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,CONST_INT imm8</sign>
<instr>VRNDSCALESS</instr>
<desc>Round the lower single-precision (32-bit) floating-point element in a to the number of fraction bits specified by imm8, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}

dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4463</id>
<name>_MM_RSQRT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>RSQRTPS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 1.5*2^-12.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4465</id>
<name>_MM_RSQRT_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>RSQRTSS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 1.5*2^-12.</desc>
<oper>dst[31:0] := APPROXIMATE(1.0 / SQRT(a[31:0]))
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>4482</id>
<name>_MM_RSQRT14_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VRSQRT14SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4485</id>
<name>_MM_RSQRT14_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VRSQRT14SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4502</id>
<name>_MM_RSQRT28_ROUND_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VRSQRT28SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>dst[63:0] := (1.0/SQRT(b[63:0]));
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4505</id>
<name>_MM_RSQRT28_ROUND_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VRSQRT28SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>dst[31:0] := (1.0/SQRT(b[31:0]));
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4508</id>
<name>_MM_RSQRT28_SD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VRSQRT28SD</instr>
<desc>Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>dst[63:0] := (1.0/SQRT(b[63:0]));
dst[127:64] := a[127:64];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4511</id>
<name>_MM_RSQRT28_SS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VRSQRT28SS</instr>
<desc>Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in b, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>dst[31:0] := (1.0/SQRT(b[31:0]));
dst[127:32] := a[127:32];
dst[MAX:128] := 0;</oper>
</intrinsic>
<intrinsic>
<id>4512</id>
<name>_MM_SAD_EPU8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSADBW</instr>
<desc>Compute the absolute differences of packed unsigned 8-bit integers in a and b, then horizontally sum each consecutive 8 differences to produce two unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low 16 bits of 64-bit elements in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
ENDFOR
FOR j := 0 to 1
	i := j*64
	dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] +
	               tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
	dst[i+63:i+16] := 0
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4515</id>
<name>_MM_SAD_PU8</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSADBW</instr>
<desc>Compute the absolute differences of packed unsigned 8-bit integers in a and b, then horizontally sum each consecutive 8 differences to produce four unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low 16 bits of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
ENDFOR

dst[15:0] := tmp[7:0] + tmp[15:8] + tmp[23:16] + tmp[31:24] + tmp[39:32] + tmp[47:40] + tmp[55:48] + tmp[63:56]
dst[63:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>4526</id>
<name>_MM_SCALEF_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4535</id>
<name>_MM_SCALEF_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4550</id>
<name>_MM_SCALEF_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VSCALEFSD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

dst[63:0] := SCALE(a[63:0], b[63:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4553</id>
<name>_MM_SCALEF_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VSCALEFSS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[63:0]
}

dst[31:0] := SCALE(a[31:0], b[31:0])
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4556</id>
<name>_MM_SCALEF_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VSCALEFSD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

dst[63:0] := SCALE(a[63:0], b[63:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4559</id>
<name>_MM_SCALEF_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VSCALEFSS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[63:0]
}

dst[31:0] := SCALE(a[31:0], b[31:0])
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4560</id>
<name>_MM_SET_EPI16</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>SHORT e7,SHORT e6,SHORT e5,SHORT e4,SHORT e3,SHORT e2,SHORT e1,SHORT e0</sign>
<instr>NONE</instr>
<desc>Set packed 16-bit integers in dst with the supplied values.</desc>
<oper>dst[15:0] := e0
dst[31:16] := e1
dst[47:32] := e2
dst[63:48] := e3
dst[79:64] := e4
dst[95:80] := e5
dst[111:96] := e6
dst[127:112] := e7</oper>
</intrinsic>
<intrinsic>
<id>4562</id>
<name>_MM_SET_EPI32</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>INT e3,INT e2,INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3</oper>
</intrinsic>
<intrinsic>
<id>4565</id>
<name>_MM_SET_EPI64</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M64 e1,__M64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1</oper>
</intrinsic>
<intrinsic>
<id>4567</id>
<name>_MM_SET_EPI64X</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>__INT64 e1,__INT64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1</oper>
</intrinsic>
<intrinsic>
<id>4569</id>
<name>_MM_SET_EPI8</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>CHAR e15,CHAR e14,CHAR e13,CHAR e12,CHAR e11,CHAR e10,CHAR e9,CHAR e8,CHAR e7,CHAR e6,CHAR e5,CHAR e4,CHAR e3,CHAR e2,CHAR e1,CHAR e0</sign>
<instr>NONE</instr>
<desc>Set packed 8-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[7:0] := e0
dst[15:8] := e1
dst[23:16] := e2
dst[31:24] := e3
dst[39:32] := e4
dst[47:40] := e5
dst[55:48] := e6
dst[63:56] := e7
dst[71:64] := e8
dst[79:72] := e9
dst[87:80] := e10
dst[95:88] := e11
dst[103:96] := e12
dst[111:104] := e13
dst[119:112] := e14
dst[127:120] := e15</oper>
</intrinsic>
<intrinsic>
<id>4571</id>
<name>_MM_SET_EXCEPTION_MASK</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr></instr>
<desc>Macro: Set the exception mask bits of the MXCSR control and status register to the value in unsigned 32-bit integer a. The exception mask may contain any of the following flags: _MM_MASK_INVALID, _MM_MASK_DIV_ZERO, _MM_MASK_DENORM, _MM_MASK_OVERFLOW, _MM_MASK_UNDERFLOW, _MM_MASK_INEXACT</desc>
<oper>MXCSR := a[31:0] AND ~_MM_MASK_MASK</oper>
</intrinsic>
<intrinsic>
<id>4572</id>
<name>_MM_SET_EXCEPTION_STATE</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr></instr>
<desc>Macro: Set the exception state bits of the MXCSR control and status register to the value in unsigned 32-bit integer a. The exception state may contain any of the following flags: _MM_EXCEPT_INVALID, _MM_EXCEPT_DIV_ZERO, _MM_EXCEPT_DENORM, _MM_EXCEPT_OVERFLOW, _MM_EXCEPT_UNDERFLOW, _MM_EXCEPT_INEXACT</desc>
<oper>MXCSR := a[31:0] AND ~_MM_EXCEPT_MASK</oper>
</intrinsic>
<intrinsic>
<id>4573</id>
<name>_MM_SET_FLUSH_ZERO_MODE</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr></instr>
<desc>Macro: Set the flush zero bits of the MXCSR control and status register to the value in unsigned 32-bit integer a. The flush zero may contain any of the following flags: _MM_FLUSH_ZERO_ON or _MM_FLUSH_ZERO_OFF</desc>
<oper>MXCSR := a[31:0] AND ~_MM_FLUSH_MASK</oper>
</intrinsic>
<intrinsic>
<id>4577</id>
<name>_MM_SET_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE e1,DOUBLE e0</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1</oper>
</intrinsic>
<intrinsic>
<id>4580</id>
<name>_MM_SET_PD1</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE a</sign>
<instr>NONE</instr>
<desc>Broadcast double-precision (64-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4581</id>
<name>_MM_SET_PI16</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>SHORT e3,SHORT e2,SHORT e1,SHORT e0</sign>
<instr>NONE</instr>
<desc>Set packed 16-bit integers in dst with the supplied values.</desc>
<oper>dst[15:0] := e0
dst[31:16] := e1
dst[47:32] := e2
dst[63:48] := e3</oper>
</intrinsic>
<intrinsic>
<id>4582</id>
<name>_MM_SET_PI32</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1</oper>
</intrinsic>
<intrinsic>
<id>4583</id>
<name>_MM_SET_PI8</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>CHAR e7,CHAR e6,CHAR e5,CHAR e4,CHAR e3,CHAR e2,CHAR e1,CHAR e0</sign>
<instr>NONE</instr>
<desc>Set packed 8-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[7:0] := e0
dst[15:8] := e1
dst[23:16] := e2
dst[31:24] := e3
dst[39:32] := e4
dst[47:40] := e5
dst[55:48] := e6
dst[63:56] := e7</oper>
</intrinsic>
<intrinsic>
<id>4584</id>
<name>_MM_SET_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT e3,FLOAT e2,FLOAT e1,FLOAT e0</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3</oper>
</intrinsic>
<intrinsic>
<id>4587</id>
<name>_MM_SET_PS1</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT a</sign>
<instr>NONE</instr>
<desc>Broadcast single-precision (32-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4588</id>
<name>_MM_SET_ROUNDING_MODE</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr></instr>
<desc>Macro: Set the rounding mode bits of the MXCSR control and status register to the value in unsigned 32-bit integer a. The rounding mode may contain any of the following flags: _MM_ROUND_NEAREST, _MM_ROUND_DOWN, _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO</desc>
<oper>MXCSR := a[31:0] AND ~_MM_ROUND_MASK</oper>
</intrinsic>
<intrinsic>
<id>4589</id>
<name>_MM_SET_SD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE a</sign>
<instr>NONE</instr>
<desc>Copy double-precision (64-bit) floating-point element a to the lower element of dst, and zero the upper element.</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>4590</id>
<name>_MM_SET_SS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT a</sign>
<instr>NONE</instr>
<desc>Copy single-precision (32-bit) floating-point element a to the lower element of dst, and zero the upper 3 elements.</desc>
<oper>dst[31:0] := a[31:0]
dst[127:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>4593</id>
<name>_MM_SET1_EPI16</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>SHORT a</sign>
<instr>NONE</instr>
<desc>Broadcast 16-bit integer a to all all elements of dst. This intrinsic may generate vpbroadcastw.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4602</id>
<name>_MM_SET1_EPI32</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>INT a</sign>
<instr>NONE</instr>
<desc>Broadcast 32-bit integer a to all elements of dst. This intrinsic may generate vpbroadcastd.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4611</id>
<name>_MM_SET1_EPI64</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M64 a</sign>
<instr>NONE</instr>
<desc>Broadcast 64-bit integer a to all elements of dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4617</id>
<name>_MM_SET1_EPI64X</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>__INT64 a</sign>
<instr>NONE</instr>
<desc>Broadcast 64-bit integer a to all elements of dst. This intrinsic may generate the vpbroadcastq.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4621</id>
<name>_MM_SET1_EPI8</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>CHAR a</sign>
<instr>NONE</instr>
<desc>Broadcast 8-bit integer a to all elements of dst. This intrinsic may generate vpbroadcastb.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4628</id>
<name>_MM_SET1_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE a</sign>
<instr>NONE</instr>
<desc>Broadcast double-precision (64-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4631</id>
<name>_MM_SET1_PI16</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>SHORT a</sign>
<instr>NONE</instr>
<desc>Broadcast 16-bit integer a to all all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4632</id>
<name>_MM_SET1_PI32</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>INT a</sign>
<instr>NONE</instr>
<desc>Broadcast 32-bit integer a to all elements of dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4633</id>
<name>_MM_SET1_PI8</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>CHAR a</sign>
<instr>NONE</instr>
<desc>Broadcast 8-bit integer a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4634</id>
<name>_MM_SET1_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT a</sign>
<instr>NONE</instr>
<desc>Broadcast single-precision (32-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4641</id>
<name>_MM_SETCSR</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr>LDMXCSR</instr>
<desc>Set the MXCSR control and status register with the value in unsigned 32-bit integer a.</desc>
<oper>MXCSR := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4642</id>
<name>_MM_SETR_EPI16</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>SHORT e7,SHORT e6,SHORT e5,SHORT e4,SHORT e3,SHORT e2,SHORT e1,SHORT e0</sign>
<instr>NONE</instr>
<desc>Set packed 16-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[15:0] := e7
dst[31:16] := e6
dst[47:32] := e5
dst[63:48] := e4
dst[79:64] := e3
dst[95:80] := e2
dst[111:96] := e1
dst[127:112] := e0</oper>
</intrinsic>
<intrinsic>
<id>4644</id>
<name>_MM_SETR_EPI32</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>INT e3,INT e2,INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e3
dst[63:32] := e2
dst[95:64] := e1
dst[127:96] := e0</oper>
</intrinsic>
<intrinsic>
<id>4647</id>
<name>_MM_SETR_EPI64</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M64 e1,__M64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[63:0] := e1
dst[127:64] := e0</oper>
</intrinsic>
<intrinsic>
<id>4650</id>
<name>_MM_SETR_EPI8</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>CHAR e15,CHAR e14,CHAR e13,CHAR e12,CHAR e11,CHAR e10,CHAR e9,CHAR e8,CHAR e7,CHAR e6,CHAR e5,CHAR e4,CHAR e3,CHAR e2,CHAR e1,CHAR e0</sign>
<instr>NONE</instr>
<desc>Set packed 8-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[7:0] := e15
dst[15:8] := e14
dst[23:16] := e13
dst[31:24] := e12
dst[39:32] := e11
dst[47:40] := e10
dst[55:48] := e9
dst[63:56] := e8
dst[71:64] := e7
dst[79:72] := e6
dst[87:80] := e5
dst[95:88] := e4
dst[103:96] := e3
dst[111:104] := e2
dst[119:112] := e1
dst[127:120] := e0</oper>
</intrinsic>
<intrinsic>
<id>4655</id>
<name>_MM_SETR_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>__m128d</ret>
<sign>DOUBLE e1,DOUBLE e0</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the supplied values in reverse order.</desc>
<oper>dst[63:0] := e1
dst[127:64] := e0</oper>
</intrinsic>
<intrinsic>
<id>4658</id>
<name>_MM_SETR_PI16</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>SHORT e3,SHORT e2,SHORT e1,SHORT e0</sign>
<instr>NONE</instr>
<desc>Set packed 16-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[15:0] := e3
dst[31:16] := e2
dst[47:32] := e1
dst[63:48] := e0</oper>
</intrinsic>
<intrinsic>
<id>4659</id>
<name>_MM_SETR_PI32</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e1
dst[63:32] := e0</oper>
</intrinsic>
<intrinsic>
<id>4660</id>
<name>_MM_SETR_PI8</name>
<cpuid>MMX, SVML</cpuid>
<ret>__m64</ret>
<sign>CHAR e7,CHAR e6,CHAR e5,CHAR e4,CHAR e3,CHAR e2,CHAR e1,CHAR e0</sign>
<instr>NONE</instr>
<desc>Set packed 8-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[7:0] := e7
dst[15:8] := e6
dst[23:16] := e5
dst[31:24] := e4
dst[39:32] := e3
dst[47:40] := e2
dst[55:48] := e1
dst[63:56] := e0</oper>
</intrinsic>
<intrinsic>
<id>4661</id>
<name>_MM_SETR_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>FLOAT e3,FLOAT e2,FLOAT e1,FLOAT e0</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e3
dst[63:32] := e2
dst[95:64] := e1
dst[127:96] := e0</oper>
</intrinsic>
<intrinsic>
<id>4670</id>
<name>_MM_SETZERO_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign></sign>
<instr>XORPD</instr>
<desc>Return vector of type __m128d with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4673</id>
<name>_MM_SETZERO_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign></sign>
<instr>XORPS</instr>
<desc>Return vector of type __m128 with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4676</id>
<name>_MM_SETZERO_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign></sign>
<instr>PXOR</instr>
<desc>Return vector of type __m128i with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4679</id>
<name>_MM_SETZERO_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign></sign>
<instr>PXOR</instr>
<desc>Return vector of type __m64 with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4680</id>
<name>_MM_SFENCE</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign></sign>
<instr>SFENCE</instr>
<desc>Perform a serializing operation on all store-to-memory instructions that were issued prior to this instruction. Guarantees that every store instruction that precedes, in program order, is globally visible before any store instruction which follows the fence in program order.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>4681</id>
<name>_MM_SHA1MSG1_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>SHA1MSG1</instr>
<desc>Perform an intermediate calculation for the next four SHA1 message values (unsigned 32-bit integers) using previous message values from a and b, and store the result in dst.</desc>
<oper>W0 := a[127:96];
W1 := a[95:64];
W2 := a[63:32];
W3 := a[31:0];
W4 := b[127:96];
W5 := b[95:64];

dst[127:96] := W2 XOR W0;
dst[95:64] := W3 XOR W1;
dst[63:32] := W4 XOR W2;
dst[31:0] := W5 XOR W3;</oper>
</intrinsic>
<intrinsic>
<id>4682</id>
<name>_MM_SHA1MSG2_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>SHA1MSG2</instr>
<desc>Perform the final calculation for the next four SHA1 message values (unsigned 32-bit integers) using the intermediate result in a and the previous message values in b, and store the result in dst.</desc>
<oper>W13 := b[95:64];
W14 := b[63:32];
W15 := b[31:0];
W16 := (a[127:96] XOR W13) &amp;lt;&amp;lt;&amp;lt; 1;
W17 := (a[95:64] XOR W14) &amp;lt;&amp;lt;&amp;lt; 1;
W18 := (a[63:32] XOR W15) &amp;lt;&amp;lt;&amp;lt; 1;
W19 := (a[31:0] XOR W16) &amp;lt;&amp;lt;&amp;lt; 1;

dst[127:96] := W16;
dst[95:64] := W17;
dst[63:32] := W18;
dst[31:0] := W19;</oper>
</intrinsic>
<intrinsic>
<id>4683</id>
<name>_MM_SHA1NEXTE_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>SHA1NEXTE</instr>
<desc>Calculate SHA1 state variable E after four rounds of operation from the current SHA1 state variable a, add that value to the scheduled values (unsigned 32-bit integers) in b, and store the result in dst.</desc>
<oper>tmp := (a[127:96] &amp;lt;&amp;lt;&amp;lt; 30);
dst[127:96] := b[127:96] + tmp;
dst[95:64] := b[95:64];
dst[63:32] := b[63:32];
dst[31:0] := b[31:0];</oper>
</intrinsic>
<intrinsic>
<id>4684</id>
<name>_MM_SHA1RNDS4_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,CONST_INT func</sign>
<instr>SHA1RNDS4</instr>
<desc>Perform four rounds of SHA1 operation using an initial SHA1 state (A,B,C,D) from a and some pre-computed sum of the next 4 round message values (unsigned 32-bit integers), and state variable E from b, and store the updated SHA1 state (A,B,C,D) in dst. func contains the logic functions and round constants.</desc>
<oper>IF (func[1:0] = 0) THEN
	f() := f0(), K := K0;
ELSE IF (func[1:0] = 1) THEN
	f() := f1(), K := K1;
ELSE IF (func[1:0] = 2) THEN
	f() := f2(), K := K2;
ELSE IF (func[1:0] = 3) THEN
	f() := f3(), K := K3;
FI;

A := a[127:96];
B := a[95:64];
C := a[63:32];
D := a[31:0];

W[0] := b[127:96];
W[1] := b[95:64];
W[2] := b[63:32];
W[3] := b[31:0];

A[1] := f(B, C, D) + (A &amp;lt;&amp;lt;&amp;lt; 5) + W[0] + K;
B[1] := A;
C[1] := B &amp;lt;&amp;lt;&amp;lt; 30;
D[1] := C;
E[1] := D;

FOR i = 1 to 3
		A[i+1] := f(B[i], C[i], D[i]) + (A[i] &amp;lt;&amp;lt;&amp;lt; 5) + W[i] + E[i] + K;
		B[i+1] := A[i];
		C[i+1] := B[i] &amp;lt;&amp;lt;&amp;lt; 30;
		D[i+1] := C[i];
		E[i+1] := D[i];
ENDFOR;

dst[127:96] := A[4];
dst[95:64] := B[4];
dst[63:32] := C[4];
dst[31:0] := D[4];</oper>
</intrinsic>
<intrinsic>
<id>4685</id>
<name>_MM_SHA256MSG1_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>SHA256MSG1</instr>
<desc>Perform an intermediate calculation for the next four SHA256 message values (unsigned 32-bit integers) using previous message values from a and b, and store the result in dst.</desc>
<oper>W4 := b[31:0];
W3 := a[127:96];
W2 := a[95:64];
W1 := a[63:32];
W0 := a[31:0];

dst[127:96] := W3 + sigma0(W4);
dst[95:64] := W2 + sigma0(W3);
dst[63:32] := W1 + sigma0(W2);
dst[31:0] := W0 + sigma0(W1);</oper>
</intrinsic>
<intrinsic>
<id>4686</id>
<name>_MM_SHA256MSG2_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>SHA256MSG2</instr>
<desc>Perform the final calculation for the next four SHA256 message values (unsigned 32-bit integers) using previous message values from a and b, and store the result in dst."</desc>
<oper>W14 := b[95:64];
W15 := b[127:96];
W16 := a[31:0] + sigma1(W14);
W17 := a[63:32] + sigma1(W15);
W18 := a[95:64] + sigma1(W16);
W19 := a[127:96] + sigma1(W17);

dst[127:96] := W19;
dst[95:64] := W18;
dst[63:32] := W17;
dst[31:0] := W16;</oper>
</intrinsic>
<intrinsic>
<id>4687</id>
<name>_MM_SHA256RNDS2_EPU32</name>
<cpuid>SHA</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,__M128I k</sign>
<instr>SHA256RNDS2</instr>
<desc>Perform 2 rounds of SHA256 operation using an initial SHA256 state (C,D,G,H) from a, an initial SHA256 state (A,B,E,F) from b, and a pre-computed sum of the next 2 round message values (unsigned 32-bit integers) and the corresponding round constants from k, and store the updated SHA256 state (A,B,E,F) in dst.</desc>
<oper>A[0] := b[127:96];
B[0] := b[95:64];
C[0] := a[127:96];
D[0] := a[95:64];
E[0] := b[63:32];
F[0] := b[31:0];
G[0] := a[63:32];
H[0] := a[31:0];

W_K0 := k[31:0];
W_K1 := k[63:32];

FOR i = 0 to 1
		A_(i+1) := Ch(E[i], F[i], G[i]) + sum1(E[i]) + WKi + H[i] + Maj(A[i], B[i], C[i]) + sum0(A[i]);
		B_(i+1) := A[i];
		C_(i+1) := B[i];
		D_(i+1) := C[i];
		E_(i+1) := Ch(E[i], F[i], G[i]) + sum1(E[i]) + WKi + H[i] + D[i];
		F_(i+1) := E[i];
		G_(i+1) := F[i];
		H_(i+1) := G[i];
ENDFOR;

dst[127:96] := A[2];
dst[95:64] := B[2];
dst[63:32] := E[2];
dst[31:0] := F[2];</oper>
</intrinsic>
<intrinsic>
<id>4690</id>
<name>_MM_SHUFFLE_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSHUFD</instr>
<desc>Shuffle 32-bit integers in a using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])</oper>
</intrinsic>
<intrinsic>
<id>4699</id>
<name>_MM_SHUFFLE_EPI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF b[i+7] == 1
		dst[i+7:i] := 0
	ELSE
		index[3:0] := b[i+3:i]
		dst[i+7:i] := a[index*8+7:index*8]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4732</id>
<name>_MM_SHUFFLE_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT imm8</sign>
<instr>SHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements using the control in imm8, and store the results in dst. </desc>
<oper>dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]</oper>
</intrinsic>
<intrinsic>
<id>4739</id>
<name>_MM_SHUFFLE_PI16</name>
<cpuid>SSE</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSHUFW</instr>
<desc>Shuffle 16-bit integers in a using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[15:0] := src[15:0]
	1:	tmp[15:0] := src[31:16]
	2:	tmp[15:0] := src[47:32]
	3:	tmp[15:0] := src[63:48]
	ESAC
	RETURN tmp[15:0]
}

dst[15:0] := SELECT4(a[63:0], imm8[1:0])
dst[31:16] := SELECT4(a[63:0], imm8[3:2])
dst[47:32] := SELECT4(a[63:0], imm8[5:4])
dst[63:48] := SELECT4(a[63:0], imm8[7:6])</oper>
</intrinsic>
<intrinsic>
<id>4740</id>
<name>_MM_SHUFFLE_PI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF b[i+7] == 1
		dst[i+7:i] := 0
	ELSE
		index[2:0] := b[i+2:i]
		dst[i+7:i] := a[index*8+7:index*8]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4743</id>
<name>_MM_SHUFFLE_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,UNSIGNED_INT imm8</sign>
<instr>SHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(b[127:0], imm8[5:4])
dst[127:96] := SELECT4(b[127:0], imm8[7:6])</oper>
</intrinsic>
<intrinsic>
<id>4752</id>
<name>_MM_SHUFFLEHI_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of a using the control in imm8. Store the results in the high 64 bits of dst, with the low 64 bits being copied from from a to dst.</desc>
<oper>dst[63:0] := a[63:0]
dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]</oper>
</intrinsic>
<intrinsic>
<id>4761</id>
<name>_MM_SHUFFLELO_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of a using the control in imm8. Store the results in the low 64 bits of dst, with the high 64 bits being copied from from a to dst.</desc>
<oper>dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>4768</id>
<name>_MM_SIGN_EPI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSIGNW</instr>
<desc>Negate packed 16-bit integers in a when the corresponding signed 16-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF b[i+15:i] &amp;lt; 0
		dst[i+15:i] := NEG(a[i+15:i])
	ELSE IF b[i+15:i] = 0
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4770</id>
<name>_MM_SIGN_EPI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSIGND</instr>
<desc>Negate packed 32-bit integers in a when the corresponding signed 32-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF b[i+31:i] &amp;lt; 0
		dst[i+31:i] := NEG(a[i+31:i])
	ELSE IF b[i+31:i] = 0
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4772</id>
<name>_MM_SIGN_EPI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSIGNB</instr>
<desc>Negate packed 8-bit integers in a when the corresponding signed 8-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF b[i+7:i] &amp;lt; 0
		dst[i+7:i] := NEG(a[i+7:i])
	ELSE IF b[i+7:i] = 0
		dst[i+7:i] := 0
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4774</id>
<name>_MM_SIGN_PI16</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSIGNW</instr>
<desc>Negate packed 16-bit integers in a when the corresponding signed 16-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF b[i+15:i] &amp;lt; 0
		dst[i+15:i] := NEG(a[i+15:i])
	ELSE IF b[i+15:i] = 0
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4775</id>
<name>_MM_SIGN_PI32</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSIGND</instr>
<desc>Negate packed 32-bit integers in a when the corresponding signed 32-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF b[i+31:i] &amp;lt; 0
		dst[i+31:i] := NEG(a[i+31:i])
	ELSE IF b[i+31:i] = 0
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4776</id>
<name>_MM_SIGN_PI8</name>
<cpuid>SSSE3</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSIGNB</instr>
<desc>Negate packed 8-bit integers in a when the corresponding signed 8-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	IF b[i+7:i] &amp;lt; 0
		dst[i+7:i] := NEG(a[i+7:i])
	ELSE IF b[i+7:i] = 0
		dst[i+7:i] := 0
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4777</id>
<name>_MM_SIN_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SIN(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4781</id>
<name>_MM_SIN_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SIN(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4785</id>
<name>_MM_SINCOS_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D_PTR mem_addr,__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the sine and cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, store the sine in dst, and store the cosine into memory at mem_addr.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SIN(a[i+63:i])
	MEM[mem_addr+i+63:mem_addr+i] := COS(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4789</id>
<name>_MM_SINCOS_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128_PTR mem_addr,__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the sine and cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, store the sine in dst, and store the cosine into memory at mem_addr.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SIN(a[i+31:i])
	MEM[mem_addr+i+31:mem_addr+i] := COS(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4793</id>
<name>_MM_SIND_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SIND(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4797</id>
<name>_MM_SIND_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SIND(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4801</id>
<name>_MM_SINH_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SINH(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4805</id>
<name>_MM_SINH_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SINH(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4811</id>
<name>_MM_SLL_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4820</id>
<name>_MM_SLL_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4829</id>
<name>_MM_SLL_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4836</id>
<name>_MM_SLL_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4837</id>
<name>_MM_SLL_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4838</id>
<name>_MM_SLL_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSLLQ</instr>
<desc>Shift 64-bit integer a left by count while shifting in zeros, and store the result in dst. </desc>
<oper>IF count[63:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;lt;&amp;lt; count[63:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>4841</id>
<name>_MM_SLLI_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4850</id>
<name>_MM_SLLI_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4859</id>
<name>_MM_SLLI_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4866</id>
<name>_MM_SLLI_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4867</id>
<name>_MM_SLLI_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4868</id>
<name>_MM_SLLI_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSLLDQ</instr>
<desc>Shift a left by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;lt;&amp;lt; (tmp*8)</oper>
</intrinsic>
<intrinsic>
<id>4870</id>
<name>_MM_SLLI_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSLLQ</instr>
<desc>Shift 64-bit integer a left by imm8 while shifting in zeros, and store the result in dst. </desc>
<oper>IF imm8[7:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;lt;&amp;lt; imm8[7:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>4873</id>
<name>_MM_SLLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4882</id>
<name>_MM_SLLV_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4891</id>
<name>_MM_SLLV_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4898</id>
<name>_MM_SPFLT_32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT r1</sign>
<instr>SPFLT</instr>
<desc>Set performance monitoring filtering mask to 32-bit unsigned integer r1.</desc>
<oper>SetPerfMonMask(r1[31:0])</oper>
</intrinsic>
<intrinsic>
<id>4899</id>
<name>_MM_SPFLT_64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>UNSIGNED__INT64 r1</sign>
<instr>SPFLT</instr>
<desc>Set performance monitoring filtering mask to 64-bit unsigned integer r1.</desc>
<oper>SetPerfMonMask(r1[63:0])</oper>
</intrinsic>
<intrinsic>
<id>4902</id>
<name>_MM_SQRT_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>SQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4911</id>
<name>_MM_SQRT_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>SQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4926</id>
<name>_MM_SQRT_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VSQRTSD</instr>
<desc>Compute the square root of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := SQRT(a[63:0])
dst[127:64] := b[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4929</id>
<name>_MM_SQRT_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VSQRTSS</instr>
<desc>Compute the square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from b to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := SQRT(a[31:0])
dst[127:32] := b[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4932</id>
<name>_MM_SQRT_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>SQRTSD</instr>
<desc>Compute the square root of the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper element from b to the upper element of dst.</desc>
<oper>dst[63:0] := SQRT(a[63:0])
dst[127:64] := b[127:64]</oper>
</intrinsic>
<intrinsic>
<id>4935</id>
<name>_MM_SQRT_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>SQRTSS</instr>
<desc>Compute the square root of the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := SQRT(a[31:0])
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>4938</id>
<name>_MM_SRA_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4947</id>
<name>_MM_SRA_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4956</id>
<name>_MM_SRA_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := SignBit
	ELSE
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4963</id>
<name>_MM_SRA_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4964</id>
<name>_MM_SRA_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4967</id>
<name>_MM_SRAI_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4976</id>
<name>_MM_SRAI_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4985</id>
<name>_MM_SRAI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := SignBit
	ELSE
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>4992</id>
<name>_MM_SRAI_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4993</id>
<name>_MM_SRAI_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4996</id>
<name>_MM_SRAV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5005</id>
<name>_MM_SRAV_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5014</id>
<name>_MM_SRAV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5023</id>
<name>_MM_SRL_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5032</id>
<name>_MM_SRL_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5041</id>
<name>_MM_SRL_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>PSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5048</id>
<name>_MM_SRL_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5049</id>
<name>_MM_SRL_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5050</id>
<name>_MM_SRL_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 count</sign>
<instr>PSRLQ</instr>
<desc>Shift 64-bit integer a right by count while shifting in zeros, and store the result in dst. </desc>
<oper>IF count[63:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;gt;&amp;gt; count[63:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>5053</id>
<name>_MM_SRLI_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5062</id>
<name>_MM_SRLI_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5071</id>
<name>_MM_SRLI_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5078</id>
<name>_MM_SRLI_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5079</id>
<name>_MM_SRLI_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5080</id>
<name>_MM_SRLI_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,INT imm8</sign>
<instr>PSRLDQ</instr>
<desc>Shift a right by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;gt;&amp;gt; (tmp*8)</oper>
</intrinsic>
<intrinsic>
<id>5082</id>
<name>_MM_SRLI_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,INT imm8</sign>
<instr>PSRLQ</instr>
<desc>Shift 64-bit integer a right by imm8 while shifting in zeros, and store the result in dst. </desc>
<oper>IF imm8[7:0] &amp;gt; 63
	dst[63:0] := 0
ELSE
	dst[63:0] := ZeroExtend(a[63:0] &amp;gt;&amp;gt; imm8[7:0])
FI</oper>
</intrinsic>
<intrinsic>
<id>5085</id>
<name>_MM_SRLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5094</id>
<name>_MM_SRLV_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5103</id>
<name>_MM_SRLV_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5119</id>
<name>_MM_STORE_PD</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>MOVAPD</instr>
<desc>Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a into memory.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5124</id>
<name>_MM_STORE_PD1</name>
<cpuid>SSE2, SVML</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>NONE</instr>
<desc>Store the lower double-precision (64-bit) floating-point element from a into 2 contiguous elements in memory. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]
MEM[mem_addr+127:mem_addr+64] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5126</id>
<name>_MM_STORE_PS</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>MOVAPS</instr>
<desc>Store 128-bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a into memory.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5131</id>
<name>_MM_STORE_PS1</name>
<cpuid>SSE, SVML</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>NONE</instr>
<desc>Store the lower single-precision (32-bit) floating-point element from a into 4 contiguous elements in memory. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]
MEM[mem_addr+63:mem_addr+32] := a[31:0]
MEM[mem_addr+95:mem_addr+64] := a[31:0]
MEM[mem_addr+127:mem_addr+96] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5133</id>
<name>_MM_STORE_SD</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>MOVSD</instr>
<desc>Store the lower double-precision (64-bit) floating-point element from a into memory. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5134</id>
<name>_MM_STORE_SI128</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>__M128I_PTR mem_addr,__M128I a</sign>
<instr>MOVDQA</instr>
<desc>Store 128-bits of integer data from a into memory. 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5138</id>
<name>_MM_STORE_SS</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>MOVSS</instr>
<desc>Store the lower single-precision (32-bit) floating-point element from a into memory. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5139</id>
<name>_MM_STORE1_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>NONE</instr>
<desc>Store the lower double-precision (64-bit) floating-point element from a into 2 contiguous elements in memory. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]
MEM[mem_addr+127:mem_addr+64] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5140</id>
<name>_MM_STORE1_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>NONE</instr>
<desc>Store the lower single-precision (32-bit) floating-point element from a into 4 contiguous elements in memory. mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]
MEM[mem_addr+63:mem_addr+32] := a[31:0]
MEM[mem_addr+95:mem_addr+64] := a[31:0]
MEM[mem_addr+127:mem_addr+96] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5144</id>
<name>_MM_STOREH_PD</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>MOVHPD</instr>
<desc>Store the upper double-precision (64-bit) floating-point element from a into memory.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>5145</id>
<name>_MM_STOREH_PI</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>__M64_PTR mem_addr,__M128 a</sign>
<instr>MOVHPS</instr>
<desc>Store the upper 2 single-precision (32-bit) floating-point elements from a into memory.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[95:64]
MEM[mem_addr+63:mem_addr+32] := a[127:96]</oper>
</intrinsic>
<intrinsic>
<id>5146</id>
<name>_MM_STOREL_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>__M128I_PTR mem_addr,__M128I a</sign>
<instr>MOVQ</instr>
<desc>Store 64-bit integer from the first element of a into memory.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5147</id>
<name>_MM_STOREL_PD</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>MOVLPD</instr>
<desc>Store the lower double-precision (64-bit) floating-point element from a into memory.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5148</id>
<name>_MM_STOREL_PI</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>__M64_PTR mem_addr,__M128 a</sign>
<instr>MOVLPS</instr>
<desc>Store the lower 2 single-precision (32-bit) floating-point elements from a into memory.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]
MEM[mem_addr+63:mem_addr+32] := a[63:32]</oper>
</intrinsic>
<intrinsic>
<id>5153</id>
<name>_MM_STORER_PD</name>
<cpuid>SSE2, SVML</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>NONE</instr>
<desc>Store 2 double-precision (64-bit) floating-point elements from a into memory in reverse order.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[127:64]
MEM[mem_addr+127:mem_addr+64] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5154</id>
<name>_MM_STORER_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>NONE</instr>
<desc>Store 4 single-precision (32-bit) floating-point elements from a into memory in reverse order.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[127:96]
MEM[mem_addr+63:mem_addr+32] := a[95:64]
MEM[mem_addr+95:mem_addr+64] := a[63:32]
MEM[mem_addr+127:mem_addr+96] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5168</id>
<name>_MM_STOREU_PD</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>MOVUPD</instr>
<desc>Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a into memory.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5174</id>
<name>_MM_STOREU_PS</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>MOVUPS</instr>
<desc>Store 128-bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a into memory.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5179</id>
<name>_MM_STOREU_SI128</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>__M128I_PTR mem_addr,__M128I a</sign>
<instr>MOVDQU</instr>
<desc>Store 128-bits of integer data from a into memory.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5180</id>
<name>_MM_STOREU_SI16</name>
<cpuid>SSE, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M128I a</sign>
<instr>NONE</instr>
<desc>Store 16-bit integer from the first element of a into memory. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+15:mem_addr] := a[15:0]</oper>
</intrinsic>
<intrinsic>
<id>5181</id>
<name>_MM_STOREU_SI16</name>
<cpuid></cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M128I a</sign>
<instr>MOVD+MOVW</instr>
<desc>Store 16-bit integer from the first element of a into memory.  mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+15:mem_addr] := a[15:0]</oper>
</intrinsic>
<intrinsic>
<id>5183</id>
<name>_MM_STOREU_SI32</name>
<cpuid></cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M128I a</sign>
<instr>MOVD</instr>
<desc>Store 32-bit integer from the first element of a into memory.  mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5184</id>
<name>_MM_STOREU_SI32</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M128I a</sign>
<instr>MOVD</instr>
<desc>Store 32-bit integer from the first element of a into memory. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5186</id>
<name>_MM_STOREU_SI64</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M128I a</sign>
<instr>MOVQ</instr>
<desc>Store 64-bit integer from the first element of a into memory. mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5187</id>
<name>_MM_STOREU_SI64</name>
<cpuid></cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M128I a</sign>
<instr>MOVQ</instr>
<desc>Store 64-bit integer from the first element of a into memory.  mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5191</id>
<name>_MM_STREAM_LOAD_SI128</name>
<cpuid>SSE4_1</cpuid>
<ret>__m128d</ret>
<sign>__M128I_PTR mem_addr</sign>
<instr>MOVNTDQA</instr>
<desc>Load 128-bits of integer data from memory into dst using a non-temporal memory hint.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[127:0] := MEM[mem_addr+127:mem_addr]</oper>
</intrinsic>
<intrinsic>
<id>5194</id>
<name>_MM_STREAM_PD</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M128D a</sign>
<instr>MOVNTPD</instr>
<desc>Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a into memory using a non-temporal memory hint.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5197</id>
<name>_MM_STREAM_PI</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>__M64_PTR mem_addr,__M64 a</sign>
<instr>MOVNTQ</instr>
<desc>Store 64-bits of integer data from a into memory using a non-temporal memory hint.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5198</id>
<name>_MM_STREAM_PS</name>
<cpuid>SSE</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M128 a</sign>
<instr>MOVNTPS</instr>
<desc>Store 128-bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a into memory using a non-temporal memory hint.
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5201</id>
<name>_MM_STREAM_SI128</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>__M128I_PTR mem_addr,__M128I a</sign>
<instr>MOVNTDQ</instr>
<desc>Store 128-bits of integer data from a into memory using a non-temporal memory hint. 
	mem_addr must be aligned on a 16-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+127:mem_addr] := a[127:0]</oper>
</intrinsic>
<intrinsic>
<id>5203</id>
<name>_MM_STREAM_SI32</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>INT_PTR mem_addr,INT a</sign>
<instr>MOVNTI</instr>
<desc>Store 32-bit integer a into memory using a non-temporal hint to minimize cache pollution. If the cache line containing address mem_addr is already in the cache, the cache will be updated.</desc>
<oper>MEM[mem_addr+31:mem_addr] := a[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5205</id>
<name>_MM_STREAM_SI64</name>
<cpuid>SSE2</cpuid>
<ret>void</ret>
<sign>__INT64_PTR mem_addr,__INT64 a</sign>
<instr>MOVNTI</instr>
<desc>Store 64-bit integer a into memory using a non-temporal hint to minimize cache pollution. If the cache line containing address mem_addr is already in the cache, the cache will be updated.</desc>
<oper>MEM[mem_addr+63:mem_addr] := a[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5208</id>
<name>_MM_SUB_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := a[i+15:i] - b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5217</id>
<name>_MM_SUB_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5226</id>
<name>_MM_SUB_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5235</id>
<name>_MM_SUB_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := a[i+7:i] - b[i+7:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5244</id>
<name>_MM_SUB_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>SUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5251</id>
<name>_MM_SUB_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := a[i+15:i] - b[i+15:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5252</id>
<name>_MM_SUB_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5253</id>
<name>_MM_SUB_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := a[i+7:i] - b[i+7:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5256</id>
<name>_MM_SUB_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>SUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5271</id>
<name>_MM_SUB_ROUND_SD</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b,INT rounding</sign>
<instr>VSUBSD</instr>
<desc>Subtract the lower double-precision (64-bit) floating-point element in b from the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[63:0] := a[63:0] - b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5274</id>
<name>_MM_SUB_ROUND_SS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b,INT rounding</sign>
<instr>VSUBSS</instr>
<desc>Subtract the lower single-precision (32-bit) floating-point element in b from the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>dst[31:0] := a[31:0] - b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5277</id>
<name>_MM_SUB_SD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>SUBSD</instr>
<desc>Subtract the lower double-precision (64-bit) floating-point element in b from the lower double-precision (64-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper element from a to the upper element of dst.</desc>
<oper>dst[63:0] := a[63:0] - b[63:0]
dst[127:64] := a[127:64]</oper>
</intrinsic>
<intrinsic>
<id>5278</id>
<name>_MM_SUB_SI64</name>
<cpuid>SSE2</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBQ</instr>
<desc>Subtract 64-bit integer b from 64-bit integer a, and store the result in dst.</desc>
<oper>dst[63:0] := a[63:0] - b[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5281</id>
<name>_MM_SUB_SS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>SUBSS</instr>
<desc>Subtract the lower single-precision (32-bit) floating-point element in b from the lower single-precision (32-bit) floating-point element in a, store the result in the lower element of dst, and copy the upper 3 packed elements from a to the upper elements of dst.</desc>
<oper>dst[31:0] := a[31:0] - b[31:0]
dst[127:32] := a[127:32]</oper>
</intrinsic>
<intrinsic>
<id>5298</id>
<name>_MM_SUBS_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5307</id>
<name>_MM_SUBS_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5316</id>
<name>_MM_SUBS_EPU16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5325</id>
<name>_MM_SUBS_EPU8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5332</id>
<name>_MM_SUBS_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5333</id>
<name>_MM_SUBS_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5334</id>
<name>_MM_SUBS_PU16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5335</id>
<name>_MM_SUBS_PU8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5338</id>
<name>_MM_SVML_CEIL_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a up to an integer value, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := CEIL(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5340</id>
<name>_MM_SVML_CEIL_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a up to an integer value, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := CEIL(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5342</id>
<name>_MM_SVML_FLOOR_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a down to an integer value, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := FLOOR(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5344</id>
<name>_MM_SVML_FLOOR_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a down to an integer value, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := FLOOR(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5346</id>
<name>_MM_SVML_ROUND_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a to the nearest integer value, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := ROUND(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5350</id>
<name>_MM_SVML_ROUND_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a to the nearest integer value, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := ROUND(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5352</id>
<name>_MM_SVML_SQRT_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. Note that this intrinsic is less efficient than _mm_sqrt_pd.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5354</id>
<name>_MM_SVML_SQRT_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. Note that this intrinsic is less efficient than _mm_sqrt_ps.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5364</id>
<name>_MM_TAN_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := TAN(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5368</id>
<name>_MM_TAN_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := TAN(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5372</id>
<name>_MM_TAND_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := TAND(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5376</id>
<name>_MM_TAND_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := TAND(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5380</id>
<name>_MM_TANH_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := TANH(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5384</id>
<name>_MM_TANH_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := TANH(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5390</id>
<name>_MM_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,__M128I c,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	FOR h := 0 to 31
		index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
		dst[i+h] := imm8[index[2:0]]
	ENDFOR
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5399</id>
<name>_MM_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b,__M128I c,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	FOR h := 0 to 63
		index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
		dst[i+h] := imm8[index[2:0]]
	ENDFOR
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5406</id>
<name>_MM_TEST_ALL_ONES</name>
<cpuid>SSE4_1, SVML</cpuid>
<ret>int</ret>
<sign>__M128I a</sign>
<instr>NONE</instr>
<desc>Compute the bitwise NOT of a and then AND with a 128-bit vector containing all 1's, and return 1 if the result is zero, otherwise return 0.</desc>
<oper>FOR j := 0 to 127
	tmp[i] := 1
ENDFOR

IF ((NOT a[127:0]) AND tmp[127:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5407</id>
<name>_MM_TEST_ALL_ZEROS</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I mask</sign>
<instr>PTEST</instr>
<desc>Compute the bitwise AND of 128 bits (representing integer data) in a and mask, and return 1 if the result is zero, otherwise return 0.</desc>
<oper>IF (a[127:0] AND mask[127:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5409</id>
<name>_MM_TEST_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTMW</instr>
<desc>Compute the bitwise AND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5415</id>
<name>_MM_TEST_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTMD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5421</id>
<name>_MM_TEST_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTMQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>5427</id>
<name>_MM_TEST_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTMB</instr>
<desc>Compute the bitwise AND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5432</id>
<name>_MM_TEST_MIX_ONES_ZEROS</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I mask</sign>
<instr>PTEST</instr>
<desc>Compute the bitwise AND of 128 bits (representing integer data) in a and mask, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with mask, and set CF to 1 if the result is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>IF (a[127:0] AND mask[127:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[127:0]) AND mask[127:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5433</id>
<name>_MM_TESTC_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VTESTPD</instr>
<desc>Compute the bitwise AND of 128 bits (representing double-precision (64-bit) floating-point elements) in a and b, producing an intermediate 128-bit value, and set ZF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set CF to 0. Return the CF value.</desc>
<oper>tmp[127:0] := a[127:0] AND b[127:0]
IF (tmp[63] == tmp[127] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[127:0] := (NOT a[127:0]) AND b[127:0]
IF (tmp[63] == tmp[127] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5435</id>
<name>_MM_TESTC_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VTESTPS</instr>
<desc>Compute the bitwise AND of 128 bits (representing single-precision (32-bit) floating-point elements) in a and b, producing an intermediate 128-bit value, and set ZF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set CF to 0. Return the CF value.</desc>
<oper>tmp[127:0] := a[127:0] AND b[127:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[127:0] := (NOT a[127:0]) AND b[127:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5437</id>
<name>_MM_TESTC_SI128</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PTEST</instr>
<desc>Compute the bitwise AND of 128 bits (representing integer data) in a and b, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero, otherwise set CF to 0. Return the CF value.</desc>
<oper>IF (a[127:0] AND b[127:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[127:0]) AND b[127:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5440</id>
<name>_MM_TESTN_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTNMW</instr>
<desc>Compute the bitwise NAND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*16
	k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5446</id>
<name>_MM_TESTN_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTNMD</instr>
<desc>Compute the bitwise NAND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	k[j] := ((a[i+31:i] NAND b[i+31:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5452</id>
<name>_MM_TESTN_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTNMQ</instr>
<desc>Compute the bitwise NAND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:2] := 0</oper>
</intrinsic>
<intrinsic>
<id>5458</id>
<name>_MM_TESTN_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M128I a,__M128I b</sign>
<instr>VPTESTNMB</instr>
<desc>Compute the bitwise NAND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5463</id>
<name>_MM_TESTNZC_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VTESTPD</instr>
<desc>Compute the bitwise AND of 128 bits (representing double-precision (64-bit) floating-point elements) in a and b, producing an intermediate 128-bit value, and set ZF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>tmp[127:0] := a[127:0] AND b[127:0]
IF (tmp[63] == tmp[127] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[127:0] := (NOT a[127:0]) AND b[127:0]
IF (tmp[63] == tmp[127] == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5465</id>
<name>_MM_TESTNZC_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VTESTPS</instr>
<desc>Compute the bitwise AND of 128 bits (representing single-precision (32-bit) floating-point elements) in a and b, producing an intermediate 128-bit value, and set ZF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>tmp[127:0] := a[127:0] AND b[127:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[127:0] := (NOT a[127:0]) AND b[127:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5467</id>
<name>_MM_TESTNZC_SI128</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PTEST</instr>
<desc>Compute the bitwise AND of 128 bits (representing integer data) in a and b, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>IF (a[127:0] AND b[127:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[127:0]) AND b[127:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5469</id>
<name>_MM_TESTZ_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>VTESTPD</instr>
<desc>Compute the bitwise AND of 128 bits (representing double-precision (64-bit) floating-point elements) in a and b, producing an intermediate 128-bit value, and set ZF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set CF to 0. Return the ZF value.</desc>
<oper>tmp[127:0] := a[127:0] AND b[127:0]
IF (tmp[63] == tmp[127] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[127:0] := (NOT a[127:0]) AND b[127:0]
IF (tmp[63] == tmp[127] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5471</id>
<name>_MM_TESTZ_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>VTESTPS</instr>
<desc>Compute the bitwise AND of 128 bits (representing single-precision (32-bit) floating-point elements) in a and b, producing an intermediate 128-bit value, and set ZF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set CF to 0. Return the ZF value.</desc>
<oper>tmp[127:0] := a[127:0] AND b[127:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[127:0] := (NOT a[127:0]) AND b[127:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5473</id>
<name>_MM_TESTZ_SI128</name>
<cpuid>SSE4_1</cpuid>
<ret>int</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PTEST</instr>
<desc>Compute the bitwise AND of 128 bits (representing integer data) in a and b, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero, otherwise set CF to 0. Return the ZF value.</desc>
<oper>IF (a[127:0] AND b[127:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[127:0]) AND b[127:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5477</id>
<name>_MM_TRANSPOSE4_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>void</ret>
<sign>__M128 row0,__M128 row1,__M128 row2,__M128 row3</sign>
<instr>NONE</instr>
<desc>Macro: Transpose the 4x4 matrix formed by the 4 rows of single-precision (32-bit) floating-point elements in row0, row1, row2, and row3, and store the transposed matrix in these vectors (row0 now contains column 0, etc.).</desc>
<oper>__m128 tmp3, tmp2, tmp1, tmp0;
tmp0 = _mm_unpacklo_ps(row0, row1);
tmp2 = _mm_unpacklo_ps(row2, row3);
tmp1 = _mm_unpackhi_ps(row0, row1);
tmp3 = _mm_unpackhi_ps(row2, row3);
row0 = _mm_movelh_ps(tmp0, tmp2);
row1 = _mm_movehl_ps(tmp2, tmp0);
row2 = _mm_movelh_ps(tmp1, tmp3);
row3 = _mm_movehl_ps(tmp3, tmp1);</oper>
</intrinsic>
<intrinsic>
<id>5478</id>
<name>_MM_TRUNC_PD</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128D a</sign>
<instr>NONE</instr>
<desc>Truncate the packed double-precision (64-bit) floating-point elements in a, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := TRUNCATE(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5482</id>
<name>_MM_TRUNC_PS</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128</ret>
<sign>__M128 a</sign>
<instr>NONE</instr>
<desc>Truncate the packed single-precision (32-bit) floating-point elements in a, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := TRUNCATE(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5486</id>
<name>_MM_TZCNT_32</name>
<cpuid>BMI1</cpuid>
<ret>int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>TZCNT</instr>
<desc>Count the number of trailing zero bits in unsigned 32-bit integer a, and return that count in dst.</desc>
<oper>tmp := 0
dst := 0
DO WHILE ((tmp &amp;lt; 32) AND a[tmp] = 0)
	tmp := tmp + 1
	dst := dst + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>5487</id>
<name>_MM_TZCNT_64</name>
<cpuid>BMI1</cpuid>
<ret>__int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>TZCNT</instr>
<desc>Count the number of trailing zero bits in unsigned 64-bit integer a, and return that count in dst.</desc>
<oper>tmp := 0
dst := 0
DO WHILE ((tmp &amp;lt; 64) AND a[tmp] = 0)
	tmp := tmp + 1
	dst := dst + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>5490</id>
<name>_MM_TZCNTI_32</name>
<cpuid>KNCNI</cpuid>
<ret>int</ret>
<sign>INT a,UNSIGNED_INT x</sign>
<instr>TZCNTI</instr>
<desc>Counts the number of trailing bits in unsigned 32-bit integer x starting at bit a storing the result in dst.</desc>
<oper>count := 0
FOR j := a to 31
	IF NOT(x[j]  1)
		count := count + 1
	FI
ENDFOR
dst := count</oper>
</intrinsic>
<intrinsic>
<id>5491</id>
<name>_MM_TZCNTI_64</name>
<cpuid>KNCNI</cpuid>
<ret>__int64</ret>
<sign>__INT64 a,UNSIGNED__INT64 x</sign>
<instr>TZCNTI</instr>
<desc>Counts the number of trailing bits in unsigned 64-bit integer x starting at bit a storing the result in dst.</desc>
<oper>count := 0
FOR j := a to 63
	IF NOT(x[j]  1)
		count := count + 1
	FI
ENDFOR
dst := count</oper>
</intrinsic>
<intrinsic>
<id>5492</id>
<name>_MM_UCOMIEQ_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for equality, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[63:0] == b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5493</id>
<name>_MM_UCOMIEQ_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for equality, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[31:0] == b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5494</id>
<name>_MM_UCOMIGE_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for greater-than-or-equal, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[63:0] &amp;gt;= b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5495</id>
<name>_MM_UCOMIGE_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for greater-than-or-equal, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[31:0] &amp;gt;= b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5496</id>
<name>_MM_UCOMIGT_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for greater-than, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[63:0] &amp;gt; b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5497</id>
<name>_MM_UCOMIGT_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for greater-than, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[31:0] &amp;gt; b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5498</id>
<name>_MM_UCOMILE_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for less-than-or-equal, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[63:0] &amp;lt;= b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5499</id>
<name>_MM_UCOMILE_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for less-than-or-equal, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[31:0] &amp;lt;= b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5500</id>
<name>_MM_UCOMILT_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for less-than, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[63:0] &amp;lt; b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5501</id>
<name>_MM_UCOMILT_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for less-than, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[31:0] &amp;lt; b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5502</id>
<name>_MM_UCOMINEQ_SD</name>
<cpuid>SSE2</cpuid>
<ret>int</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UCOMISD</instr>
<desc>Compare the lower double-precision (64-bit) floating-point element in a and b for not-equal, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[63:0] != b[63:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5503</id>
<name>_MM_UCOMINEQ_SS</name>
<cpuid>SSE</cpuid>
<ret>int</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UCOMISS</instr>
<desc>Compare the lower single-precision (32-bit) floating-point element in a and b for not-equal, and return the boolean result (0 or 1). This instruction will not signal an exception for QNaNs.</desc>
<oper>RETURN ( a[31:0] != b[31:0] ) ? 1 : 0</oper>
</intrinsic>
<intrinsic>
<id>5504</id>
<name>_MM_UDIV_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5506</id>
<name>_MM_UDIVREM_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I_PTR mem_addr,__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, store the truncated results in dst, and store the remainders as packed unsigned 32-bit integers into memory at mem_addr.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
	MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5510</id>
<name>_MM_UNDEFINED_PD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m128d with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5513</id>
<name>_MM_UNDEFINED_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m128 with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5516</id>
<name>_MM_UNDEFINED_SI128</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m128i with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5520</id>
<name>_MM_UNPACKHI_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5529</id>
<name>_MM_UNPACKHI_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5538</id>
<name>_MM_UNPACKHI_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5547</id>
<name>_MM_UNPACKHI_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5556</id>
<name>_MM_UNPACKHI_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5563</id>
<name>_MM_UNPACKHI_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLBW</instr>
<desc>Unpack and interleave 16-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[63:0], src2[63:0]){
	dst[15:0] := src1[47:32]
	dst[31:16] := src2[47:32]
	dst[47:32] := src1[63:48]
	dst[63:48] := src2[63:48]
	RETURN dst[63:0]
}

dst[63:0] := INTERLEAVE_HIGH_WORDS(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>5564</id>
<name>_MM_UNPACKHI_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of a and b, and store the results in dst.</desc>
<oper>dst[31:0] := a[63:32]
dst[63:32] := b[63:32]</oper>
</intrinsic>
<intrinsic>
<id>5565</id>
<name>_MM_UNPACKHI_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[63:0], src2[63:0]){
	dst[7:0] := src1[39:32]
	dst[15:8] := src2[39:32] 
	dst[23:16] := src1[47:40]
	dst[31:24] := src2[47:40]
	dst[39:32] := src1[55:48]
	dst[47:40] := src2[55:48]
	dst[55:48] := src1[63:56]
	dst[63:56] := src2[63:56]
	RETURN dst[63:0]
}	

dst[63:0] := INTERLEAVE_HIGH_BYTES(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>5568</id>
<name>_MM_UNPACKHI_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5577</id>
<name>_MM_UNPACKLO_EPI16</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5586</id>
<name>_MM_UNPACKLO_EPI32</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5595</id>
<name>_MM_UNPACKLO_EPI64</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5604</id>
<name>_MM_UNPACKLO_EPI8</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5613</id>
<name>_MM_UNPACKLO_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>UNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5620</id>
<name>_MM_UNPACKLO_PI16</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_WORDS(src1[63:0], src2[63:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	RETURN dst[63:0]
}	

dst[63:0] := INTERLEAVE_WORDS(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>5621</id>
<name>_MM_UNPACKLO_PI32</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>dst[31:0] := a[31:0]
dst[63:32] := b[31:0]</oper>
</intrinsic>
<intrinsic>
<id>5622</id>
<name>_MM_UNPACKLO_PI8</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_BYTES(src1[63:0], src2[63:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	RETURN dst[63:0]
}	

dst[63:0] := INTERLEAVE_BYTES(a[63:0], b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>5625</id>
<name>_MM_UNPACKLO_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>UNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5632</id>
<name>_MM_UREM_EPI32</name>
<cpuid>SSE, SVML</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>5658</id>
<name>_MM_XOR_PD</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128D a,__M128D b</sign>
<instr>XORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 1
	i := j*64
	dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5667</id>
<name>_MM_XOR_PS</name>
<cpuid>SSE</cpuid>
<ret>__m128</ret>
<sign>__M128 a,__M128 b</sign>
<instr>XORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5674</id>
<name>_MM_XOR_SI128</name>
<cpuid>SSE2</cpuid>
<ret>__m128d</ret>
<sign>__M128I a,__M128I b</sign>
<instr>PXOR</instr>
<desc>Compute the bitwise XOR of 128 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[127:0] := (a[127:0] XOR b[127:0])</oper>
</intrinsic>
<intrinsic>
<id>5677</id>
<name>_MM_XOR_SI64</name>
<cpuid>MMX</cpuid>
<ret>__m64</ret>
<sign>__M64 a,__M64 b</sign>
<instr>PXOR</instr>
<desc>Compute the bitwise XOR of 64 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[63:0] := (a[63:0] XOR b[63:0])</oper>
</intrinsic>
<intrinsic>
<id>3</id>
<name>_MM256_ABS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := ABS(a[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>12</id>
<name>_MM256_ABS_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ABS(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>21</id>
<name>_MM256_ABS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ABS(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>30</id>
<name>_MM256_ABS_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := ABS(a[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>44</id>
<name>_MM256_ACOS_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ACOS(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>48</id>
<name>_MM256_ACOS_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ACOS(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>52</id>
<name>_MM256_ACOSH_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ACOSH(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>56</id>
<name>_MM256_ACOSH_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ACOSH(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>64</id>
<name>_MM256_ADD_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := a[i+15:i] + b[i+15:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>73</id>
<name>_MM256_ADD_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>82</id>
<name>_MM256_ADD_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>91</id>
<name>_MM256_ADD_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := a[i+7:i] + b[i+7:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>100</id>
<name>_MM256_ADD_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>112</id>
<name>_MM256_ADD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>152</id>
<name>_MM256_ADDS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>161</id>
<name>_MM256_ADDS_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>170</id>
<name>_MM256_ADDS_EPU16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>179</id>
<name>_MM256_ADDS_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>198</id>
<name>_MM256_ADDSUB_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VADDSUBPD</instr>
<desc>Alternatively add and subtract packed double-precision (64-bit) floating-point elements in a to/from packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF (j is even) 
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>200</id>
<name>_MM256_ADDSUB_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VADDSUBPS</instr>
<desc>Alternatively add and subtract packed single-precision (32-bit) floating-point elements in a to/from packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF (j is even) 
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>210</id>
<name>_MM256_ALIGNR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 64-byte immediate result, shift the result right by count 32-bit elements, and store the low 32 bytes (8 elements) in dst.</desc>
<oper>temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &amp;gt;&amp;gt; (32*count)
dst[255:0] := temp[255:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>219</id>
<name>_MM256_ALIGNR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 64-byte immediate result, shift the result right by count 64-bit elements, and store the low 32 bytes (4 elements) in dst.</desc>
<oper>temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &amp;gt;&amp;gt; (64*count)
dst[255:0] := temp[255:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>228</id>
<name>_MM256_ALIGNR_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst. </desc>
<oper>FOR j := 0 to 1
	i := j*128
	tmp[255:0] := ((a[i+127:i] &amp;lt;&amp;lt; 128) OR b[i+127:i]) &amp;gt;&amp;gt; (count[7:0]*8)
	dst[i+127:i] := tmp[127:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>253</id>
<name>_MM256_AND_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>262</id>
<name>_MM256_AND_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>269</id>
<name>_MM256_AND_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPAND</instr>
<desc>Compute the bitwise AND of 256 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[255:0] := (a[255:0] AND b[255:0])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>289</id>
<name>_MM256_ANDNOT_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>298</id>
<name>_MM256_ANDNOT_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>305</id>
<name>_MM256_ANDNOT_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPANDN</instr>
<desc>Compute the bitwise NOT of 256 bits (representing integer data) in a and then AND with b, and store the result in dst.</desc>
<oper>dst[255:0] := ((NOT a[255:0]) AND b[255:0])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>309</id>
<name>_MM256_ASIN_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ASIN(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>313</id>
<name>_MM256_ASIN_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ASIN(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>317</id>
<name>_MM256_ASINH_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ASINH(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>321</id>
<name>_MM256_ASINH_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ASINH(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>325</id>
<name>_MM256_ATAN_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ATAN(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>329</id>
<name>_MM256_ATAN_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ATAN(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>333</id>
<name>_MM256_ATAN2_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>337</id>
<name>_MM256_ATAN2_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>341</id>
<name>_MM256_ATANH_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ATANH(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>345</id>
<name>_MM256_ATANH_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ATANH(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>351</id>
<name>_MM256_AVG_EPU16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>360</id>
<name>_MM256_AVG_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>386</id>
<name>_MM256_BLEND_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPBLENDW</instr>
<desc>Blend packed 16-bit integers from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF imm8[j%8]
		dst[i+15:i] := b[i+15:i]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>391</id>
<name>_MM256_BLEND_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPBLENDD</instr>
<desc>Blend packed 32-bit integers from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF imm8[j%8]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>402</id>
<name>_MM256_BLEND_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VBLENDPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF imm8[j%8]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>407</id>
<name>_MM256_BLEND_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VBLENDPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using control mask imm8, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF imm8[j%8]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>411</id>
<name>_MM256_BLENDV_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,__M256I mask</sign>
<instr>VPBLENDVB</instr>
<desc>Blend packed 8-bit integers from a and b using mask, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF mask[i+7]
		dst[i+7:i] := b[i+7:i]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>413</id>
<name>_MM256_BLENDV_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D mask</sign>
<instr>VBLENDVPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using mask, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>415</id>
<name>_MM256_BLENDV_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 mask</sign>
<instr>VBLENDVPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using mask, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>432</id>
<name>_MM256_BROADCAST_F32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTF32X2</instr>
<desc>Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 2)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>438</id>
<name>_MM256_BROADCAST_F32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTF32X4</instr>
<desc>Broadcast the 4 packed single-precision (32-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 4)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>447</id>
<name>_MM256_BROADCAST_F64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M128D a</sign>
<instr>VBROADCASTF64X2</instr>
<desc>Broadcast the 2 packed double-precision (64-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	n := (j mod 2)*64
	dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>459</id>
<name>_MM256_BROADCAST_I32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of "dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 2)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>465</id>
<name>_MM256_BROADCAST_I32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI32X4</instr>
<desc>Broadcast the 4 packed 32-bit integers from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 4)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>474</id>
<name>_MM256_BROADCAST_I64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI64X2</instr>
<desc>Broadcast the 2 packed 64-bit integers from a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	n := (j mod 2)*64
	dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>483</id>
<name>_MM256_BROADCAST_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M128D_CONST_PTR mem_addr</sign>
<instr>VBROADCASTF128</instr>
<desc>Broadcast 128 bits from memory (composed of 2 packed double-precision (64-bit) floating-point elements) to all elements of dst.</desc>
<oper>tmp[127:0] = MEM[mem_addr+127:mem_addr]
dst[127:0] := tmp[127:0]
dst[255:128] := tmp[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>484</id>
<name>_MM256_BROADCAST_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M128_CONST_PTR mem_addr</sign>
<instr>VBROADCASTF128</instr>
<desc>Broadcast 128 bits from memory (composed of 4 packed single-precision (32-bit) floating-point elements) to all elements of dst.</desc>
<oper>tmp[127:0] = MEM[mem_addr+127:mem_addr]
dst[127:0] := tmp[127:0]
dst[255:128] := tmp[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>485</id>
<name>_MM256_BROADCAST_SD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast a double-precision (64-bit) floating-point element from memory to all elements of dst.</desc>
<oper>tmp[63:0] = MEM[mem_addr+63:mem_addr]
FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>487</id>
<name>_MM256_BROADCAST_SS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast a single-precision (32-bit) floating-point element from memory to all elements of dst.</desc>
<oper>tmp[31:0] = MEM[mem_addr+31:mem_addr]
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := tmp[31:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>491</id>
<name>_MM256_BROADCASTB_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>500</id>
<name>_MM256_BROADCASTD_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>507</id>
<name>_MM256_BROADCASTMB_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k</sign>
<instr>VPBROADCASTMB2Q</instr>
<desc>Broadcast the low 8-bits from input mask k to all 64-bit elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ZeroExtend(k[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>510</id>
<name>_MM256_BROADCASTMW_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k</sign>
<instr>VPBROADCASTMW2D</instr>
<desc>Broadcast the low 16-bits from input mask k to all 32-bit elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ZeroExtend(k[15:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>515</id>
<name>_MM256_BROADCASTQ_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>522</id>
<name>_MM256_BROADCASTSD_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m256d</ret>
<sign>__M128D a</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>528</id>
<name>_MM256_BROADCASTSI128_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI128</instr>
<desc>Broadcast 128 bits of integer data from a to all 128-bit lanes in dst.
	</desc>
<oper>dst[127:0] := a[127:0]
dst[255:128] := a[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>532</id>
<name>_MM256_BROADCASTSS_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m256</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>541</id>
<name>_MM256_BROADCASTW_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>547</id>
<name>_MM256_BSLLI_EPI128</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSLLDQ</instr>
<desc>Shift 128-bit lanes in a left by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;lt;&amp;lt; (tmp*8)
dst[255:128] := a[255:128] &amp;lt;&amp;lt; (tmp*8)
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>550</id>
<name>_MM256_BSRLI_EPI128</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSRLDQ</instr>
<desc>Shift 128-bit lanes in a right by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;gt;&amp;gt; (tmp*8)
dst[255:128] := a[255:128] &amp;gt;&amp;gt; (tmp*8)
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>560</id>
<name>_MM256_CASTPD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256D a</sign>
<instr></instr>
<desc>Cast vector of type __m256d to type __m256.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>563</id>
<name>_MM256_CASTPD_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256D a</sign>
<instr></instr>
<desc>Casts vector of type __m256d to type __m256i. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>565</id>
<name>_MM256_CASTPD128_PD256</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M128D a</sign>
<instr></instr>
<desc>Casts vector of type __m128d to type __m256d; the upper 128 bits of the result are undefined. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>567</id>
<name>_MM256_CASTPD256_PD128</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M256D a</sign>
<instr></instr>
<desc>Casts vector of type __m256d to type __m128d. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>572</id>
<name>_MM256_CASTPS_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256 a</sign>
<instr></instr>
<desc>Cast vector of type __m256 to type __m256d.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>575</id>
<name>_MM256_CASTPS_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256 a</sign>
<instr></instr>
<desc>Casts vector of type __m256 to type __m256i. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>577</id>
<name>_MM256_CASTPS128_PS256</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M128 a</sign>
<instr></instr>
<desc>Casts vector of type __m128 to type __m256; the upper 128 bits of the result are undefined. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>579</id>
<name>_MM256_CASTPS256_PS128</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M256 a</sign>
<instr></instr>
<desc>Casts vector of type __m256 to type __m128. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>585</id>
<name>_MM256_CASTSI128_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr></instr>
<desc>Casts vector of type __m128i to type __m256i; the upper 128 bits of the result are undefined. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>587</id>
<name>_MM256_CASTSI256_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256I a</sign>
<instr></instr>
<desc>Casts vector of type __m256i to type __m256d. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>588</id>
<name>_MM256_CASTSI256_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256I a</sign>
<instr></instr>
<desc>Casts vector of type __m256i to type __m256. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>589</id>
<name>_MM256_CASTSI256_SI128</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr></instr>
<desc>Casts vector of type __m256i to type __m128i. This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.
	</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>598</id>
<name>_MM256_CBRT_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := CubeRoot(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>602</id>
<name>_MM256_CBRT_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := CubeRoot(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>606</id>
<name>_MM256_CDFNORM_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := CDFNormal(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>610</id>
<name>_MM256_CDFNORM_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := CDFNormal(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>614</id>
<name>_MM256_CDFNORMINV_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := InverseCDFNormal(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>618</id>
<name>_MM256_CDFNORMINV_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := InverseCDFNormal(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>622</id>
<name>_MM256_CEIL_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>VROUNDPD</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a up to an integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := CEIL(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>626</id>
<name>_MM256_CEIL_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a up to an integer value, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := CEIL(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>632</id>
<name>_MM256_CEXP_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed complex single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := e^(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>638</id>
<name>_MM256_CLOG_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed complex single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ln(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>641</id>
<name>_MM256_CMP_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>647</id>
<name>_MM256_CMP_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>653</id>
<name>_MM256_CMP_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>659</id>
<name>_MM256_CMP_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>665</id>
<name>_MM256_CMP_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>671</id>
<name>_MM256_CMP_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>677</id>
<name>_MM256_CMP_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>683</id>
<name>_MM256_CMP_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>688</id>
<name>_MM256_CMP_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in dst.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ( a[i+63:i] OP b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>691</id>
<name>_MM256_CMP_PD_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
	i := j*64
	k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>696</id>
<name>_MM256_CMP_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in dst.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ( a[i+31:i] OP b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>699</id>
<name>_MM256_CMP_PS_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*32
	k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>718</id>
<name>_MM256_CMPEQ_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPEQW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>721</id>
<name>_MM256_CMPEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>726</id>
<name>_MM256_CMPEQ_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPEQD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>729</id>
<name>_MM256_CMPEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>734</id>
<name>_MM256_CMPEQ_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPEQQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ( a[i+63:i] == b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>737</id>
<name>_MM256_CMPEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>742</id>
<name>_MM256_CMPEQ_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPEQB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>745</id>
<name>_MM256_CMPEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>751</id>
<name>_MM256_CMPEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>757</id>
<name>_MM256_CMPEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>763</id>
<name>_MM256_CMPEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>769</id>
<name>_MM256_CMPEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>793</id>
<name>_MM256_CMPGE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>799</id>
<name>_MM256_CMPGE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>805</id>
<name>_MM256_CMPGE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>811</id>
<name>_MM256_CMPGE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>817</id>
<name>_MM256_CMPGE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>823</id>
<name>_MM256_CMPGE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>829</id>
<name>_MM256_CMPGE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>835</id>
<name>_MM256_CMPGE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>844</id>
<name>_MM256_CMPGT_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPGTW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 0xFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>847</id>
<name>_MM256_CMPGT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>852</id>
<name>_MM256_CMPGT_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 0xFFFFFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>855</id>
<name>_MM256_CMPGT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>860</id>
<name>_MM256_CMPGT_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPGTQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>863</id>
<name>_MM256_CMPGT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>868</id>
<name>_MM256_CMPGT_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPGTB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 0xFF : 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>871</id>
<name>_MM256_CMPGT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>877</id>
<name>_MM256_CMPGT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>883</id>
<name>_MM256_CMPGT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>889</id>
<name>_MM256_CMPGT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>895</id>
<name>_MM256_CMPGT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>915</id>
<name>_MM256_CMPLE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>921</id>
<name>_MM256_CMPLE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>927</id>
<name>_MM256_CMPLE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>933</id>
<name>_MM256_CMPLE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>939</id>
<name>_MM256_CMPLE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>945</id>
<name>_MM256_CMPLE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>951</id>
<name>_MM256_CMPLE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>957</id>
<name>_MM256_CMPLE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>972</id>
<name>_MM256_CMPLT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>979</id>
<name>_MM256_CMPLT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>987</id>
<name>_MM256_CMPLT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>994</id>
<name>_MM256_CMPLT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1000</id>
<name>_MM256_CMPLT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1006</id>
<name>_MM256_CMPLT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1012</id>
<name>_MM256_CMPLT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1018</id>
<name>_MM256_CMPLT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1032</id>
<name>_MM256_CMPNEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1038</id>
<name>_MM256_CMPNEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1044</id>
<name>_MM256_CMPNEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1050</id>
<name>_MM256_CMPNEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1056</id>
<name>_MM256_CMPNEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1062</id>
<name>_MM256_CMPNEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1068</id>
<name>_MM256_CMPNEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1074</id>
<name>_MM256_CMPNEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1179</id>
<name>_MM256_CONFLICT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit. Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	FOR k := 0 to j-1
		m := k*32
		dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
	ENDFOR
	dst[i+31:i+j] := 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1188</id>
<name>_MM256_CONFLICT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit. Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	FOR k := 0 to j-1
		m := k*64
		dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
	ENDFOR
	dst[i+63:i+j] := 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1195</id>
<name>_MM256_COS_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := COS(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1199</id>
<name>_MM256_COS_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := COS(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1203</id>
<name>_MM256_COSD_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := COSD(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1207</id>
<name>_MM256_COSD_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := COSD(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1211</id>
<name>_MM256_COSH_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := COSH(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1215</id>
<name>_MM256_COSH_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := COSH(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1225</id>
<name>_MM256_CSQRT_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the square root of packed complex single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1320</id>
<name>_MM256_CVTEPI16_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j:= 0 to 7
	i := 32*j
	k := 16*j
	dst[i+31:i] := SignExtend(a[k+15:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1329</id>
<name>_MM256_CVTEPI16_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j:= 0 to 3
	i := 64*j
	k := 16*j
	dst[i+63:i] := SignExtend(a[k+15:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1338</id>
<name>_MM256_CVTEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1350</id>
<name>_MM256_CVTEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 16*j
	dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1359</id>
<name>_MM256_CVTEPI32_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j:= 0 to 3
	i := 64*j
	k := 32*j
	dst[i+63:i] := SignExtend(a[k+31:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1368</id>
<name>_MM256_CVTEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 8*j
	dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1377</id>
<name>_MM256_CVTEPI32_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M128I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1386</id>
<name>_MM256_CVTEPI32_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1403</id>
<name>_MM256_CVTEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 16*j
	dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1412</id>
<name>_MM256_CVTEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 32*j
	dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1421</id>
<name>_MM256_CVTEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 8*j
	dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1430</id>
<name>_MM256_CVTEPI64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1439</id>
<name>_MM256_CVTEPI64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M256I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1457</id>
<name>_MM256_CVTEPI8_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	l := j*16
	dst[l+15:l] := SignExtend(a[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1466</id>
<name>_MM256_CVTEPI8_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 8*j
	dst[i+31:i] := SignExtend(a[k+7:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1475</id>
<name>_MM256_CVTEPI8_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 8*j
	dst[i+63:i] := SignExtend(a[k+7:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1484</id>
<name>_MM256_CVTEPU16_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 16*j
	dst[i+31:i] := ZeroExtend(a[k+15:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1493</id>
<name>_MM256_CVTEPU16_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j:= 0 to 3
	i := 64*j
	k := 16*j
	dst[i+63:i] := ZeroExtend(a[k+15:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1502</id>
<name>_MM256_CVTEPU32_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j:= 0 to 3
	i := 64*j
	k := 32*j
	dst[i+63:i] := ZeroExtend(a[k+31:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1511</id>
<name>_MM256_CVTEPU32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M128I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1525</id>
<name>_MM256_CVTEPU64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1534</id>
<name>_MM256_CVTEPU64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M256I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1543</id>
<name>_MM256_CVTEPU8_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	l := j*16
	dst[l+15:l] := ZeroExtend(a[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1552</id>
<name>_MM256_CVTEPU8_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 8*j
	dst[i+31:i] := ZeroExtend(a[k+7:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1561</id>
<name>_MM256_CVTEPU8_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 byte sof a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 8*j
	dst[i+63:i] := ZeroExtend(a[k+7:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1584</id>
<name>_MM256_CVTPD_EPI32</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M256D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1593</id>
<name>_MM256_CVTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1602</id>
<name>_MM256_CVTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1611</id>
<name>_MM256_CVTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1621</id>
<name>_MM256_CVTPD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M256D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1632</id>
<name>_MM256_CVTPH_PS</name>
<cpuid>FP16C</cpuid>
<ret>__m256</ret>
<sign>__M128I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*16
	dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1646</id>
<name>_MM256_CVTPS_EPI32</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1655</id>
<name>_MM256_CVTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1664</id>
<name>_MM256_CVTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1673</id>
<name>_MM256_CVTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1680</id>
<name>_MM256_CVTPS_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M128 a</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 32*j
	dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1687</id>
<name>_MM256_CVTPS_PH</name>
<cpuid>FP16C</cpuid>
<ret>__m128d</ret>
<sign>__M256 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 32*j
	dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1714</id>
<name>_MM256_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1726</id>
<name>_MM256_CVTSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 16*j
	dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1735</id>
<name>_MM256_CVTSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 8*j
	dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1750</id>
<name>_MM256_CVTSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 16*j
	dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1759</id>
<name>_MM256_CVTSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 32*j
	dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1768</id>
<name>_MM256_CVTSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 8*j
	dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1850</id>
<name>_MM256_CVTTPD_EPI32</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M256D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1859</id>
<name>_MM256_CVTTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1868</id>
<name>_MM256_CVTTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1877</id>
<name>_MM256_CVTTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1887</id>
<name>_MM256_CVTTPS_EPI32</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1896</id>
<name>_MM256_CVTTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1905</id>
<name>_MM256_CVTTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1914</id>
<name>_MM256_CVTTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M128 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1941</id>
<name>_MM256_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1953</id>
<name>_MM256_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 16*j
	dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1962</id>
<name>_MM256_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 8*j
	dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1977</id>
<name>_MM256_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 16*j
	dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1986</id>
<name>_MM256_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 32*j
	dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1995</id>
<name>_MM256_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	k := 8*j
	dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>2013</id>
<name>_MM256_DBSAD_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst.
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected from within 128-bit lanes according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>FOR j := 0 to 1
	i := j*128
	tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
	tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
	tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
	tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
ENDFOR

FOR j := 0 to 3
	i := j*64
	dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2022</id>
<name>_MM256_DIV_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 16-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2025</id>
<name>_MM256_DIV_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2029</id>
<name>_MM256_DIV_EPI64</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 64-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2032</id>
<name>_MM256_DIV_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 8-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 31
	i := 8*j
	dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2035</id>
<name>_MM256_DIV_EPU16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 16-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2038</id>
<name>_MM256_DIV_EPU32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2042</id>
<name>_MM256_DIV_EPU64</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 64-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2045</id>
<name>_MM256_DIV_EPU8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 8-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 31
	i := 8*j
	dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2050</id>
<name>_MM256_DIV_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	dst[i+63:i] := a[i+63:i] / b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2059</id>
<name>_MM256_DIV_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := a[i+31:i] / b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2085</id>
<name>_MM256_DP_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VDPPS</instr>
<desc>Conditionally multiply the packed single-precision (32-bit) floating-point elements in a and b using the high 4 bits in imm8, sum the four products, and conditionally store the sum in dst using the low 4 bits of imm8.</desc>
<oper>DP(a[127:0], b[127:0], imm8[7:0]) {
	FOR j := 0 to 3
		i := j*32
		IF imm8[(4+j)%8]
			temp[i+31:i] := a[i+31:i] * b[i+31:i]
		ELSE
			temp[i+31:i] := 0
		FI
	ENDFOR
	
	sum[31:0] := (temp[127:96] + temp[95:64]) + (temp[63:32] + temp[31:0])
	
	FOR j := 0 to 3
		i := j*32
		IF imm8[j%8]
			tmpdst[i+31:i] := sum[31:0]
		ELSE
			tmpdst[i+31:i] := 0
		FI
	ENDFOR
	RETURN tmpdst[127:0]
}

dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])
dst[255:128] := DP(a[255:128], b[255:128], imm8[7:0])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2089</id>
<name>_MM256_ERF_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ERF(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2093</id>
<name>_MM256_ERF_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ERF(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2097</id>
<name>_MM256_ERFC_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := 1.0 - ERF(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2101</id>
<name>_MM256_ERFC_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := 1.0 - ERF(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2105</id>
<name>_MM256_ERFCINV_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2109</id>
<name>_MM256_ERFCINV_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2113</id>
<name>_MM256_ERFINV_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := 1.0 / ERF(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2117</id>
<name>_MM256_ERFINV_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := 1.0 / ERF(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2121</id>
<name>_MM256_EXP_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := e^(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2125</id>
<name>_MM256_EXP_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := e^(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2129</id>
<name>_MM256_EXP10_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := 10^(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2133</id>
<name>_MM256_EXP10_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := 10^(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2137</id>
<name>_MM256_EXP2_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := 2^(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2141</id>
<name>_MM256_EXP2_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := 2^(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2207</id>
<name>_MM256_EXPM1_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, subtract one from each element, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := e^(a[i+63:i]) - 1.0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2211</id>
<name>_MM256_EXPM1_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, subtract one from each element, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := e^(a[i+31:i]) - 1.0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2255</id>
<name>_MM256_EXTRACT_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__int16</ret>
<sign>__M256I a,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Extract a 16-bit integer from a, selected with index, and store the result in dst.</desc>
<oper>dst[15:0] := (a[255:0] &amp;gt;&amp;gt; (index * 16))[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2257</id>
<name>_MM256_EXTRACT_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__int32</ret>
<sign>__M256I a,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Extract a 32-bit integer from a, selected with index, and store the result in dst.</desc>
<oper>dst[31:0] := (a[255:0] &amp;gt;&amp;gt; (index * 32))[31:0]</oper>
</intrinsic>
<intrinsic>
<id>2259</id>
<name>_MM256_EXTRACT_EPI64</name>
<cpuid>AVX, SVML</cpuid>
<ret>__int64</ret>
<sign>__M256I a,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Extract a 64-bit integer from a, selected with index, and store the result in dst.</desc>
<oper>dst[63:0] := (a[255:0] &amp;gt;&amp;gt; (index * 64))[63:0]</oper>
</intrinsic>
<intrinsic>
<id>2261</id>
<name>_MM256_EXTRACT_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__int8</ret>
<sign>__M256I a,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Extract an 8-bit integer from a, selected with index, and store the result in dst.</desc>
<oper>dst[7:0] := (a[255:0] &amp;gt;&amp;gt; (index * 8))[7:0]</oper>
</intrinsic>
<intrinsic>
<id>2264</id>
<name>_MM256_EXTRACTF128_PD</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M256D a,CONST_INT imm8</sign>
<instr>VEXTRACTF128</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2265</id>
<name>_MM256_EXTRACTF128_PS</name>
<cpuid>AVX</cpuid>
<ret>__m128</ret>
<sign>__M256 a,CONST_INT imm8</sign>
<instr>VEXTRACTF128</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2266</id>
<name>_MM256_EXTRACTF128_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m128d</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VEXTRACTF128</instr>
<desc>Extract 128 bits (composed of integer data) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2267</id>
<name>_MM256_EXTRACTF32X4_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M256 a,INT imm8</sign>
<instr>VEXTRACTF32X4</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2276</id>
<name>_MM256_EXTRACTF64X2_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256D a,INT imm8</sign>
<instr>VEXTRACTF64X2</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2285</id>
<name>_MM256_EXTRACTI128_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VEXTRACTI128</instr>
<desc>Extract 128 bits (composed of integer data) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2286</id>
<name>_MM256_EXTRACTI32X4_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VEXTRACTI32X4</instr>
<desc>Extract 128 bits (composed of 4 packed 32-bit integers) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2295</id>
<name>_MM256_EXTRACTI64X2_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VEXTRACTI64X2</instr>
<desc>Extract 128 bits (composed of 2 packed 64-bit integers) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2315</id>
<name>_MM256_FIXUPIMM_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2324</id>
<name>_MM256_FIXUPIMM_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2353</id>
<name>_MM256_FLOOR_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>VROUNDPD</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a down to an integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := FLOOR(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2357</id>
<name>_MM256_FLOOR_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a down to an integer value, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := FLOOR(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2369</id>
<name>_MM256_FMADD_PD</name>
<cpuid>FMA</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2381</id>
<name>_MM256_FMADD_PS</name>
<cpuid>FMA</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2421</id>
<name>_MM256_FMADDSUB_PD</name>
<cpuid>FMA</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2433</id>
<name>_MM256_FMADDSUB_PS</name>
<cpuid>FMA</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2453</id>
<name>_MM256_FMSUB_PD</name>
<cpuid>FMA</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2465</id>
<name>_MM256_FMSUB_PS</name>
<cpuid>FMA</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2499</id>
<name>_MM256_FMSUBADD_PD</name>
<cpuid>FMA</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2511</id>
<name>_MM256_FMSUBADD_PS</name>
<cpuid>FMA</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2531</id>
<name>_MM256_FNMADD_PD</name>
<cpuid>FMA</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2543</id>
<name>_MM256_FNMADD_PS</name>
<cpuid>FMA</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2577</id>
<name>_MM256_FNMSUB_PD</name>
<cpuid>FMA</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2589</id>
<name>_MM256_FNMSUB_PS</name>
<cpuid>FMA</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2621</id>
<name>_MM256_FPCLASS_PD_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256D a,INT imm8</sign>
<instr>VFPCLASSPD</instr>
<desc>Test packed double-precision (64-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2627</id>
<name>_MM256_FPCLASS_PS_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256 a,INT imm8</sign>
<instr>VFPCLASSPS</instr>
<desc>Test packed single-precision (32-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2650</id>
<name>_MM256_GETEXP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2659</id>
<name>_MM256_GETEXP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2686</id>
<name>_MM256_GETMANT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2695</id>
<name>_MM256_GETMANT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2730</id>
<name>_MM256_HADD_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPHADDW</instr>
<desc>Horizontally add adjacent pairs of 16-bit integers in a and b, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0] := a[31:16] + a[15:0]
dst[31:16] := a[63:48] + a[47:32]
dst[47:32] := a[95:80] + a[79:64]
dst[63:48] := a[127:112] + a[111:96]
dst[79:64] := b[31:16] + b[15:0]
dst[95:80] := b[63:48] + b[47:32]
dst[111:96] := b[95:80] + b[79:64]
dst[127:112] := b[127:112] + b[111:96]
dst[143:128] := a[159:144] + a[143:128]
dst[159:144] := a[191:176] + a[175:160]
dst[175:160] := a[223:208] + a[207:192]
dst[191:176] := a[255:240] + a[239:224]
dst[207:192] := b[127:112] + b[143:128]
dst[223:208] := b[159:144] + b[175:160]
dst[239:224] := b[191:176] + b[207:192]
dst[255:240] := b[223:208] + b[239:224]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2732</id>
<name>_MM256_HADD_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPHADDD</instr>
<desc>Horizontally add adjacent pairs of 32-bit integers in a and b, and pack the signed 32-bit results in dst.</desc>
<oper>dst[31:0] := a[63:32] + a[31:0]
dst[63:32] := a[127:96] + a[95:64]
dst[95:64] := b[63:32] + b[31:0]
dst[127:96] := b[127:96] + b[95:64]
dst[159:128] := a[191:160] + a[159:128]
dst[191:160] := a[255:224] + a[223:192]
dst[223:192] := b[191:160] + b[159:128]
dst[255:224] := b[255:224] + b[223:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2734</id>
<name>_MM256_HADD_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VHADDPD</instr>
<desc>Horizontally add adjacent pairs of double-precision (64-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[63:0] := a[127:64] + a[63:0]
dst[127:64] := b[127:64] + b[63:0]
dst[191:128] := a[255:192] + a[191:128]
dst[255:192] := b[255:192] + b[191:128]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2738</id>
<name>_MM256_HADD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VHADDPS</instr>
<desc>Horizontally add adjacent pairs of single-precision (32-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[31:0] := a[63:32] + a[31:0]
dst[63:32] := a[127:96] + a[95:64]
dst[95:64] := b[63:32] + b[31:0]
dst[127:96] := b[127:96] + b[95:64]
dst[159:128] := a[191:160] + a[159:128]
dst[191:160] := a[255:224] + a[223:192]
dst[223:192] := b[191:160] + b[159:128]
dst[255:224] := b[255:224] + b[223:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2740</id>
<name>_MM256_HADDS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPHADDSW</instr>
<desc>Horizontally add adjacent pairs of 16-bit integers in a and b using saturation, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0]= Saturate_To_Int16(a[31:16] + a[15:0])
dst[31:16] = Saturate_To_Int16(a[63:48] + a[47:32])
dst[47:32] = Saturate_To_Int16(a[95:80] + a[79:64])
dst[63:48] = Saturate_To_Int16(a[127:112] + a[111:96])
dst[79:64] = Saturate_To_Int16(b[31:16] + b[15:0])
dst[95:80] = Saturate_To_Int16(b[63:48] + b[47:32])
dst[111:96] = Saturate_To_Int16(b[95:80] + b[79:64])
dst[127:112] = Saturate_To_Int16(b[127:112] + b[111:96])
dst[143:128] = Saturate_To_Int16(a[159:144] + a[143:128])
dst[159:144] = Saturate_To_Int16(a[191:176] + a[175:160])
dst[175:160] = Saturate_To_Int16( a[223:208] + a[207:192])
dst[191:176] = Saturate_To_Int16(a[255:240] + a[239:224])
dst[207:192] = Saturate_To_Int16(b[127:112] + b[143:128])
dst[223:208] = Saturate_To_Int16(b[159:144] + b[175:160])
dst[239:224] = Saturate_To_Int16(b[191-160] + b[159-128])
dst[255:240] = Saturate_To_Int16(b[255:240] + b[239:224])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2743</id>
<name>_MM256_HSUB_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPHSUBW</instr>
<desc>Horizontally subtract adjacent pairs of 16-bit integers in a and b, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0] := a[15:0] - a[31:16]
dst[31:16] := a[47:32] - a[63:48]
dst[47:32] := a[79:64] - a[95:80]
dst[63:48] := a[111:96] - a[127:112]
dst[79:64] := b[15:0] - b[31:16]
dst[95:80] := b[47:32] - b[63:48]
dst[111:96] := b[79:64] - b[95:80]
dst[127:112] := b[111:96] - b[127:112]
dst[143:128] := a[143:128] - a[159:144]
dst[159:144] := a[175:160] - a[191:176]
dst[175:160] := a[207:192] - a[223:208]
dst[191:176] := a[239:224] - a[255:240]
dst[207:192] := b[143:128] - b[159:144]
dst[223:208] := b[175:160] - b[191:176]
dst[239:224] := b[207:192] - b[223:208]
dst[255:240] := b[239:224] - b[255:240]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2745</id>
<name>_MM256_HSUB_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPHSUBD</instr>
<desc>Horizontally subtract adjacent pairs of 32-bit integers in a and b, and pack the signed 32-bit results in dst.</desc>
<oper>dst[31:0] := a[31:0] - a[63:32]
dst[63:32] := a[95:64] - a[127:96]
dst[95:64] := b[31:0] - b[63:32]
dst[127:96] := b[95:64] - b[127:96]
dst[159:128] := a[159:128] - a[191:160]
dst[191:160] := a[223:192] - a[255:224]
dst[223:192] := b[159:128] - b[191:160]
dst[255:224] := b[223:192] - b[255:224]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2747</id>
<name>_MM256_HSUB_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VHSUBPD</instr>
<desc>Horizontally subtract adjacent pairs of double-precision (64-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[63:0] := a[63:0] - a[127:64]
dst[127:64] := b[63:0] - b[127:64]
dst[191:128] := a[191:128] - a[255:192]
dst[255:192] := b[191:128] - b[255:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2751</id>
<name>_MM256_HSUB_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VHSUBPS</instr>
<desc>Horizontally add adjacent pairs of single-precision (32-bit) floating-point elements in a and b, and pack the results in dst.</desc>
<oper>dst[31:0] := a[31:0] - a[63:32]
dst[63:32] := a[95:64] - a[127:96]
dst[95:64] := b[31:0] - b[63:32]
dst[127:96] := b[95:64] - b[127:96]
dst[159:128] := a[159:128] - a[191:160]
dst[191:160] := a[223:192] - a[255:224]
dst[223:192] := b[159:128] - b[191:160]
dst[255:224] := b[223:192] - b[255:224]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2753</id>
<name>_MM256_HSUBS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPHSUBSW</instr>
<desc>Horizontally subtract adjacent pairs of 16-bit integers in a and b using saturation, and pack the signed 16-bit results in dst.</desc>
<oper>dst[15:0]= Saturate_To_Int16(a[15:0] - a[31:16])
dst[31:16] = Saturate_To_Int16(a[47:32] - a[63:48])
dst[47:32] = Saturate_To_Int16(a[79:64] - a[95:80])
dst[63:48] = Saturate_To_Int16(a[111:96] - a[127:112])
dst[79:64] = Saturate_To_Int16(b[15:0] - b[31:16])
dst[95:80] = Saturate_To_Int16(b[47:32] - b[63:48])
dst[111:96] = Saturate_To_Int16(b[79:64] - b[95:80])
dst[127:112] = Saturate_To_Int16(b[111:96] - b[127:112])
dst[143:128]= Saturate_To_Int16(a[143:128] - a[159:144])
dst[159:144] = Saturate_To_Int16(a[175:160] - a[191:176])
dst[175:160] = Saturate_To_Int16(a[207:192] - a[223:208])
dst[191:176] = Saturate_To_Int16(a[239:224] - a[255:240])
dst[207:192] = Saturate_To_Int16(b[143:128] - b[159:144])
dst[223:208] = Saturate_To_Int16(b[175:160] - b[191:176])
dst[239:224] = Saturate_To_Int16(b[207:192] - b[223:208])
dst[255:240] = Saturate_To_Int16(b[239:224] - b[255:240])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2756</id>
<name>_MM256_HYPOT_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2760</id>
<name>_MM256_HYPOT_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2774</id>
<name>_MM256_I32GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>INT_CONST_PTR base_addr,__M256I vindex,CONST_INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2782</id>
<name>_MM256_I32GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__INT64_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	m := j*32
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2790</id>
<name>_MM256_I32GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR base_addr,__M128I vindex,CONST_INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	m := j*32
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2798</id>
<name>_MM256_I32GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m256</ret>
<sign>FLOAT_CONST_PTR base_addr,__M256I vindex,CONST_INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2821</id>
<name>_MM256_I32SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M256I a,CONST_INT scale</sign>
<instr>VPSCATTERDD</instr>
<desc>Scatter 32-bit integers from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2827</id>
<name>_MM256_I32SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M256I a,CONST_INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Scatter 64-bit integers from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2833</id>
<name>_MM256_I32SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M128I vindex,__M256D a,CONST_INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2839</id>
<name>_MM256_I32SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M256 a,CONST_INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2862</id>
<name>_MM256_I64GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>INT_CONST_PTR base_addr,__M256I vindex,CONST_INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2872</id>
<name>_MM256_I64GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__INT64_CONST_PTR base_addr,__M256I vindex,CONST_INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2880</id>
<name>_MM256_I64GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR base_addr,__M256I vindex,CONST_INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2888</id>
<name>_MM256_I64GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>FLOAT_CONST_PTR base_addr,__M256I vindex,CONST_INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2897</id>
<name>_MM256_I64SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERQD</instr>
<desc>Scatter 32-bit integers from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2905</id>
<name>_MM256_I64SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M256I a,CONST_INT scale</sign>
<instr>VPSCATTERQQ</instr>
<desc>Scatter 64-bit integers from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2911</id>
<name>_MM256_I64SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M256D a,CONST_INT scale</sign>
<instr>VSCATTERQPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2917</id>
<name>_MM256_I64SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M128 a,CONST_INT scale</sign>
<instr>VSCATTERQPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2924</id>
<name>_MM256_IDIV_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2926</id>
<name>_MM256_IDIVREM_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I_PTR mem_addr,__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, store the truncated results in dst, and store the remainders as packed 32-bit integers into memory at mem_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
	MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2928</id>
<name>_MM256_INSERT_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__INT16 i,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Copy a to dst, and insert the 16-bit integer i into dst at the location specified by index. </desc>
<oper>dst[255:0] := a[255:0]
sel := index*16
dst[sel+15:sel] := i[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2930</id>
<name>_MM256_INSERT_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__INT32 i,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Copy a to dst, and insert the 32-bit integer i into dst at the location specified by index. </desc>
<oper>dst[255:0] := a[255:0]
sel := index*32
dst[sel+31:sel] := i[31:0]</oper>
</intrinsic>
<intrinsic>
<id>2932</id>
<name>_MM256_INSERT_EPI64</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__INT64 i,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Copy a to dst, and insert the 64-bit integer i into dst at the location specified by index. </desc>
<oper>dst[255:0] := a[255:0]
sel := index*64
dst[sel+63:sel] := i[63:0]</oper>
</intrinsic>
<intrinsic>
<id>2934</id>
<name>_MM256_INSERT_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__INT8 i,CONST_INT index</sign>
<instr>NONE</instr>
<desc>Copy a to dst, and insert the 8-bit integer i into dst at the location specified by index. </desc>
<oper>dst[255:0] := a[255:0]
sel := index*8
dst[sel+7:sel] := i[7:0]</oper>
</intrinsic>
<intrinsic>
<id>2937</id>
<name>_MM256_INSERTF128_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M128D b,INT imm8</sign>
<instr>VINSERTF128</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE imm8[7:0] of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2938</id>
<name>_MM256_INSERTF128_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M128 b,INT imm8</sign>
<instr>VINSERTF128</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2939</id>
<name>_MM256_INSERTF128_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTF128</instr>
<desc>Copy a to dst, then insert 128 bits from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2940</id>
<name>_MM256_INSERTF32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M128 b,INT imm8</sign>
<instr>VINSERTF32X4</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2949</id>
<name>_MM256_INSERTF64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M128D b,INT imm8</sign>
<instr>VINSERTF64X2</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE imm8[7:0] of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2958</id>
<name>_MM256_INSERTI128_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I b,CONST_INT imm8</sign>
<instr>VINSERTI128</instr>
<desc>Copy a to dst, then insert 128 bits (composed of integer data) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2959</id>
<name>_MM256_INSERTI32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTI32X4</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 4 packed 32-bit integers) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2968</id>
<name>_MM256_INSERTI64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTI64X2</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 2 packed 64-bit integers) from b into dst at the location specified by imm8.</desc>
<oper>dst[255:0] := a[255:0]
CASE imm8[7:0] of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2980</id>
<name>_MM256_INVCBRT_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cube root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := InvCubeRoot(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2982</id>
<name>_MM256_INVCBRT_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cube root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := InvCubeRoot(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2985</id>
<name>_MM256_INVSQRT_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := InvSQRT(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2989</id>
<name>_MM256_INVSQRT_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := InvSQRT(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2993</id>
<name>_MM256_IREM_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3024</id>
<name>_MM256_LDDQU_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256I_CONST_PTR mem_addr</sign>
<instr>VLDDQU</instr>
<desc>Load 256-bits of integer data from unaligned memory into dst. This intrinsic may perform better than _mm256_loadu_si256 when the data crosses a cache line boundary.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3043</id>
<name>_MM256_LOAD_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from memory into dst.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3053</id>
<name>_MM256_LOAD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load 256-bits (composed of 8 packed single-precision (32-bit) floating-point elements) from memory into dst.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3064</id>
<name>_MM256_LOAD_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256I_CONST_PTR mem_addr</sign>
<instr>VMOVDQA</instr>
<desc>Load 256-bits of integer data from memory into dst.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3109</id>
<name>_MM256_LOADU_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3118</id>
<name>_MM256_LOADU_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>FLOAT_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load 256-bits (composed of 8 packed single-precision (32-bit) floating-point elements) from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3127</id>
<name>_MM256_LOADU_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256I_CONST_PTR mem_addr</sign>
<instr>VMOVDQU</instr>
<desc>Load 256-bits of integer data from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3133</id>
<name>_MM256_LOADU2_M128</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>FLOAT_CONST_PTR hiaddr,FLOAT_CONST_PTR loaddr</sign>
<instr>NONE</instr>
<desc>Load two 128-bit values (composed of 4 packed single-precision (32-bit) floating-point elements) from memory, and combine them into a 256-bit value in dst.
	hiaddr and loaddr do not need to be aligned on any particular boundary.</desc>
<oper>dst[127:0] := MEM[loaddr+127:loaddr]
dst[255:128] := MEM[hiaddr+127:hiaddr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3134</id>
<name>_MM256_LOADU2_M128D</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR hiaddr,DOUBLE_CONST_PTR loaddr</sign>
<instr>NONE</instr>
<desc>Load two 128-bit values (composed of 2 packed double-precision (64-bit) floating-point elements) from memory, and combine them into a 256-bit value in dst.
	hiaddr and loaddr do not need to be aligned on any particular boundary.</desc>
<oper>dst[127:0] := MEM[loaddr+127:loaddr]
dst[255:128] := MEM[hiaddr+127:hiaddr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3135</id>
<name>_MM256_LOADU2_M128I</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M128I_CONST_PTR hiaddr,__M128I_CONST_PTR loaddr</sign>
<instr>NONE</instr>
<desc>Load two 128-bit values (composed of integer data) from memory, and combine them into a 256-bit value in dst.
	hiaddr and loaddr do not need to be aligned on any particular boundary.</desc>
<oper>dst[127:0] := MEM[loaddr+127:loaddr]
dst[255:128] := MEM[hiaddr+127:hiaddr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3153</id>
<name>_MM256_LOG_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ln(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3157</id>
<name>_MM256_LOG_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ln(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3161</id>
<name>_MM256_LOG10_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := log10(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3165</id>
<name>_MM256_LOG10_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := log10(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3169</id>
<name>_MM256_LOG1P_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ln(1.0 + a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3173</id>
<name>_MM256_LOG1P_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ln(1.0 + a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3177</id>
<name>_MM256_LOG2_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the base-2 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := log2(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3181</id>
<name>_MM256_LOG2_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the base-2 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := log2(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3187</id>
<name>_MM256_LOGB_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3191</id>
<name>_MM256_LOGB_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3199</id>
<name>_MM256_LZCNT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	tmp := 31
	dst[i+31:i] := 0
	DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
		tmp := tmp - 1
		dst[i+31:i] := dst[i+31:i] + 1
	OD
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3208</id>
<name>_MM256_LZCNT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	tmp := 63
	dst[i+63:i] := 0
	DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
		tmp := tmp - 1
		dst[i+63:i] := dst[i+63:i] + 1
	OD
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3219</id>
<name>_MM256_MADD_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed signed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3229</id>
<name>_MM256_MADD52HI_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,__M256I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
	dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3238</id>
<name>_MM256_MADD52LO_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,__M256I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
	dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3247</id>
<name>_MM256_MADDUBS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Vertically multiply each unsigned 8-bit integer from a with the corresponding signed 8-bit integer from b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4</id>
<name>_MM256_MASK_ABS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ABS(a[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>13</id>
<name>_MM256_MASK_ABS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>22</id>
<name>_MM256_MASK_ABS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>31</id>
<name>_MM256_MASK_ABS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := ABS(a[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>65</id>
<name>_MM256_MASK_ADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] + b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>74</id>
<name>_MM256_MASK_ADD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>83</id>
<name>_MM256_MASK_ADD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>92</id>
<name>_MM256_MASK_ADD_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] + b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>101</id>
<name>_MM256_MASK_ADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>113</id>
<name>_MM256_MASK_ADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>153</id>
<name>_MM256_MASK_ADDS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>162</id>
<name>_MM256_MASK_ADDS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>171</id>
<name>_MM256_MASK_ADDS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>180</id>
<name>_MM256_MASK_ADDS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>211</id>
<name>_MM256_MASK_ALIGNR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 64-byte immediate result, shift the result right by count 32-bit elements, and store the low 32 bytes (8 elements) in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &amp;gt;&amp;gt; (32*count)
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := temp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>220</id>
<name>_MM256_MASK_ALIGNR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 64-byte immediate result, shift the result right by count 64-bit elements, and store the low 32 bytes (4 elements) in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &amp;gt;&amp;gt; (64*count)
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := temp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>229</id>
<name>_MM256_MASK_ALIGNR_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 1
	i := j*128
	tmp[255:0] := ((a[i+127:i] &amp;lt;&amp;lt; 128) OR b[i+127:i]) &amp;gt;&amp;gt; (count[7:0]*8)
	tmp_dst[i+127:i] := tmp[127:0]
ENDFOR

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>238</id>
<name>_MM256_MASK_AND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>245</id>
<name>_MM256_MASK_AND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] AND b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>254</id>
<name>_MM256_MASK_AND_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>263</id>
<name>_MM256_MASK_AND_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>274</id>
<name>_MM256_MASK_ANDNOT_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>281</id>
<name>_MM256_MASK_ANDNOT_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>290</id>
<name>_MM256_MASK_ANDNOT_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>299</id>
<name>_MM256_MASK_ANDNOT_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>352</id>
<name>_MM256_MASK_AVG_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>361</id>
<name>_MM256_MASK_AVG_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>387</id>
<name>_MM256_MASK_BLEND_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPBLENDMW</instr>
<desc>Blend packed 16-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := b[i+15:i]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>392</id>
<name>_MM256_MASK_BLEND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPBLENDMD</instr>
<desc>Blend packed 32-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>395</id>
<name>_MM256_MASK_BLEND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPBLENDMQ</instr>
<desc>Blend packed 64-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>398</id>
<name>_MM256_MASK_BLEND_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPBLENDMB</instr>
<desc>Blend packed 8-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := b[i+7:i]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>403</id>
<name>_MM256_MASK_BLEND_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VBLENDMPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>408</id>
<name>_MM256_MASK_BLEND_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VBLENDMPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>433</id>
<name>_MM256_MASK_BROADCAST_F32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTF32X2</instr>
<desc>Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>439</id>
<name>_MM256_MASK_BROADCAST_F32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTF32X4</instr>
<desc>Broadcast the 4 packed single-precision (32-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>448</id>
<name>_MM256_MASK_BROADCAST_F64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTF64X2</instr>
<desc>Broadcast the 2 packed double-precision (64-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := src[n+63:n]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>460</id>
<name>_MM256_MASK_BROADCAST_I32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[n+31:n]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>466</id>
<name>_MM256_MASK_BROADCAST_I32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI32X4</instr>
<desc>Broadcast the 4 packed 32-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[n+31:n]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>475</id>
<name>_MM256_MASK_BROADCAST_I64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI64X2</instr>
<desc>Broadcast the 2 packed 64-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := src[n+63:n]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>492</id>
<name>_MM256_MASK_BROADCASTB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>501</id>
<name>_MM256_MASK_BROADCASTD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>516</id>
<name>_MM256_MASK_BROADCASTQ_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>523</id>
<name>_MM256_MASK_BROADCASTSD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>533</id>
<name>_MM256_MASK_BROADCASTSS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>542</id>
<name>_MM256_MASK_BROADCASTW_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>642</id>
<name>_MM256_MASK_CMP_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>648</id>
<name>_MM256_MASK_CMP_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>654</id>
<name>_MM256_MASK_CMP_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>660</id>
<name>_MM256_MASK_CMP_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>666</id>
<name>_MM256_MASK_CMP_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>672</id>
<name>_MM256_MASK_CMP_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>678</id>
<name>_MM256_MASK_CMP_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>684</id>
<name>_MM256_MASK_CMP_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>692</id>
<name>_MM256_MASK_CMP_PD_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>700</id>
<name>_MM256_MASK_CMP_PS_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>722</id>
<name>_MM256_MASK_CMPEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>730</id>
<name>_MM256_MASK_CMPEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>738</id>
<name>_MM256_MASK_CMPEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>746</id>
<name>_MM256_MASK_CMPEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>752</id>
<name>_MM256_MASK_CMPEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>758</id>
<name>_MM256_MASK_CMPEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>764</id>
<name>_MM256_MASK_CMPEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>770</id>
<name>_MM256_MASK_CMPEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>794</id>
<name>_MM256_MASK_CMPGE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>800</id>
<name>_MM256_MASK_CMPGE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>806</id>
<name>_MM256_MASK_CMPGE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>812</id>
<name>_MM256_MASK_CMPGE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>818</id>
<name>_MM256_MASK_CMPGE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>824</id>
<name>_MM256_MASK_CMPGE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>830</id>
<name>_MM256_MASK_CMPGE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>836</id>
<name>_MM256_MASK_CMPGE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>848</id>
<name>_MM256_MASK_CMPGT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>856</id>
<name>_MM256_MASK_CMPGT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>864</id>
<name>_MM256_MASK_CMPGT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>872</id>
<name>_MM256_MASK_CMPGT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>878</id>
<name>_MM256_MASK_CMPGT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>884</id>
<name>_MM256_MASK_CMPGT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>890</id>
<name>_MM256_MASK_CMPGT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>896</id>
<name>_MM256_MASK_CMPGT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>916</id>
<name>_MM256_MASK_CMPLE_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>922</id>
<name>_MM256_MASK_CMPLE_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>928</id>
<name>_MM256_MASK_CMPLE_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>934</id>
<name>_MM256_MASK_CMPLE_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>940</id>
<name>_MM256_MASK_CMPLE_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
	ELSE 
			k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>946</id>
<name>_MM256_MASK_CMPLE_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>952</id>
<name>_MM256_MASK_CMPLE_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>958</id>
<name>_MM256_MASK_CMPLE_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>973</id>
<name>_MM256_MASK_CMPLT_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>980</id>
<name>_MM256_MASK_CMPLT_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>988</id>
<name>_MM256_MASK_CMPLT_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>995</id>
<name>_MM256_MASK_CMPLT_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1001</id>
<name>_MM256_MASK_CMPLT_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1007</id>
<name>_MM256_MASK_CMPLT_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1013</id>
<name>_MM256_MASK_CMPLT_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1019</id>
<name>_MM256_MASK_CMPLT_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1033</id>
<name>_MM256_MASK_CMPNEQ_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1039</id>
<name>_MM256_MASK_CMPNEQ_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1045</id>
<name>_MM256_MASK_CMPNEQ_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1051</id>
<name>_MM256_MASK_CMPNEQ_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1057</id>
<name>_MM256_MASK_CMPNEQ_EPU16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1063</id>
<name>_MM256_MASK_CMPNEQ_EPU32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1069</id>
<name>_MM256_MASK_CMPNEQ_EPU64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>1075</id>
<name>_MM256_MASK_CMPNEQ_EPU8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1142</id>
<name>_MM256_MASK_COMPRESS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1148</id>
<name>_MM256_MASK_COMPRESS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1154</id>
<name>_MM256_MASK_COMPRESS_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1160</id>
<name>_MM256_MASK_COMPRESS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1165</id>
<name>_MM256_MASK_COMPRESSSTOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 32
m := base_addr
FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1168</id>
<name>_MM256_MASK_COMPRESSSTOREU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 64
m := base_addr
FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1171</id>
<name>_MM256_MASK_COMPRESSSTOREU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 64
m := base_addr
FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1174</id>
<name>_MM256_MASK_COMPRESSSTOREU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 32
m := base_addr
FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1180</id>
<name>_MM256_MASK_CONFLICT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit using writemask k (elements are copied from src when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[i]
		FOR l := 0 to j-1
			m := l*32
			dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
		ENDFOR
		dst[i+31:i+j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1189</id>
<name>_MM256_MASK_CONFLICT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit using writemask k (elements are copied from src when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		FOR l := 0 to j-1
			m := l*64
			dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
		ENDFOR
		dst[i+63:i+j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1286</id>
<name>_MM256_MASK_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1321</id>
<name>_MM256_MASK_CVTEPI16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*16
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1330</id>
<name>_MM256_MASK_CVTEPI16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1339</id>
<name>_MM256_MASK_CVTEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M256I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1345</id>
<name>_MM256_MASK_CVTEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M256I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1351</id>
<name>_MM256_MASK_CVTEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1360</id>
<name>_MM256_MASK_CVTEPI32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1369</id>
<name>_MM256_MASK_CVTEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1378</id>
<name>_MM256_MASK_CVTEPI32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M128I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	IF k[j]
		dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
	ELSE
		dst[m+63:m] := src[m+63:m]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1387</id>
<name>_MM256_MASK_CVTEPI32_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1393</id>
<name>_MM256_MASK_CVTEPI32_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1396</id>
<name>_MM256_MASK_CVTEPI32_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1404</id>
<name>_MM256_MASK_CVTEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1413</id>
<name>_MM256_MASK_CVTEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1422</id>
<name>_MM256_MASK_CVTEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1431</id>
<name>_MM256_MASK_CVTEPI64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1440</id>
<name>_MM256_MASK_CVTEPI64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M256I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1446</id>
<name>_MM256_MASK_CVTEPI64_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1449</id>
<name>_MM256_MASK_CVTEPI64_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1452</id>
<name>_MM256_MASK_CVTEPI64_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1458</id>
<name>_MM256_MASK_CVTEPI8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M128I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := SignExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1467</id>
<name>_MM256_MASK_CVTEPI8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1476</id>
<name>_MM256_MASK_CVTEPI8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1485</id>
<name>_MM256_MASK_CVTEPU16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1494</id>
<name>_MM256_MASK_CVTEPU16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1503</id>
<name>_MM256_MASK_CVTEPU32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1512</id>
<name>_MM256_MASK_CVTEPU32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M128I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1526</id>
<name>_MM256_MASK_CVTEPU64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1535</id>
<name>_MM256_MASK_CVTEPU64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M256I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1544</id>
<name>_MM256_MASK_CVTEPU8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M128I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := ZeroExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1553</id>
<name>_MM256_MASK_CVTEPU8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 bytes of a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1562</id>
<name>_MM256_MASK_CVTEPU8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1585</id>
<name>_MM256_MASK_CVTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1594</id>
<name>_MM256_MASK_CVTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1603</id>
<name>_MM256_MASK_CVTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1612</id>
<name>_MM256_MASK_CVTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1622</id>
<name>_MM256_MASK_CVTPD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1633</id>
<name>_MM256_MASK_CVTPH_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M128I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1647</id>
<name>_MM256_MASK_CVTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1656</id>
<name>_MM256_MASK_CVTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1665</id>
<name>_MM256_MASK_CVTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1674</id>
<name>_MM256_MASK_CVTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1688</id>
<name>_MM256_MASK_CVTPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1715</id>
<name>_MM256_MASK_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M256I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1721</id>
<name>_MM256_MASK_CVTSEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M256I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1727</id>
<name>_MM256_MASK_CVTSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1736</id>
<name>_MM256_MASK_CVTSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1742</id>
<name>_MM256_MASK_CVTSEPI32_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1745</id>
<name>_MM256_MASK_CVTSEPI32_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1751</id>
<name>_MM256_MASK_CVTSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1760</id>
<name>_MM256_MASK_CVTSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1769</id>
<name>_MM256_MASK_CVTSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1775</id>
<name>_MM256_MASK_CVTSEPI64_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1778</id>
<name>_MM256_MASK_CVTSEPI64_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1781</id>
<name>_MM256_MASK_CVTSEPI64_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1851</id>
<name>_MM256_MASK_CVTTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1860</id>
<name>_MM256_MASK_CVTTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1869</id>
<name>_MM256_MASK_CVTTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1878</id>
<name>_MM256_MASK_CVTTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1888</id>
<name>_MM256_MASK_CVTTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1897</id>
<name>_MM256_MASK_CVTTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1906</id>
<name>_MM256_MASK_CVTTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed double-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1915</id>
<name>_MM256_MASK_CVTTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1942</id>
<name>_MM256_MASK_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M256I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1948</id>
<name>_MM256_MASK_CVTUSEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M256I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1954</id>
<name>_MM256_MASK_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1963</id>
<name>_MM256_MASK_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1969</id>
<name>_MM256_MASK_CVTUSEPI32_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1972</id>
<name>_MM256_MASK_CVTUSEPI32_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1978</id>
<name>_MM256_MASK_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1987</id>
<name>_MM256_MASK_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1996</id>
<name>_MM256_MASK_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>2002</id>
<name>_MM256_MASK_CVTUSEPI64_STOREU_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2005</id>
<name>_MM256_MASK_CVTUSEPI64_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2008</id>
<name>_MM256_MASK_CVTUSEPI64_STOREU_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>2014</id>
<name>_MM256_MASK_DBSAD_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected from within 128-bit lanes according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>FOR j := 0 to 1
	i := j*128
	tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
	tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
	tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
	tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
ENDFOR

FOR j := 0 to 3
	i := j*64
	tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2051</id>
<name>_MM256_MASK_DIV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2060</id>
<name>_MM256_MASK_DIV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2160</id>
<name>_MM256_MASK_EXPAND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2166</id>
<name>_MM256_MASK_EXPAND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2172</id>
<name>_MM256_MASK_EXPAND_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2178</id>
<name>_MM256_MASK_EXPAND_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2184</id>
<name>_MM256_MASK_EXPANDLOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2190</id>
<name>_MM256_MASK_EXPANDLOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2196</id>
<name>_MM256_MASK_EXPANDLOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2202</id>
<name>_MM256_MASK_EXPANDLOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2268</id>
<name>_MM256_MASK_EXTRACTF32X4_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M256 a,INT imm8</sign>
<instr>VEXTRACTF32X4</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2277</id>
<name>_MM256_MASK_EXTRACTF64X2_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VEXTRACTF64X2</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2287</id>
<name>_MM256_MASK_EXTRACTI32X4_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a,INT imm8</sign>
<instr>VEXTRACTI32X4</instr>
<desc>Extract 128 bits (composed of 4 packed 32-bit integers) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2296</id>
<name>_MM256_MASK_EXTRACTI64X2_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I a,INT imm8</sign>
<instr>VEXTRACTI64X2</instr>
<desc>Extract 128 bits (composed of 2 packed 64-bit integers) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2316</id>
<name>_MM256_MASK_FIXUPIMM_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2325</id>
<name>_MM256_MASK_FIXUPIMM_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2370</id>
<name>_MM256_MASK_FMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2382</id>
<name>_MM256_MASK_FMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2422</id>
<name>_MM256_MASK_FMADDSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2434</id>
<name>_MM256_MASK_FMADDSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2454</id>
<name>_MM256_MASK_FMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2466</id>
<name>_MM256_MASK_FMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2500</id>
<name>_MM256_MASK_FMSUBADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2512</id>
<name>_MM256_MASK_FMSUBADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2532</id>
<name>_MM256_MASK_FNMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2544</id>
<name>_MM256_MASK_FNMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2578</id>
<name>_MM256_MASK_FNMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256D b,__M256D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2590</id>
<name>_MM256_MASK_FNMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256 b,__M256 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2622</id>
<name>_MM256_MASK_FPCLASS_PD_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256D a,INT imm8</sign>
<instr>VFPCLASSPD</instr>
<desc>Test packed double-precision (64-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2628</id>
<name>_MM256_MASK_FPCLASS_PS_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256 a,INT imm8</sign>
<instr>VFPCLASSPS</instr>
<desc>Test packed single-precision (32-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2651</id>
<name>_MM256_MASK_GETEXP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2660</id>
<name>_MM256_MASK_GETEXP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2687</id>
<name>_MM256_MASK_GETMANT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2696</id>
<name>_MM256_MASK_GETMANT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2775</id>
<name>_MM256_MASK_I32GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,INT_CONST_PTR base_addr,__M256I vindex,__M256I mask,CONST_INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:256] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2783</id>
<name>_MM256_MASK_I32GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__INT64_CONST_PTR base_addr,__M128I vindex,__M256I mask,CONST_INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>
	Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	m := j*32
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:256] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2791</id>
<name>_MM256_MASK_I32GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,DOUBLE_CONST_PTR base_addr,__M128I vindex,__M256D mask,CONST_INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	m := j*32
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:256] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2799</id>
<name>_MM256_MASK_I32GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m256</ret>
<sign>__M256 src,FLOAT_CONST_PTR base_addr,__M256I vindex,__M256 mask,CONST_INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:256] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2822</id>
<name>_MM256_MASK_I32SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M256I a,CONST_INT scale</sign>
<instr>VPSCATTERDD</instr>
<desc>Scatter 32-bit integers from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2828</id>
<name>_MM256_MASK_I32SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M256I a,CONST_INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Scatter 64-bit integers from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2834</id>
<name>_MM256_MASK_I32SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M128I vindex,__M256D a,CONST_INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2840</id>
<name>_MM256_MASK_I32SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M256 a,CONST_INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2863</id>
<name>_MM256_MASK_I64GATHER_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,INT_CONST_PTR base_addr,__M256I vindex,__M128I mask,CONST_INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>
	Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2873</id>
<name>_MM256_MASK_I64GATHER_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__INT64_CONST_PTR base_addr,__M256I vindex,__M256I mask,CONST_INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>
	Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:256] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2881</id>
<name>_MM256_MASK_I64GATHER_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,DOUBLE_CONST_PTR base_addr,__M256I vindex,__M256D mask,CONST_INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+63] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
mask[MAX:256] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2889</id>
<name>_MM256_MASK_I64GATHER_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m128</ret>
<sign>__M128 src,FLOAT_CONST_PTR base_addr,__M256I vindex,__M128 mask,CONST_INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using mask (elements are copied from src when the highest bit is not set in the corresponding element). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	IF mask[i+31]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		mask[i+31] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
mask[MAX:128] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2898</id>
<name>_MM256_MASK_I64SCATTER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M128I a,CONST_INT scale</sign>
<instr>VPSCATTERQD</instr>
<desc>Scatter 32-bit integers from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2906</id>
<name>_MM256_MASK_I64SCATTER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M256I a,CONST_INT scale</sign>
<instr>VPSCATTERQQ</instr>
<desc>Scatter 64-bit integers from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2912</id>
<name>_MM256_MASK_I64SCATTER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M256D a,CONST_INT scale</sign>
<instr>VSCATTERQPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2918</id>
<name>_MM256_MASK_I64SCATTER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M128 a,CONST_INT scale</sign>
<instr>VSCATTERQPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>2941</id>
<name>_MM256_MASK_INSERTF32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M128 b,INT imm8</sign>
<instr>VINSERTF32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2950</id>
<name>_MM256_MASK_INSERTF64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M128D b,INT imm8</sign>
<instr>VINSERTF64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2960</id>
<name>_MM256_MASK_INSERTI32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTI32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed 32-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2969</id>
<name>_MM256_MASK_INSERTI64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTI64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed 64-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3028</id>
<name>_MM256_MASK_LOAD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3035</id>
<name>_MM256_MASK_LOAD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load packed 64-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3044</id>
<name>_MM256_MASK_LOAD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3054</id>
<name>_MM256_MASK_LOAD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3084</id>
<name>_MM256_MASK_LOADU_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU16</instr>
<desc>Load packed 16-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3090</id>
<name>_MM256_MASK_LOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3096</id>
<name>_MM256_MASK_LOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU64</instr>
<desc>Load packed 64-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3102</id>
<name>_MM256_MASK_LOADU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU8</instr>
<desc>Load packed 8-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3110</id>
<name>_MM256_MASK_LOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memoy into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3119</id>
<name>_MM256_MASK_LOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3200</id>
<name>_MM256_MASK_LZCNT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		tmp := 31
		dst[i+31:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+31:i] := dst[i+31:i] + 1
		OD
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3209</id>
<name>_MM256_MASK_LZCNT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp := 63
		dst[i+63:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+63:i] := dst[i+63:i] + 1
		OD
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3220</id>
<name>_MM256_MASK_MADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3230</id>
<name>_MM256_MASK_MADD52HI_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__MMASK8 k,__M256I b,__M256I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3239</id>
<name>_MM256_MASK_MADD52LO_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__MMASK8 k,__M256I b,__M256I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3248</id>
<name>_MM256_MASK_MADDUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Multiply packed unsigned 8-bit integers in a by packed signed 8-bit integers in b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3279</id>
<name>_MM256_MASK_MAX_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3288</id>
<name>_MM256_MASK_MAX_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3297</id>
<name>_MM256_MASK_MAX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3306</id>
<name>_MM256_MASK_MAX_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3315</id>
<name>_MM256_MASK_MAX_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3324</id>
<name>_MM256_MASK_MAX_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3333</id>
<name>_MM256_MASK_MAX_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3342</id>
<name>_MM256_MASK_MAX_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3351</id>
<name>_MM256_MASK_MAX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3361</id>
<name>_MM256_MASK_MAX_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3393</id>
<name>_MM256_MASK_MIN_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3402</id>
<name>_MM256_MASK_MIN_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3411</id>
<name>_MM256_MASK_MIN_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3420</id>
<name>_MM256_MASK_MIN_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3429</id>
<name>_MM256_MASK_MIN_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3438</id>
<name>_MM256_MASK_MIN_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3447</id>
<name>_MM256_MASK_MIN_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3456</id>
<name>_MM256_MASK_MIN_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3465</id>
<name>_MM256_MASK_MIN_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3475</id>
<name>_MM256_MASK_MIN_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3504</id>
<name>_MM256_MASK_MOV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a</sign>
<instr>VMOVDQU16</instr>
<desc>Move packed 16-bit integers from a into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3510</id>
<name>_MM256_MASK_MOV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VMOVDQA32</instr>
<desc>Move packed 32-bit integers from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3516</id>
<name>_MM256_MASK_MOV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a</sign>
<instr>VMOVDQA64</instr>
<desc>Move packed 64-bit integers from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3522</id>
<name>_MM256_MASK_MOV_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a</sign>
<instr>VMOVDQU8</instr>
<desc>Move packed 8-bit integers from a into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3528</id>
<name>_MM256_MASK_MOV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VMOVAPD</instr>
<desc>Move packed double-precision (64-bit) floating-point elements from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3534</id>
<name>_MM256_MASK_MOV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VMOVAPS</instr>
<desc>Move packed single-precision (32-bit) floating-point elements from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3548</id>
<name>_MM256_MASK_MOVEDUP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3557</id>
<name>_MM256_MASK_MOVEHDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3567</id>
<name>_MM256_MASK_MOVELDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3612</id>
<name>_MM256_MASK_MUL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3621</id>
<name>_MM256_MASK_MUL_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3630</id>
<name>_MM256_MASK_MUL_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3639</id>
<name>_MM256_MASK_MUL_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  RM.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3667</id>
<name>_MM256_MASK_MULHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3678</id>
<name>_MM256_MASK_MULHI_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3691</id>
<name>_MM256_MASK_MULHRS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
		dst[i+15:i] := tmp[16:1]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3701</id>
<name>_MM256_MASK_MULLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3710</id>
<name>_MM256_MASK_MULLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		tmp[63:0] := a[i+31:i] * b[i+31:i]
		dst[i+31:i] := tmp[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3719</id>
<name>_MM256_MASK_MULLO_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp[127:0] := a[i+63:i] * b[i+63:i]
		dst[i+63:i] := tmp[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3731</id>
<name>_MM256_MASK_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR i := 0 to 3
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		IF k[i*8+j]
			dst[q+j*8+7:q+j*8] := tmp8[7:0]
		ELSE
			dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
		FI
	ENDFOR
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3744</id>
<name>_MM256_MASK_OR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] OR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3751</id>
<name>_MM256_MASK_OR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] OR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3759</id>
<name>_MM256_MASK_OR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3768</id>
<name>_MM256_MASK_OR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3781</id>
<name>_MM256_MASK_PACKS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3790</id>
<name>_MM256_MASK_PACKS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3820</id>
<name>_MM256_MASK_PACKUS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3829</id>
<name>_MM256_MASK_PACKUS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3859</id>
<name>_MM256_MASK_PERMUTE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3868</id>
<name>_MM256_MASK_PERMUTE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3889</id>
<name>_MM256_MASK_PERMUTEVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3898</id>
<name>_MM256_MASK_PERMUTEVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3906</id>
<name>_MM256_MASK_PERMUTEX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3912</id>
<name>_MM256_MASK_PERMUTEX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3922</id>
<name>_MM256_MASK_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__MMASK16 k,__M256I idx,__M256I b</sign>
<instr>VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		off := 16*idx[i+3:i]
		dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3934</id>
<name>_MM256_MASK_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__MMASK8 k,__M256I idx,__M256I b</sign>
<instr>VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3946</id>
<name>_MM256_MASK_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__MMASK8 k,__M256I idx,__M256I b</sign>
<instr>VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3958</id>
<name>_MM256_MASK_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__MMASK32 k,__M256I idx,__M256I b</sign>
<instr>VPERMT2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		off := 8*idx[i+4:i]
		dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3970</id>
<name>_MM256_MASK_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__MMASK8 k,__M256I idx,__M256D b</sign>
<instr>VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3982</id>
<name>_MM256_MASK_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__MMASK8 k,__M256I idx,__M256 b</sign>
<instr>VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3993</id>
<name>_MM256_MASK_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I idx,__M256I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	id := idx[i+3:i]*16
	IF k[j]
		dst[i+15:i] := a[id+15:id]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3999</id>
<name>_MM256_MASK_PERMUTEXVAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I idx,__M256I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4005</id>
<name>_MM256_MASK_PERMUTEXVAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I idx,__M256I a</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	id := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4014</id>
<name>_MM256_MASK_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I idx,__M256I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	id := idx[i+4:i]*8
	IF k[j]
		dst[i+7:i] := a[id+7:id]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4020</id>
<name>_MM256_MASK_PERMUTEXVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256I idx,__M256D a</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	id := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4026</id>
<name>_MM256_MASK_PERMUTEXVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256I idx,__M256 a</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4116</id>
<name>_MM256_MASK_RANGE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4125</id>
<name>_MM256_MASK_RANGE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4153</id>
<name>_MM256_MASK_RCP14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4162</id>
<name>_MM256_MASK_RCP14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4276</id>
<name>_MM256_MASK_REDUCE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4285</id>
<name>_MM256_MASK_REDUCE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4342</id>
<name>_MM256_MASK_ROL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4351</id>
<name>_MM256_MASK_ROL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4360</id>
<name>_MM256_MASK_ROLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4369</id>
<name>_MM256_MASK_ROLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4378</id>
<name>_MM256_MASK_ROR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4387</id>
<name>_MM256_MASK_ROR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4396</id>
<name>_MM256_MASK_RORV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4405</id>
<name>_MM256_MASK_RORV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4430</id>
<name>_MM256_MASK_ROUNDSCALE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4439</id>
<name>_MM256_MASK_ROUNDSCALE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4468</id>
<name>_MM256_MASK_RSQRT14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4475</id>
<name>_MM256_MASK_RSQRT14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4527</id>
<name>_MM256_MASK_SCALEF_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4536</id>
<name>_MM256_MASK_SCALEF_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4594</id>
<name>_MM256_MASK_SET1_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,SHORT a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4603</id>
<name>_MM256_MASK_SET1_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4612</id>
<name>_MM256_MASK_SET1_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4622</id>
<name>_MM256_MASK_SET1_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,CHAR a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast 8-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4691</id>
<name>_MM256_MASK_SHUFFLE_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4700</id>
<name>_MM256_MASK_SHUFFLE_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF b[i+7] == 1
			dst[i+7:i] := 0
		ELSE
			index[3:0] := b[i+3:i]
			dst[i+7:i] := a[index*8+7:index*8]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4706</id>
<name>_MM256_MASK_SHUFFLE_F32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VSHUFF32X4</instr>
<desc>Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4712</id>
<name>_MM256_MASK_SHUFFLE_F64X2</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VSHUFF64X2</instr>
<desc>Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT4(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4718</id>
<name>_MM256_MASK_SHUFFLE_I32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VSHUFI32X4</instr>
<desc>Shuffle 128-bits (composed of 4 32-bit integers) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4724</id>
<name>_MM256_MASK_SHUFFLE_I64X2</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VSHUFI64X2</instr>
<desc>Shuffle 128-bits (composed of 2 64-bit integers) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4733</id>
<name>_MM256_MASK_SHUFFLE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4744</id>
<name>_MM256_MASK_SHUFFLE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4753</id>
<name>_MM256_MASK_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the high 64 bits of 128-bit lanes of dst, with the low 64 bits of 128-bit lanes being copied from from a to dst, using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[207:192]

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4762</id>
<name>_MM256_MASK_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the low 64 bits of 128-bit lanes of dst, with the high 64 bits of 128-bit lanes being copied from from a to dst, using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4812</id>
<name>_MM256_MASK_SLL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4821</id>
<name>_MM256_MASK_SLL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4830</id>
<name>_MM256_MASK_SLL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4842</id>
<name>_MM256_MASK_SLLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4851</id>
<name>_MM256_MASK_SLLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4860</id>
<name>_MM256_MASK_SLLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4874</id>
<name>_MM256_MASK_SLLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4883</id>
<name>_MM256_MASK_SLLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4892</id>
<name>_MM256_MASK_SLLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4903</id>
<name>_MM256_MASK_SQRT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4912</id>
<name>_MM256_MASK_SQRT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4939</id>
<name>_MM256_MASK_SRA_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4948</id>
<name>_MM256_MASK_SRA_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4957</id>
<name>_MM256_MASK_SRA_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4968</id>
<name>_MM256_MASK_SRAI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4977</id>
<name>_MM256_MASK_SRAI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4986</id>
<name>_MM256_MASK_SRAI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4997</id>
<name>_MM256_MASK_SRAV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5006</id>
<name>_MM256_MASK_SRAV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5015</id>
<name>_MM256_MASK_SRAV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5024</id>
<name>_MM256_MASK_SRL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5033</id>
<name>_MM256_MASK_SRL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5042</id>
<name>_MM256_MASK_SRL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5054</id>
<name>_MM256_MASK_SRLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5063</id>
<name>_MM256_MASK_SRLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5072</id>
<name>_MM256_MASK_SRLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5086</id>
<name>_MM256_MASK_SRLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5095</id>
<name>_MM256_MASK_SRLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5104</id>
<name>_MM256_MASK_SRLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5111</id>
<name>_MM256_MASK_STORE_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256I a</sign>
<instr>VMOVDQA32</instr>
<desc>Store packed 32-bit integers from a into memory using writemask k.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5115</id>
<name>_MM256_MASK_STORE_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256I a</sign>
<instr>VMOVDQA64</instr>
<desc>Store packed 64-bit integers from a into memory using writemask k.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5120</id>
<name>_MM256_MASK_STORE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256D a</sign>
<instr>VMOVAPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using writemask k.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5127</id>
<name>_MM256_MASK_STORE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256 a</sign>
<instr>VMOVAPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using writemask k.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5156</id>
<name>_MM256_MASK_STOREU_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK16 k,__M256I a</sign>
<instr>VMOVDQU16</instr>
<desc>Store packed 16-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5159</id>
<name>_MM256_MASK_STOREU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256I a</sign>
<instr>VMOVDQU32</instr>
<desc>Store packed 32-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5162</id>
<name>_MM256_MASK_STOREU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256I a</sign>
<instr>VMOVDQU64</instr>
<desc>Store packed 64-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5165</id>
<name>_MM256_MASK_STOREU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK32 k,__M256I a</sign>
<instr>VMOVDQU8</instr>
<desc>Store packed 8-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5169</id>
<name>_MM256_MASK_STOREU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256D a</sign>
<instr>VMOVUPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5175</id>
<name>_MM256_MASK_STOREU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M256 a</sign>
<instr>VMOVUPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5209</id>
<name>_MM256_MASK_SUB_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] - b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5218</id>
<name>_MM256_MASK_SUB_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5227</id>
<name>_MM256_MASK_SUB_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5236</id>
<name>_MM256_MASK_SUB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] - b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5245</id>
<name>_MM256_MASK_SUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5257</id>
<name>_MM256_MASK_SUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5299</id>
<name>_MM256_MASK_SUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5308</id>
<name>_MM256_MASK_SUBS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5317</id>
<name>_MM256_MASK_SUBS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5326</id>
<name>_MM256_MASK_SUBS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5391</id>
<name>_MM256_MASK_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from src, a, and b are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using writemask k at 32-bit granularity (32-bit elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		FOR h := 0 to 31
			index[2:0] := (src[i+h] &amp;lt;&amp;lt; 2) OR (a[i+h] &amp;lt;&amp;lt; 1) OR b[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5400</id>
<name>_MM256_MASK_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from src, a, and b are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using writemask k at 64-bit granularity (64-bit elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		FOR h := 0 to 63
			index[2:0] := (src[i+h] &amp;lt;&amp;lt; 2) OR (a[i+h] &amp;lt;&amp;lt; 1) OR b[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5410</id>
<name>_MM256_MASK_TEST_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPTESTMW</instr>
<desc>Compute the bitwise AND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5416</id>
<name>_MM256_MASK_TEST_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPTESTMD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5422</id>
<name>_MM256_MASK_TEST_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPTESTMQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5428</id>
<name>_MM256_MASK_TEST_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPTESTMB</instr>
<desc>Compute the bitwise AND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5441</id>
<name>_MM256_MASK_TESTN_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M256I a,__M256I b</sign>
<instr>VPTESTNMW</instr>
<desc>Compute the bitwise NAND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k1[j]
		k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5447</id>
<name>_MM256_MASK_TESTN_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPTESTNMD</instr>
<desc>Compute the bitwise NAND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k1[j]
		k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5453</id>
<name>_MM256_MASK_TESTN_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M256I a,__M256I b</sign>
<instr>VPTESTNMQ</instr>
<desc>Compute the bitwise NAND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k1[j]
		k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5459</id>
<name>_MM256_MASK_TESTN_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M256I a,__M256I b</sign>
<instr>VPTESTNMB</instr>
<desc>Compute the bitwise NAND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k1[j]
		k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5521</id>
<name>_MM256_MASK_UNPACKHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5530</id>
<name>_MM256_MASK_UNPACKHI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5539</id>
<name>_MM256_MASK_UNPACKHI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5548</id>
<name>_MM256_MASK_UNPACKHI_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5557</id>
<name>_MM256_MASK_UNPACKHI_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5569</id>
<name>_MM256_MASK_UNPACKHI_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5578</id>
<name>_MM256_MASK_UNPACKLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5587</id>
<name>_MM256_MASK_UNPACKLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5596</id>
<name>_MM256_MASK_UNPACKLO_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5605</id>
<name>_MM256_MASK_UNPACKLO_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5614</id>
<name>_MM256_MASK_UNPACKLO_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5626</id>
<name>_MM256_MASK_UNPACKLO_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5644</id>
<name>_MM256_MASK_XOR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5651</id>
<name>_MM256_MASK_XOR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5659</id>
<name>_MM256_MASK_XOR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5668</id>
<name>_MM256_MASK_XOR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3923</id>
<name>_MM256_MASK2_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__MMASK16 k,__M256I b</sign>
<instr>VPERMI2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		off := 16*idx[i+3:i]
		dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := idx[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3935</id>
<name>_MM256_MASK2_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__MMASK8 k,__M256I b</sign>
<instr>VPERMI2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := idx[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3947</id>
<name>_MM256_MASK2_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__MMASK8 k,__M256I b</sign>
<instr>VPERMI2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := idx[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3959</id>
<name>_MM256_MASK2_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__MMASK32 k,__M256I b</sign>
<instr>VPERMI2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		off := 8*idx[i+4:i]
		dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3971</id>
<name>_MM256_MASK2_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256I idx,__MMASK8 k,__M256D b</sign>
<instr>VPERMI2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := idx[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3983</id>
<name>_MM256_MASK2_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256I idx,__MMASK8 k,__M256 b</sign>
<instr>VPERMI2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := idx[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2371</id>
<name>_MM256_MASK3_FMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c,__MMASK8 k</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2383</id>
<name>_MM256_MASK3_FMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c,__MMASK8 k</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2423</id>
<name>_MM256_MASK3_FMADDSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c,__MMASK8 k</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2435</id>
<name>_MM256_MASK3_FMADDSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c,__MMASK8 k</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2455</id>
<name>_MM256_MASK3_FMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c,__MMASK8 k</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2467</id>
<name>_MM256_MASK3_FMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c,__MMASK8 k</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2501</id>
<name>_MM256_MASK3_FMSUBADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c,__MMASK8 k</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2513</id>
<name>_MM256_MASK3_FMSUBADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c,__MMASK8 k</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2533</id>
<name>_MM256_MASK3_FNMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c,__MMASK8 k</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2545</id>
<name>_MM256_MASK3_FNMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c,__MMASK8 k</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2579</id>
<name>_MM256_MASK3_FNMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,__M256D c,__MMASK8 k</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2591</id>
<name>_MM256_MASK3_FNMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,__M256 c,__MMASK8 k</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3258</id>
<name>_MM256_MASKLOAD_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>INT_CONST_PTR mem_addr,__M256I mask</sign>
<instr>VPMASKMOVD</instr>
<desc>Load packed 32-bit integers from memory into dst using mask (elements are zeroed out when the highest bit is not set in the corresponding element).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3260</id>
<name>_MM256_MASKLOAD_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__INT64_CONST_PTR mem_addr,__M256I mask</sign>
<instr>VPMASKMOVQ</instr>
<desc>Load packed 64-bit integers from memory into dst using mask (elements are zeroed out when the highest bit is not set in the corresponding element).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3262</id>
<name>_MM256_MASKLOAD_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE_CONST_PTR mem_addr,__M256I mask</sign>
<instr>VMASKMOVPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using mask (elements are zeroed out when the high bit of the corresponding element is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3264</id>
<name>_MM256_MASKLOAD_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>FLOAT_CONST_PTR mem_addr,__M256I mask</sign>
<instr>VMASKMOVPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using mask (elements are zeroed out when the high bit of the corresponding element is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3269</id>
<name>_MM256_MASKSTORE_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>void</ret>
<sign>INT_PTR mem_addr,__M256I mask,__M256I a</sign>
<instr>VPMASKMOVD</instr>
<desc>Store packed 32-bit integers from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element).
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3271</id>
<name>_MM256_MASKSTORE_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>void</ret>
<sign>__INT64_PTR mem_addr,__M256I mask,__M256I a</sign>
<instr>VPMASKMOVQ</instr>
<desc>Store packed 64-bit integers from a into memory using mask (elements are not stored when the highest bit is not set in the corresponding element).
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3273</id>
<name>_MM256_MASKSTORE_PD</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M256I mask,__M256D a</sign>
<instr>VMASKMOVPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using mask.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF mask[i+63]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3275</id>
<name>_MM256_MASKSTORE_PS</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M256I mask,__M256 a</sign>
<instr>VMASKMOVPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using mask.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF mask[i+31]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5</id>
<name>_MM256_MASKZ_ABS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ABS(a[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>14</id>
<name>_MM256_MASKZ_ABS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>23</id>
<name>_MM256_MASKZ_ABS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>32</id>
<name>_MM256_MASKZ_ABS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := ABS(a[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>66</id>
<name>_MM256_MASKZ_ADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] + b[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>75</id>
<name>_MM256_MASKZ_ADD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>84</id>
<name>_MM256_MASKZ_ADD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] :=0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>93</id>
<name>_MM256_MASKZ_ADD_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] + b[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>102</id>
<name>_MM256_MASKZ_ADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>114</id>
<name>_MM256_MASKZ_ADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>154</id>
<name>_MM256_MASKZ_ADDS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>163</id>
<name>_MM256_MASKZ_ADDS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>172</id>
<name>_MM256_MASKZ_ADDS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>181</id>
<name>_MM256_MASKZ_ADDS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>212</id>
<name>_MM256_MASKZ_ALIGNR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 64-byte immediate result, shift the result right by count 32-bit elements, and store the low 32 bytes (8 elements) in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &amp;gt;&amp;gt; (32*count)
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := temp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>221</id>
<name>_MM256_MASKZ_ALIGNR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 64-byte immediate result, shift the result right by count 64-bit elements, and store the low 32 bytes (4 elements) in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &amp;gt;&amp;gt; (64*count)
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := temp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>230</id>
<name>_MM256_MASKZ_ALIGNR_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 1
	i := j*128
	tmp[255:0] := ((a[i+127:i] &amp;lt;&amp;lt; 128) OR b[i+127:i]) &amp;gt;&amp;gt; (count[7:0]*8)
	tmp_dst[i+127:i] := tmp[127:0]
ENDFOR

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>239</id>
<name>_MM256_MASKZ_AND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>246</id>
<name>_MM256_MASKZ_AND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] AND b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>255</id>
<name>_MM256_MASKZ_AND_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
	ELSE
		dst[i+63:i] := 0 
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>264</id>
<name>_MM256_MASKZ_AND_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>275</id>
<name>_MM256_MASKZ_ANDNOT_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>282</id>
<name>_MM256_MASKZ_ANDNOT_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>291</id>
<name>_MM256_MASKZ_ANDNOT_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>300</id>
<name>_MM256_MASKZ_ANDNOT_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>353</id>
<name>_MM256_MASKZ_AVG_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>362</id>
<name>_MM256_MASKZ_AVG_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>434</id>
<name>_MM256_MASKZ_BROADCAST_F32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTF32X2</instr>
<desc>Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>440</id>
<name>_MM256_MASKZ_BROADCAST_F32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTF32X4</instr>
<desc>Broadcast the 4 packed single-precision (32-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>449</id>
<name>_MM256_MASKZ_BROADCAST_F64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTF64X2</instr>
<desc>Broadcast the 2 packed double-precision (64-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>461</id>
<name>_MM256_MASKZ_BROADCAST_I32X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>467</id>
<name>_MM256_MASKZ_BROADCAST_I32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI32X4</instr>
<desc>Broadcast the 4 packed 32-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>476</id>
<name>_MM256_MASKZ_BROADCAST_I64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI64X2</instr>
<desc>Broadcast the 2 packed 64-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>493</id>
<name>_MM256_MASKZ_BROADCASTB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>502</id>
<name>_MM256_MASKZ_BROADCASTD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>517</id>
<name>_MM256_MASKZ_BROADCASTQ_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>524</id>
<name>_MM256_MASKZ_BROADCASTSD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>534</id>
<name>_MM256_MASKZ_BROADCASTSS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>543</id>
<name>_MM256_MASKZ_BROADCASTW_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1143</id>
<name>_MM256_MASKZ_COMPRESS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1149</id>
<name>_MM256_MASKZ_COMPRESS_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1155</id>
<name>_MM256_MASKZ_COMPRESS_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1161</id>
<name>_MM256_MASKZ_COMPRESS_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1181</id>
<name>_MM256_MASKZ_CONFLICT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[i]
		FOR l := 0 to j-1
			m := l*32
			dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
		ENDFOR
		dst[i+31:i+j] := 0
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1190</id>
<name>_MM256_MASKZ_CONFLICT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		FOR l := 0 to j-1
			m := l*64
			dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
		ENDFOR
		dst[i+63:i+j] := 0
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1287</id>
<name>_MM256_MASKZ_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1322</id>
<name>_MM256_MASKZ_CVTEPI16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1331</id>
<name>_MM256_MASKZ_CVTEPI16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1340</id>
<name>_MM256_MASKZ_CVTEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1352</id>
<name>_MM256_MASKZ_CVTEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1361</id>
<name>_MM256_MASKZ_CVTEPI32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1370</id>
<name>_MM256_MASKZ_CVTEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1379</id>
<name>_MM256_MASKZ_CVTEPI32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	IF k[j]
		dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
	ELSE
		dst[m+63:m] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1388</id>
<name>_MM256_MASKZ_CVTEPI32_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1405</id>
<name>_MM256_MASKZ_CVTEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1414</id>
<name>_MM256_MASKZ_CVTEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1423</id>
<name>_MM256_MASKZ_CVTEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1432</id>
<name>_MM256_MASKZ_CVTEPI64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1441</id>
<name>_MM256_MASKZ_CVTEPI64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1459</id>
<name>_MM256_MASKZ_CVTEPI8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := SignExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1468</id>
<name>_MM256_MASKZ_CVTEPI8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1477</id>
<name>_MM256_MASKZ_CVTEPI8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1486</id>
<name>_MM256_MASKZ_CVTEPU16_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1495</id>
<name>_MM256_MASKZ_CVTEPU16_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1504</id>
<name>_MM256_MASKZ_CVTEPU32_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+31:l])
	ELSE 
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1513</id>
<name>_MM256_MASKZ_CVTEPU32_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1527</id>
<name>_MM256_MASKZ_CVTEPU64_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1536</id>
<name>_MM256_MASKZ_CVTEPU64_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1545</id>
<name>_MM256_MASKZ_CVTEPU8_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := ZeroExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1554</id>
<name>_MM256_MASKZ_CVTEPU8_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 bytes of a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1563</id>
<name>_MM256_MASKZ_CVTEPU8_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 4 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1586</id>
<name>_MM256_MASKZ_CVTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1595</id>
<name>_MM256_MASKZ_CVTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1604</id>
<name>_MM256_MASKZ_CVTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1613</id>
<name>_MM256_MASKZ_CVTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1623</id>
<name>_MM256_MASKZ_CVTPD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1634</id>
<name>_MM256_MASKZ_CVTPH_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1648</id>
<name>_MM256_MASKZ_CVTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1657</id>
<name>_MM256_MASKZ_CVTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1666</id>
<name>_MM256_MASKZ_CVTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1675</id>
<name>_MM256_MASKZ_CVTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1689</id>
<name>_MM256_MASKZ_CVTPS_PH</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1716</id>
<name>_MM256_MASKZ_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1728</id>
<name>_MM256_MASKZ_CVTSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1737</id>
<name>_MM256_MASKZ_CVTSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1752</id>
<name>_MM256_MASKZ_CVTSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1761</id>
<name>_MM256_MASKZ_CVTSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1770</id>
<name>_MM256_MASKZ_CVTSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1852</id>
<name>_MM256_MASKZ_CVTTPD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1861</id>
<name>_MM256_MASKZ_CVTTPD_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1870</id>
<name>_MM256_MASKZ_CVTTPD_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1879</id>
<name>_MM256_MASKZ_CVTTPD_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1889</id>
<name>_MM256_MASKZ_CVTTPS_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1898</id>
<name>_MM256_MASKZ_CVTTPS_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1907</id>
<name>_MM256_MASKZ_CVTTPS_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed double-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1916</id>
<name>_MM256_MASKZ_CVTTPS_EPU64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M128 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1943</id>
<name>_MM256_MASKZ_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1955</id>
<name>_MM256_MASKZ_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1964</id>
<name>_MM256_MASKZ_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1979</id>
<name>_MM256_MASKZ_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1988</id>
<name>_MM256_MASKZ_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1997</id>
<name>_MM256_MASKZ_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>2015</id>
<name>_MM256_MASKZ_DBSAD_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected from within 128-bit lanes according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>FOR j := 0 to 1
	i := j*128
	tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
	tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
	tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
	tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
ENDFOR

FOR j := 0 to 3
	i := j*64
	tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2052</id>
<name>_MM256_MASKZ_DIV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2061</id>
<name>_MM256_MASKZ_DIV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2161</id>
<name>_MM256_MASKZ_EXPAND_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2167</id>
<name>_MM256_MASKZ_EXPAND_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2173</id>
<name>_MM256_MASKZ_EXPAND_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2179</id>
<name>_MM256_MASKZ_EXPAND_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2185</id>
<name>_MM256_MASKZ_EXPANDLOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2191</id>
<name>_MM256_MASKZ_EXPANDLOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2197</id>
<name>_MM256_MASKZ_EXPANDLOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2203</id>
<name>_MM256_MASKZ_EXPANDLOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2269</id>
<name>_MM256_MASKZ_EXTRACTF32X4_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M256 a,INT imm8</sign>
<instr>VEXTRACTF32X4</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2278</id>
<name>_MM256_MASKZ_EXTRACTF64X2_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VEXTRACTF64X2</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2288</id>
<name>_MM256_MASKZ_EXTRACTI32X4_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a,INT imm8</sign>
<instr>VEXTRACTI32X4</instr>
<desc>Extract 128 bits (composed of 4 packed 32-bit integers) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2297</id>
<name>_MM256_MASKZ_EXTRACTI64X2_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M256I a,INT imm8</sign>
<instr>VEXTRACTI64X2</instr>
<desc>Extract 128 bits (composed of 2 packed 64-bit integers) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2317</id>
<name>_MM256_MASKZ_FIXUPIMM_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2326</id>
<name>_MM256_MASKZ_FIXUPIMM_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2372</id>
<name>_MM256_MASKZ_FMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2384</id>
<name>_MM256_MASKZ_FMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2424</id>
<name>_MM256_MASKZ_FMADDSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2436</id>
<name>_MM256_MASKZ_FMADDSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2456</id>
<name>_MM256_MASKZ_FMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2468</id>
<name>_MM256_MASKZ_FMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2502</id>
<name>_MM256_MASKZ_FMSUBADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2514</id>
<name>_MM256_MASKZ_FMSUBADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2534</id>
<name>_MM256_MASKZ_FNMADD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2546</id>
<name>_MM256_MASKZ_FNMADD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2580</id>
<name>_MM256_MASKZ_FNMSUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,__M256D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2592</id>
<name>_MM256_MASKZ_FNMSUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,__M256 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2652</id>
<name>_MM256_MASKZ_GETEXP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2661</id>
<name>_MM256_MASKZ_GETEXP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2688</id>
<name>_MM256_MASKZ_GETMANT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2697</id>
<name>_MM256_MASKZ_GETMANT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2942</id>
<name>_MM256_MASKZ_INSERTF32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M128 b,INT imm8</sign>
<instr>VINSERTF32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2951</id>
<name>_MM256_MASKZ_INSERTF64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M128D b,INT imm8</sign>
<instr>VINSERTF64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2961</id>
<name>_MM256_MASKZ_INSERTI32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTI32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed 32-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2970</id>
<name>_MM256_MASKZ_INSERTI64X2</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I b,INT imm8</sign>
<instr>VINSERTI64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed 64-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[255:0] := a[255:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3029</id>
<name>_MM256_MASKZ_LOAD_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3036</id>
<name>_MM256_MASKZ_LOAD_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load packed 64-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3045</id>
<name>_MM256_MASKZ_LOAD_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3055</id>
<name>_MM256_MASKZ_LOAD_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3085</id>
<name>_MM256_MASKZ_LOADU_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU16</instr>
<desc>Load packed 16-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3091</id>
<name>_MM256_MASKZ_LOADU_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3097</id>
<name>_MM256_MASKZ_LOADU_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU64</instr>
<desc>Load packed 64-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3103</id>
<name>_MM256_MASKZ_LOADU_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU8</instr>
<desc>Load packed 8-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3111</id>
<name>_MM256_MASKZ_LOADU_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memoy into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3120</id>
<name>_MM256_MASKZ_LOADU_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3201</id>
<name>_MM256_MASKZ_LZCNT_EPI32</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		tmp := 31
		dst[i+31:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+31:i] := dst[i+31:i] + 1
		OD
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3210</id>
<name>_MM256_MASKZ_LZCNT_EPI64</name>
<cpuid>AVX512CD, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp := 63
		dst[i+63:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+63:i] := dst[i+63:i] + 1
		OD
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3221</id>
<name>_MM256_MASKZ_MADD_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3231</id>
<name>_MM256_MASKZ_MADD52HI_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,__M256I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3240</id>
<name>_MM256_MASKZ_MADD52LO_EPU64</name>
<cpuid>AVX512VL, AVX512IFMA52</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,__M256I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3249</id>
<name>_MM256_MASKZ_MADDUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Multiply packed unsigned 8-bit integers in a by packed signed 8-bit integers in b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3280</id>
<name>_MM256_MASKZ_MAX_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3289</id>
<name>_MM256_MASKZ_MAX_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3298</id>
<name>_MM256_MASKZ_MAX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3307</id>
<name>_MM256_MASKZ_MAX_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3316</id>
<name>_MM256_MASKZ_MAX_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3325</id>
<name>_MM256_MASKZ_MAX_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3334</id>
<name>_MM256_MASKZ_MAX_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3343</id>
<name>_MM256_MASKZ_MAX_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3352</id>
<name>_MM256_MASKZ_MAX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3362</id>
<name>_MM256_MASKZ_MAX_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3394</id>
<name>_MM256_MASKZ_MIN_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3403</id>
<name>_MM256_MASKZ_MIN_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3412</id>
<name>_MM256_MASKZ_MIN_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3421</id>
<name>_MM256_MASKZ_MIN_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3430</id>
<name>_MM256_MASKZ_MIN_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3439</id>
<name>_MM256_MASKZ_MIN_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3448</id>
<name>_MM256_MASKZ_MIN_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3457</id>
<name>_MM256_MASKZ_MIN_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3466</id>
<name>_MM256_MASKZ_MIN_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3476</id>
<name>_MM256_MASKZ_MIN_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3505</id>
<name>_MM256_MASKZ_MOV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VMOVDQU16</instr>
<desc>Move packed 16-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3511</id>
<name>_MM256_MASKZ_MOV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VMOVDQA32</instr>
<desc>Move packed 32-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3517</id>
<name>_MM256_MASKZ_MOV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VMOVDQA64</instr>
<desc>Move packed 64-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3523</id>
<name>_MM256_MASKZ_MOV_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a</sign>
<instr>VMOVDQU8</instr>
<desc>Move packed 8-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3529</id>
<name>_MM256_MASKZ_MOV_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VMOVAPD</instr>
<desc>Move packed double-precision (64-bit) floating-point elements from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3535</id>
<name>_MM256_MASKZ_MOV_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VMOVAPS</instr>
<desc>Move packed single-precision (32-bit) floating-point elements from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3549</id>
<name>_MM256_MASKZ_MOVEDUP_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3558</id>
<name>_MM256_MASKZ_MOVEHDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3568</id>
<name>_MM256_MASKZ_MOVELDUP_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3613</id>
<name>_MM256_MASKZ_MUL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3622</id>
<name>_MM256_MASKZ_MUL_EPU32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3631</id>
<name>_MM256_MASKZ_MUL_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3640</id>
<name>_MM256_MASKZ_MUL_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
   </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3668</id>
<name>_MM256_MASKZ_MULHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3679</id>
<name>_MM256_MASKZ_MULHI_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := o
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3692</id>
<name>_MM256_MASKZ_MULHRS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
		dst[i+15:i] := tmp[16:1]
	ELSE
		dst[i+15:i] := 9
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3702</id>
<name>_MM256_MASKZ_MULLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3711</id>
<name>_MM256_MASKZ_MULLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		tmp[63:0] := a[i+31:i] * b[i+31:i]
		dst[i+31:i] := tmp[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3720</id>
<name>_MM256_MASKZ_MULLO_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		tmp[127:0] := a[i+63:i] * b[i+63:i]
		dst[i+63:i] := tmp[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3732</id>
<name>_MM256_MASKZ_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR i := 0 to 3
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		IF k[i*8+j]
			dst[q+j*8+7:q+j*8] := tmp8[7:0]
		ELSE
			dst[q+j*8+7:q+j*8] := 0
		FI
	ENDFOR
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3745</id>
<name>_MM256_MASKZ_OR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] OR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3752</id>
<name>_MM256_MASKZ_OR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] OR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3760</id>
<name>_MM256_MASKZ_OR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3769</id>
<name>_MM256_MASKZ_OR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3782</id>
<name>_MM256_MASKZ_PACKS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3791</id>
<name>_MM256_MASKZ_PACKS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3821</id>
<name>_MM256_MASKZ_PACKUS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3830</id>
<name>_MM256_MASKZ_PACKUS_EPI32</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3860</id>
<name>_MM256_MASKZ_PERMUTE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3869</id>
<name>_MM256_MASKZ_PERMUTE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3890</id>
<name>_MM256_MASKZ_PERMUTEVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3899</id>
<name>_MM256_MASKZ_PERMUTEVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3907</id>
<name>_MM256_MASKZ_PERMUTEX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3913</id>
<name>_MM256_MASKZ_PERMUTEX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3924</id>
<name>_MM256_MASKZ_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2W, VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		off := 16*idx[i+3:i]
		dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3936</id>
<name>_MM256_MASKZ_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2D, VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3948</id>
<name>_MM256_MASKZ_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2Q, VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3960</id>
<name>_MM256_MASKZ_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2B, VPERMT2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		off := 8*idx[i+4:i]
		dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3972</id>
<name>_MM256_MASKZ_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256I idx,__M256D b</sign>
<instr>VPERMI2PD, VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3984</id>
<name>_MM256_MASKZ_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256I idx,__M256 b</sign>
<instr>VPERMI2PS, VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3994</id>
<name>_MM256_MASKZ_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I idx,__M256I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	id := idx[i+3:i]*16
	IF k[j]
		dst[i+15:i] := a[id+15:id]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4000</id>
<name>_MM256_MASKZ_PERMUTEXVAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I idx,__M256I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4006</id>
<name>_MM256_MASKZ_PERMUTEXVAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I idx,__M256I a</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	id := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4015</id>
<name>_MM256_MASKZ_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I idx,__M256I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	id := idx[i+4:i]*8
	IF k[j]
		dst[i+7:i] := a[id+7:id]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4021</id>
<name>_MM256_MASKZ_PERMUTEXVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256I idx,__M256D a</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	id := idx[i+1:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4027</id>
<name>_MM256_MASKZ_PERMUTEXVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256I idx,__M256 a</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4117</id>
<name>_MM256_MASKZ_RANGE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4126</id>
<name>_MM256_MASKZ_RANGE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4154</id>
<name>_MM256_MASKZ_RCP14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4163</id>
<name>_MM256_MASKZ_RCP14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4277</id>
<name>_MM256_MASKZ_REDUCE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4286</id>
<name>_MM256_MASKZ_REDUCE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4343</id>
<name>_MM256_MASKZ_ROL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4352</id>
<name>_MM256_MASKZ_ROL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4361</id>
<name>_MM256_MASKZ_ROLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4370</id>
<name>_MM256_MASKZ_ROLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4379</id>
<name>_MM256_MASKZ_ROR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4388</id>
<name>_MM256_MASKZ_ROR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,CONST_INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4397</id>
<name>_MM256_MASKZ_RORV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4406</id>
<name>_MM256_MASKZ_RORV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4431</id>
<name>_MM256_MASKZ_ROUNDSCALE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4440</id>
<name>_MM256_MASKZ_ROUNDSCALE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4469</id>
<name>_MM256_MASKZ_RSQRT14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4476</id>
<name>_MM256_MASKZ_RSQRT14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4528</id>
<name>_MM256_MASKZ_SCALEF_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4537</id>
<name>_MM256_MASKZ_SCALEF_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4595</id>
<name>_MM256_MASKZ_SET1_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,SHORT a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast 16-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4604</id>
<name>_MM256_MASKZ_SET1_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4613</id>
<name>_MM256_MASKZ_SET1_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4623</id>
<name>_MM256_MASKZ_SET1_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,CHAR a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast 8-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4692</id>
<name>_MM256_MASKZ_SHUFFLE_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4701</id>
<name>_MM256_MASKZ_SHUFFLE_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		IF b[i+7] == 1
			dst[i+7:i] := 0
		ELSE
			index[3:0] := b[i+3:i]
			dst[i+7:i] := a[index*8+7:index*8]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4707</id>
<name>_MM256_MASKZ_SHUFFLE_F32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VSHUFF32X4</instr>
<desc>Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4713</id>
<name>_MM256_MASKZ_SHUFFLE_F64X2</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VSHUFF64X2</instr>
<desc>Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4719</id>
<name>_MM256_MASKZ_SHUFFLE_I32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VSHUFI32X4</instr>
<desc>Shuffle 128-bits (composed of 4 32-bit integers) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4725</id>
<name>_MM256_MASKZ_SHUFFLE_I64X2</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VSHUFI64X2</instr>
<desc>Shuffle 128-bits (composed of 2 64-bit integers) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4734</id>
<name>_MM256_MASKZ_SHUFFLE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4745</id>
<name>_MM256_MASKZ_SHUFFLE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4754</id>
<name>_MM256_MASKZ_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the high 64 bits of 128-bit lanes of dst, with the low 64 bits of 128-bit lanes being copied from from a to dst, using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[207:192]

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4763</id>
<name>_MM256_MASKZ_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the low 64 bits of 128-bit lanes of dst, with the high 64 bits of 128-bit lanes being copied from from a to dst, using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4813</id>
<name>_MM256_MASKZ_SLL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4822</id>
<name>_MM256_MASKZ_SLL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4831</id>
<name>_MM256_MASKZ_SLL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4843</id>
<name>_MM256_MASKZ_SLLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4852</id>
<name>_MM256_MASKZ_SLLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4861</id>
<name>_MM256_MASKZ_SLLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4875</id>
<name>_MM256_MASKZ_SLLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4884</id>
<name>_MM256_MASKZ_SLLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4893</id>
<name>_MM256_MASKZ_SLLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4904</id>
<name>_MM256_MASKZ_SQRT_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4913</id>
<name>_MM256_MASKZ_SQRT_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4940</id>
<name>_MM256_MASKZ_SRA_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4949</id>
<name>_MM256_MASKZ_SRA_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4958</id>
<name>_MM256_MASKZ_SRA_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4969</id>
<name>_MM256_MASKZ_SRAI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4978</id>
<name>_MM256_MASKZ_SRAI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4987</id>
<name>_MM256_MASKZ_SRAI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4998</id>
<name>_MM256_MASKZ_SRAV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5007</id>
<name>_MM256_MASKZ_SRAV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5016</id>
<name>_MM256_MASKZ_SRAV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5025</id>
<name>_MM256_MASKZ_SRL_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5034</id>
<name>_MM256_MASKZ_SRL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5043</id>
<name>_MM256_MASKZ_SRL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5055</id>
<name>_MM256_MASKZ_SRLI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5064</id>
<name>_MM256_MASKZ_SRLI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5073</id>
<name>_MM256_MASKZ_SRLI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5087</id>
<name>_MM256_MASKZ_SRLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5096</id>
<name>_MM256_MASKZ_SRLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5105</id>
<name>_MM256_MASKZ_SRLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5210</id>
<name>_MM256_MASKZ_SUB_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] - b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5219</id>
<name>_MM256_MASKZ_SUB_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5228</id>
<name>_MM256_MASKZ_SUB_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5237</id>
<name>_MM256_MASKZ_SUB_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] - b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5246</id>
<name>_MM256_MASKZ_SUB_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5258</id>
<name>_MM256_MASKZ_SUB_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5300</id>
<name>_MM256_MASKZ_SUBS_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5309</id>
<name>_MM256_MASKZ_SUBS_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5318</id>
<name>_MM256_MASKZ_SUBS_EPU16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5327</id>
<name>_MM256_MASKZ_SUBS_EPU8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5392</id>
<name>_MM256_MASKZ_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,__M256I c,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using zeromask k at 32-bit granularity (32-bit elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		FOR h := 0 to 31
			index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5401</id>
<name>_MM256_MASKZ_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b,__M256I c,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using zeromask k at 64-bit granularity (64-bit elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		FOR h := 0 to 63
			index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5522</id>
<name>_MM256_MASKZ_UNPACKHI_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5531</id>
<name>_MM256_MASKZ_UNPACKHI_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5540</id>
<name>_MM256_MASKZ_UNPACKHI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5549</id>
<name>_MM256_MASKZ_UNPACKHI_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5558</id>
<name>_MM256_MASKZ_UNPACKHI_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5570</id>
<name>_MM256_MASKZ_UNPACKHI_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5579</id>
<name>_MM256_MASKZ_UNPACKLO_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])

FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5588</id>
<name>_MM256_MASKZ_UNPACKLO_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5597</id>
<name>_MM256_MASKZ_UNPACKLO_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5606</id>
<name>_MM256_MASKZ_UNPACKLO_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M256I a,__M256I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])

FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5615</id>
<name>_MM256_MASKZ_UNPACKLO_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])

FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5627</id>
<name>_MM256_MASKZ_UNPACKLO_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5645</id>
<name>_MM256_MASKZ_XOR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5652</id>
<name>_MM256_MASKZ_XOR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M256I a,__M256I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5660</id>
<name>_MM256_MASKZ_XOR_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M256D a,__M256D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5669</id>
<name>_MM256_MASKZ_XOR_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M256 a,__M256 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3281</id>
<name>_MM256_MAX_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3290</id>
<name>_MM256_MAX_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF a[i+31:i] &amp;gt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3299</id>
<name>_MM256_MAX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF a[i+63:i] &amp;gt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3308</id>
<name>_MM256_MAX_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3317</id>
<name>_MM256_MAX_EPU16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3326</id>
<name>_MM256_MAX_EPU32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF a[i+31:i] &amp;gt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3335</id>
<name>_MM256_MAX_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF a[i+63:i] &amp;gt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3344</id>
<name>_MM256_MAX_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3353</id>
<name>_MM256_MAX_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3363</id>
<name>_MM256_MAX_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3395</id>
<name>_MM256_MIN_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3404</id>
<name>_MM256_MIN_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF a[i+31:i] &amp;lt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3413</id>
<name>_MM256_MIN_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF a[i+63:i] &amp;lt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3422</id>
<name>_MM256_MIN_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3431</id>
<name>_MM256_MIN_EPU16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3440</id>
<name>_MM256_MIN_EPU32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF a[i+31:i] &amp;lt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3449</id>
<name>_MM256_MIN_EPU64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF a[i+63:i] &amp;lt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3458</id>
<name>_MM256_MIN_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3467</id>
<name>_MM256_MIN_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3477</id>
<name>_MM256_MIN_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2776</id>
<name>_MM256_MMASK_I32GATHER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2784</id>
<name>_MM256_MMASK_I32GATHER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	m := j*32
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2792</id>
<name>_MM256_MMASK_I32GATHER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M128I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	m := j*32
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2800</id>
<name>_MM256_MMASK_I32GATHER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2864</id>
<name>_MM256_MMASK_I64GATHER_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2874</id>
<name>_MM256_MMASK_I64GATHER_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2882</id>
<name>_MM256_MMASK_I64GATHER_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2890</id>
<name>_MM256_MMASK_I64GATHER_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,CONST_INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 3
	i := j*32
	m := j*64
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:4] := 0
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>3550</id>
<name>_MM256_MOVEDUP_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[63:0] := a[63:0]
dst[127:64] := a[63:0]
dst[191:128] := a[191:128]
dst[255:192] := a[191:128]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3559</id>
<name>_MM256_MOVEHDUP_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[31:0] := a[63:32] 
dst[63:32] := a[63:32] 
dst[95:64] := a[127:96] 
dst[127:96] := a[127:96]
dst[159:128] := a[191:160] 
dst[191:160] := a[191:160] 
dst[223:192] := a[255:224] 
dst[255:224] := a[255:224]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3569</id>
<name>_MM256_MOVELDUP_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[31:0] := a[31:0] 
dst[63:32] := a[31:0] 
dst[95:64] := a[95:64] 
dst[127:96] := a[95:64]
dst[159:128] := a[159:128] 
dst[191:160] := a[159:128] 
dst[223:192] := a[223:192] 
dst[255:224] := a[223:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3575</id>
<name>_MM256_MOVEMASK_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>int</ret>
<sign>__M256I a</sign>
<instr>VPMOVMSKB</instr>
<desc>
Create mask from the most significant bit of each 8-bit element in a, and store the result in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[j] := a[i+7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>3577</id>
<name>_MM256_MOVEMASK_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256D a</sign>
<instr>VMOVMSKPD</instr>
<desc>Set each bit of mask dst based on the most significant bit of the corresponding packed double-precision (64-bit) floating-point element in a.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF a[i+63]
		dst[j] := 1
	ELSE
		dst[j] := 0
	FI
ENDFOR
dst[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>3580</id>
<name>_MM256_MOVEMASK_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256 a</sign>
<instr>VMOVMSKPS</instr>
<desc>Set each bit of mask dst based on the most significant bit of the corresponding packed single-precision (32-bit) floating-point element in a.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF a[i+31]
		dst[j] := 1
	ELSE
		dst[j] := 0
	FI
ENDFOR
dst[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>3582</id>
<name>_MM256_MOVEPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a</sign>
<instr>VPMOVW2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 16-bit integer in a.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF a[i+15]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3585</id>
<name>_MM256_MOVEPI32_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a</sign>
<instr>VPMOVD2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 32-bit integer in a.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF a[i+31]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>3588</id>
<name>_MM256_MOVEPI64_MASK</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a</sign>
<instr>VPMOVQ2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 64-bit integer in a.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF a[i+63]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>3592</id>
<name>_MM256_MOVEPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a</sign>
<instr>VPMOVB2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 8-bit integer in a.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF a[i+7]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3595</id>
<name>_MM256_MOVM_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k</sign>
<instr>VPMOVM2W</instr>
<desc>Set each packed 16-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := 0xFFFF
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3598</id>
<name>_MM256_MOVM_EPI32</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k</sign>
<instr>VPMOVM2D</instr>
<desc>Set each packed 32-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := 0xFFFFFFFF
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3601</id>
<name>_MM256_MOVM_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k</sign>
<instr>VPMOVM2Q</instr>
<desc>Set each packed 64-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := 0xFFFFFFFFffffffff
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3604</id>
<name>_MM256_MOVM_EPI8</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k</sign>
<instr>VPMOVM2B</instr>
<desc>Set each packed 8-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF k[j]
		dst[i+7:i] := 0xFF
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3608</id>
<name>_MM256_MPSADBW_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VMPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst.
	Eight SADs are performed for each 128-bit lane using one quadruplet from b and eight quadruplets from a. One quadruplet is selected from b starting at on the offset specified in imm8. Eight quadruplets are formed from sequential 8-bit integers selected from a starting at the offset specified in imm8.</desc>
<oper>MPSADBW(a[127:0], b[127:0], imm8[2:0]) {
	a_offset := imm8[2]*32
	b_offset := imm8[1:0]*32
	FOR j := 0 to 7
		i := j*8
		k := a_offset+i
		l := b_offset
		tmp[i+15:i] := ABS(a[k+7:k] - b[l+7:l]) + ABS(a[k+15:k+8] - b[l+15:l+8]) + ABS(a[k+23:k+16] - b[l+23:l+16]) + ABS(a[k+31:k+24] - b[l+31:l+24])
	ENDFOR
	RETURN tmp[127:0]
}

dst[127:0] := MPSADBW(a[127:0], b[127:0], imm8[2:0])
dst[255:128] := MPSADBW(a[255:128], b[255:128], imm8[5:3])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3614</id>
<name>_MM256_MUL_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3623</id>
<name>_MM256_MUL_EPU32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3632</id>
<name>_MM256_MUL_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] * b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3641</id>
<name>_MM256_MUL_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3669</id>
<name>_MM256_MULHI_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3680</id>
<name>_MM256_MULHI_EPU16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3693</id>
<name>_MM256_MULHRS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
	dst[i+15:i] := tmp[16:1]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3703</id>
<name>_MM256_MULLO_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[15:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3712</id>
<name>_MM256_MULLO_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	tmp[63:0] := a[i+31:i] * b[i+31:i]
	dst[i+31:i] := tmp[31:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3721</id>
<name>_MM256_MULLO_EPI64</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	tmp[127:0] := a[i+63:i] * b[i+63:i]
	dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3733</id>
<name>_MM256_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst.</desc>
<oper>FOR i := 0 to 3
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		dst[q+j*8+7:q+j*8] := tmp8[7:0]
	ENDFOR
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3761</id>
<name>_MM256_OR_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3770</id>
<name>_MM256_OR_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3775</id>
<name>_MM256_OR_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPOR</instr>
<desc>Compute the bitwise OR of 256 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[255:0] := (a[255:0] OR b[255:0])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3783</id>
<name>_MM256_PACKS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst.
	</desc>
<oper>dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3792</id>
<name>_MM256_PACKS_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3822</id>
<name>_MM256_PACKUS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3831</id>
<name>_MM256_PACKUS_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3861</id>
<name>_MM256_PERMUTE_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>IF (imm8[0] == 0) dst[63:0] := a[63:0]
IF (imm8[0] == 1) dst[63:0] := a[127:64]
IF (imm8[1] == 0) dst[127:64] := a[63:0]
IF (imm8[1] == 1) dst[127:64] := a[127:64]
IF (imm8[2] == 0) dst[191:128] := a[191:128]
IF (imm8[2] == 1) dst[191:128] := a[255:192]
IF (imm8[3] == 0) dst[255:192] := a[191:128]
IF (imm8[3] == 1) dst[255:192] := a[255:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3870</id>
<name>_MM256_PERMUTE_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(a[255:128], imm8[5:4])
dst[255:224] := SELECT4(a[255:128], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3874</id>
<name>_MM256_PERMUTE2F128_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,INT imm8</sign>
<instr>VPERM2F128</instr>
<desc>Shuffle 128-bits (composed of 2 packed double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst. </desc>
<oper>SELECT4(src1, src2, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src1[127:0]
	1:	tmp[127:0] := src1[255:128]
	2:	tmp[127:0] := src2[127:0]
	3:	tmp[127:0] := src2[255:128]
	ESAC
	IF control[3]
		tmp[127:0] := 0
	FI
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3875</id>
<name>_MM256_PERMUTE2F128_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,INT imm8</sign>
<instr>VPERM2F128</instr>
<desc>Shuffle 128-bits (composed of 4 packed single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst. </desc>
<oper>SELECT4(src1, src2, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src1[127:0]
	1:	tmp[127:0] := src1[255:128]
	2:	tmp[127:0] := src2[127:0]
	3:	tmp[127:0] := src2[255:128]
	ESAC
	IF control[3]
		tmp[127:0] := 0
	FI
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3876</id>
<name>_MM256_PERMUTE2F128_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,INT imm8</sign>
<instr>VPERM2F128</instr>
<desc>Shuffle 128-bits (composed of integer data) selected by imm8 from a and b, and store the results in dst. </desc>
<oper>SELECT4(src1, src2, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src1[127:0]
	1:	tmp[127:0] := src1[255:128]
	2:	tmp[127:0] := src2[127:0]
	3:	tmp[127:0] := src2[255:128]
	ESAC
	IF control[3]
		tmp[127:0] := 0
	FI
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3877</id>
<name>_MM256_PERMUTE2X128_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VPERM2I128</instr>
<desc>Shuffle 128-bits (composed of integer data) selected by imm8 from a and b, and store the results in dst. </desc>
<oper>SELECT4(src1, src2, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src1[127:0]
	1:	tmp[127:0] := src1[255:128]
	2:	tmp[127:0] := src2[127:0]
	3:	tmp[127:0] := src2[255:128]
	ESAC
	IF control[3]
		tmp[127:0] := 0
	FI
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3882</id>
<name>_MM256_PERMUTE4X64_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3883</id>
<name>_MM256_PERMUTE4X64_PD</name>
<cpuid>AVX2</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,CONST_INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3891</id>
<name>_MM256_PERMUTEVAR_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst.</desc>
<oper>IF (b[1] == 0) dst[63:0] := a[63:0]
IF (b[1] == 1) dst[63:0] := a[127:64]
IF (b[65] == 0) dst[127:64] := a[63:0]
IF (b[65] == 1) dst[127:64] := a[127:64]
IF (b[129] == 0) dst[191:128] := a[191:128]
IF (b[129] == 1) dst[191:128] := a[255:192]
IF (b[193] == 0) dst[255:192] := a[191:128]
IF (b[193] == 1) dst[255:192] := a[255:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3900</id>
<name>_MM256_PERMUTEVAR_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], b[1:0])
dst[63:32] := SELECT4(a[127:0], b[33:32])
dst[95:64] := SELECT4(a[127:0], b[65:64])
dst[127:96] := SELECT4(a[127:0], b[97:96])
dst[159:128] := SELECT4(a[255:128], b[129:128])
dst[191:160] := SELECT4(a[255:128], b[161:160])
dst[223:192] := SELECT4(a[255:128], b[193:192])
dst[255:224] := SELECT4(a[255:128], b[225:224])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3904</id>
<name>_MM256_PERMUTEVAR8X32_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3905</id>
<name>_MM256_PERMUTEVAR8X32_PS</name>
<cpuid>AVX2</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256I idx</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3908</id>
<name>_MM256_PERMUTEX_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3914</id>
<name>_MM256_PERMUTEX_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3925</id>
<name>_MM256_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2W, VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	off := 16*idx[i+3:i]
	dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3937</id>
<name>_MM256_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2D, VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3949</id>
<name>_MM256_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2Q, VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3961</id>
<name>_MM256_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I idx,__M256I b</sign>
<instr>VPERMI2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	off := 8*idx[i+4:i]
	dst[i+7:i] := idx[i+6] ? b[off+5:off] : a[off+7:off]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3973</id>
<name>_MM256_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256I idx,__M256D b</sign>
<instr>VPERMI2PD, VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	off := idx[i+1:i]*64
	dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3985</id>
<name>_MM256_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256I idx,__M256 b</sign>
<instr>VPERMI2PS, VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	off := idx[i+2:i]*32
	dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>3995</id>
<name>_MM256_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I idx,__M256I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	id := idx[i+3:i]*16
	dst[i+15:i] := a[id+15:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4001</id>
<name>_MM256_PERMUTEXVAR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I idx,__M256I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4007</id>
<name>_MM256_PERMUTEXVAR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I idx,__M256I a</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	id := idx[i+1:i]*64
	dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4016</id>
<name>_MM256_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VL, AVX512VBMI</cpuid>
<ret>__m256i</ret>
<sign>__M256I idx,__M256I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	id := idx[i+4:i]*8
	dst[i+7:i] := a[id+7:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4022</id>
<name>_MM256_PERMUTEXVAR_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256I idx,__M256D a</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	id := idx[i+1:i]*64
	dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4028</id>
<name>_MM256_PERMUTEXVAR_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256I idx,__M256 a</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	id := idx[i+2:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4051</id>
<name>_MM256_POW_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed double-precision (64-bit) floating-point elements in a raised by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4055</id>
<name>_MM256_POW_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed single-precision (32-bit) floating-point elements in a raised by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4118</id>
<name>_MM256_RANGE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4127</id>
<name>_MM256_RANGE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4148</id>
<name>_MM256_RCP_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VRCPPS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 1.5*2^-12.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4155</id>
<name>_MM256_RCP14_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4164</id>
<name>_MM256_RCP14_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4278</id>
<name>_MM256_REDUCE_PD</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst. </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4287</id>
<name>_MM256_REDUCE_PS</name>
<cpuid>AVX512DQ, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst.</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4310</id>
<name>_MM256_REM_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 16-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4313</id>
<name>_MM256_REM_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4317</id>
<name>_MM256_REM_EPI64</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 64-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4320</id>
<name>_MM256_REM_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed 8-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 31
	i := 8*j
	dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4323</id>
<name>_MM256_REM_EPU16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 16-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4326</id>
<name>_MM256_REM_EPU32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4330</id>
<name>_MM256_REM_EPU64</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 64-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 3
	i := 64*j
	dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4333</id>
<name>_MM256_REM_EPU8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 8-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 31
	i := 8*j
	dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4344</id>
<name>_MM256_ROL_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4353</id>
<name>_MM256_ROL_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4362</id>
<name>_MM256_ROLV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4371</id>
<name>_MM256_ROLV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4380</id>
<name>_MM256_ROR_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4389</id>
<name>_MM256_ROR_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4398</id>
<name>_MM256_RORV_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4407</id>
<name>_MM256_RORV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4416</id>
<name>_MM256_ROUND_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,INT rounding</sign>
<instr>VROUNDPD</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a using the rounding parameter, and store the results as packed double-precision floating-point elements in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ROUND(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4418</id>
<name>_MM256_ROUND_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,INT rounding</sign>
<instr>VROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a using the rounding parameter, and store the results as packed single-precision floating-point elements in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ROUND(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4432</id>
<name>_MM256_ROUNDSCALE_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4441</id>
<name>_MM256_ROUNDSCALE_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4464</id>
<name>_MM256_RSQRT_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VRSQRTPS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 1.5*2^-12.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4513</id>
<name>_MM256_SAD_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSADBW</instr>
<desc>Compute the absolute differences of packed unsigned 8-bit integers in a and b, then horizontally sum each consecutive 8 differences to produce four unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low 16 bits of 64-bit elements in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
ENDFOR
FOR j := 0 to 4
	i := j*64
	dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] + tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
	dst[i+63:i+16] := 0
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4529</id>
<name>_MM256_SCALEF_PD</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4538</id>
<name>_MM256_SCALEF_PS</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4561</id>
<name>_MM256_SET_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>SHORT e15,SHORT e14,SHORT e13,SHORT e12,SHORT e11,SHORT e10,SHORT e9,SHORT e8,SHORT e7,SHORT e6,SHORT e5,SHORT e4,SHORT e3,SHORT e2,SHORT e1,SHORT e0</sign>
<instr>NONE</instr>
<desc>Set packed 16-bit integers in dst with the supplied values.</desc>
<oper>dst[15:0] := e0
dst[31:16] := e1
dst[47:32] := e2
dst[63:48] := e3
dst[79:64] := e4
dst[95:80] := e5
dst[111:96] := e6
dst[127:112] := e7
dst[145:128] := e8
dst[159:144] := e9
dst[175:160] := e10
dst[191:176] := e11
dst[207:192] := e12
dst[223:208] := e13
dst[239:224] := e14
dst[255:240] := e15
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4563</id>
<name>_MM256_SET_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>INT e7,INT e6,INT e5,INT e4,INT e3,INT e2,INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3
dst[159:128] := e4
dst[191:160] := e5
dst[223:192] := e6
dst[255:224] := e7
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4568</id>
<name>_MM256_SET_EPI64X</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__INT64 e3,__INT64 e2,__INT64 e1,__INT64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1
dst[191:128] := e2
dst[255:192] := e3
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4570</id>
<name>_MM256_SET_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>CHAR e31,CHAR e30,CHAR e29,CHAR e28,CHAR e27,CHAR e26,CHAR e25,CHAR e24,CHAR e23,CHAR e22,CHAR e21,CHAR e20,CHAR e19,CHAR e18,CHAR e17,CHAR e16,CHAR e15,CHAR e14,CHAR e13,CHAR e12,CHAR e11,CHAR e10,CHAR e9,CHAR e8,CHAR e7,CHAR e6,CHAR e5,CHAR e4,CHAR e3,CHAR e2,CHAR e1,CHAR e0</sign>
<instr>NONE</instr>
<desc>Set packed 8-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[7:0] := e0
dst[15:8] := e1
dst[23:16] := e2
dst[31:24] := e3
dst[39:32] := e4
dst[47:40] := e5
dst[55:48] := e6
dst[63:56] := e7
dst[71:64] := e8
dst[79:72] := e9
dst[87:80] := e10
dst[95:88] := e11
dst[103:96] := e12
dst[111:104] := e13
dst[119:112] := e14
dst[127:120] := e15
dst[135:128] := e16
dst[143:136] := e17
dst[151:144] := e18
dst[159:152] := e19
dst[167:160] := e20
dst[175:168] := e21
dst[183:176] := e22
dst[191:184] := e23
dst[199:192] := e24
dst[207:200] := e25
dst[215:208] := e26
dst[223:216] := e27
dst[231:224] := e28
dst[239:232] := e29
dst[247:240] := e30
dst[255:248] := e31
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4574</id>
<name>_MM256_SET_M128</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M128 hi,__M128 lo</sign>
<instr>VINSERTF128</instr>
<desc>Set packed __m256 vector dst with the supplied values.</desc>
<oper>dst[127:0] := lo[127:0]
dst[255:128] := hi[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4575</id>
<name>_MM256_SET_M128D</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M128D hi,__M128D lo</sign>
<instr>VINSERTF128</instr>
<desc>Set packed __m256d vector dst with the supplied values.</desc>
<oper>dst[127:0] := lo[127:0]
dst[255:128] := hi[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4576</id>
<name>_MM256_SET_M128I</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M128I hi,__M128I lo</sign>
<instr>VINSERTF128</instr>
<desc>Set packed __m256i vector dst with the supplied values.</desc>
<oper>dst[127:0] := lo[127:0]
dst[255:128] := hi[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4578</id>
<name>_MM256_SET_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE e3,DOUBLE e2,DOUBLE e1,DOUBLE e0</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1
dst[191:128] := e2
dst[255:192] := e3
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4585</id>
<name>_MM256_SET_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>FLOAT e7,FLOAT e6,FLOAT e5,FLOAT e4,FLOAT e3,FLOAT e2,FLOAT e1,FLOAT e0</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3
dst[159:128] := e4
dst[191:160] := e5
dst[223:192] := e6
dst[255:224] := e7
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4596</id>
<name>_MM256_SET1_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>SHORT a</sign>
<instr>NONE</instr>
<desc>Broadcast 16-bit integer a to all all elements of dst. This intrinsic may generate the vpbroadcastw.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4605</id>
<name>_MM256_SET1_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>INT a</sign>
<instr>NONE</instr>
<desc>Broadcast 32-bit integer a to all elements of dst. This intrinsic may generate the vpbroadcastd.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4618</id>
<name>_MM256_SET1_EPI64X</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>LONG_LONG a</sign>
<instr>NONE</instr>
<desc>Broadcast 64-bit integer a to all elements of dst. This intrinsic may generate the vpbroadcastq.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4624</id>
<name>_MM256_SET1_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>CHAR a</sign>
<instr>NONE</instr>
<desc>Broadcast 8-bit integer a to all elements of dst. This intrinsic may generate the vpbroadcastb.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4629</id>
<name>_MM256_SET1_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE a</sign>
<instr>NONE</instr>
<desc>Broadcast double-precision (64-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4635</id>
<name>_MM256_SET1_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>FLOAT a</sign>
<instr>NONE</instr>
<desc>Broadcast single-precision (32-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4643</id>
<name>_MM256_SETR_EPI16</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>SHORT e15,SHORT e14,SHORT e13,SHORT e12,SHORT e11,SHORT e10,SHORT e9,SHORT e8,SHORT e7,SHORT e6,SHORT e5,SHORT e4,SHORT e3,SHORT e2,SHORT e1,SHORT e0</sign>
<instr>NONE</instr>
<desc>Set packed 16-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[15:0] := e15
dst[31:16] := e14
dst[47:32] := e13
dst[63:48] := e12
dst[79:64] := e11
dst[95:80] := e10
dst[111:96] := e9
dst[127:112] := e8
dst[145:128] := e7
dst[159:144] := e6
dst[175:160] := e5
dst[191:176] := e4
dst[207:192] := e3
dst[223:208] := e2
dst[239:224] := e1
dst[255:240] := e0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4645</id>
<name>_MM256_SETR_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>INT e7,INT e6,INT e5,INT e4,INT e3,INT e2,INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e7
dst[63:32] := e6
dst[95:64] := e5
dst[127:96] := e4
dst[159:128] := e3
dst[191:160] := e2
dst[223:192] := e1
dst[255:224] := e0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4649</id>
<name>_MM256_SETR_EPI64X</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__INT64 e3,__INT64 e2,__INT64 e1,__INT64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[63:0] := e3
dst[127:64] := e2
dst[191:128] := e1
dst[255:192] := e0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4651</id>
<name>_MM256_SETR_EPI8</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>CHAR e31,CHAR e30,CHAR e29,CHAR e28,CHAR e27,CHAR e26,CHAR e25,CHAR e24,CHAR e23,CHAR e22,CHAR e21,CHAR e20,CHAR e19,CHAR e18,CHAR e17,CHAR e16,CHAR e15,CHAR e14,CHAR e13,CHAR e12,CHAR e11,CHAR e10,CHAR e9,CHAR e8,CHAR e7,CHAR e6,CHAR e5,CHAR e4,CHAR e3,CHAR e2,CHAR e1,CHAR e0</sign>
<instr>NONE</instr>
<desc>Set packed 8-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[7:0] := e31
dst[15:8] := e30
dst[23:16] := e29
dst[31:24] := e28
dst[39:32] := e27
dst[47:40] := e26
dst[55:48] := e25
dst[63:56] := e24
dst[71:64] := e23
dst[79:72] := e22
dst[87:80] := e21
dst[95:88] := e20
dst[103:96] := e19
dst[111:104] := e18
dst[119:112] := e17
dst[127:120] := e16
dst[135:128] := e15
dst[143:136] := e14
dst[151:144] := e13
dst[159:152] := e12
dst[167:160] := e11
dst[175:168] := e10
dst[183:176] := e9
dst[191:184] := e8
dst[199:192] := e7
dst[207:200] := e6
dst[215:208] := e5
dst[223:216] := e4
dst[231:224] := e3
dst[239:232] := e2
dst[247:240] := e1
dst[255:248] := e0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4652</id>
<name>_MM256_SETR_M128</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M128 lo,__M128 hi</sign>
<instr>VINSERTF128</instr>
<desc>Set packed __m256 vector dst with the supplied values.</desc>
<oper>dst[127:0] := lo[127:0]
dst[255:128] := hi[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4653</id>
<name>_MM256_SETR_M128D</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M128D lo,__M128D hi</sign>
<instr>VINSERTF128</instr>
<desc>Set packed __m256d vector dst with the supplied values.</desc>
<oper>dst[127:0] := lo[127:0]
dst[255:128] := hi[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4654</id>
<name>_MM256_SETR_M128I</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign>__M128I lo,__M128I hi</sign>
<instr>VINSERTF128</instr>
<desc>Set packed __m256i vector dst with the supplied values.</desc>
<oper>dst[127:0] := lo[127:0]
dst[255:128] := hi[127:0]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4656</id>
<name>_MM256_SETR_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>DOUBLE e3,DOUBLE e2,DOUBLE e1,DOUBLE e0</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the supplied values in reverse order.</desc>
<oper>dst[63:0] := e3
dst[127:64] := e2
dst[191:128] := e1
dst[255:192] := e0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4662</id>
<name>_MM256_SETR_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>FLOAT e7,FLOAT e6,FLOAT e5,FLOAT e4,FLOAT e3,FLOAT e2,FLOAT e1,FLOAT e0</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e7
dst[63:32] := e6
dst[95:64] := e5
dst[127:96] := e4
dst[159:128] := e3
dst[191:160] := e2
dst[223:192] := e1
dst[255:224] := e0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4671</id>
<name>_MM256_SETZERO_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign></sign>
<instr>VXORPD</instr>
<desc>Return vector of type __m256d with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4674</id>
<name>_MM256_SETZERO_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign></sign>
<instr>VXORPS</instr>
<desc>Return vector of type __m256 with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4677</id>
<name>_MM256_SETZERO_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign></sign>
<instr>VPXOR</instr>
<desc>Return vector of type __m256i with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4693</id>
<name>_MM256_SHUFFLE_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(a[255:128], imm8[5:4])
dst[255:224] := SELECT4(a[255:128], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4702</id>
<name>_MM256_SHUFFLE_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle 8-bit integers in a within 128-bit lanes according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*8
	IF b[i+7] == 1
		dst[i+7:i] := 0
	ELSE
		index[3:0] := b[i+3:i]
		dst[i+7:i] := a[index*8+7:index*8]
	FI
	IF b[128+i+7] == 1
		dst[128+i+7:i] := 0
	ELSE
		index[3:0] := b[128+i+3:128+i]
		dst[128+i+7:i] := a[128+index*8+7:128+index*8]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4708</id>
<name>_MM256_SHUFFLE_F32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VSHUFF32X4</instr>
<desc>Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst.
	</desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT2(a[255:0], imm8[0])
dst[255:128] := SELECT2(b[255:0], imm8[1])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4714</id>
<name>_MM256_SHUFFLE_F64X2</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VSHUFF64X2</instr>
<desc>Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT2(a[255:0], imm8[0])
dst[255:128] := SELECT2(b[255:0], imm8[1])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4720</id>
<name>_MM256_SHUFFLE_I32X4</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VSHUFI32X4</instr>
<desc>Shuffle 128-bits (composed of 4 32-bit integers) selected by imm8 from a and b, and store the results in dst.</desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT2(a[255:0], imm8[1:0])
dst[255:128] := SELECT2(b[255:0], imm8[3:2])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4726</id>
<name>_MM256_SHUFFLE_I64X2</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,CONST_INT imm8</sign>
<instr>VSHUFI64X2</instr>
<desc>Shuffle 128-bits (composed of 2 64-bit integers) selected by imm8 from a and b, and store the results in dst.</desc>
<oper>SELECT2(src, control){
	CASE(control[0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT2(a[255:0], imm8[1:0])
dst[255:128] := SELECT2(b[255:0], imm8[3:2])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4735</id>
<name>_MM256_SHUFFLE_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in imm8, and store the results in dst. </desc>
<oper>dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4746</id>
<name>_MM256_SHUFFLE_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(b[127:0], imm8[5:4])
dst[127:96] := SELECT4(b[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(b[255:128], imm8[5:4])
dst[255:224] := SELECT4(b[255:128], imm8[7:6])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4755</id>
<name>_MM256_SHUFFLEHI_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the high 64 bits of 128-bit lanes of dst, with the low 64 bits of 128-bit lanes being copied from from a to dst.</desc>
<oper>dst[63:0] := a[63:0]
dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]
dst[191:128] := a[191:128]
dst[207:192] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[207:192]
dst[223:208] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[207:192]
dst[239:224] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[207:192]
dst[255:240] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[207:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4764</id>
<name>_MM256_SHUFFLELO_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the low 64 bits of 128-bit lanes of dst, with the high 64 bits of 128-bit lanes being copied from from a to dst.</desc>
<oper>dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
dst[127:64] := a[127:64]
dst[143:128] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[143:128]
dst[159:144] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[143:128]
dst[175:160] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[143:128]
dst[191:176] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[143:128]
dst[255:192] := a[255:192]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4769</id>
<name>_MM256_SIGN_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSIGNW</instr>
<desc>Negate packed 16-bit integers in a when the corresponding signed 16-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF b[i+15:i] &amp;lt; 0
		dst[i+15:i] := NEG(a[i+15:i])
	ELSE IF b[i+15:i] = 0
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4771</id>
<name>_MM256_SIGN_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSIGND</instr>
<desc>Negate packed 32-bit integers in a when the corresponding signed 32-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF b[i+31:i] &amp;lt; 0
		dst[i+31:i] := NEG(a[i+31:i])
	ELSE IF b[i+31:i] = 0
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4773</id>
<name>_MM256_SIGN_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSIGNB</instr>
<desc>Negate packed 8-bit integers in a when the corresponding signed 8-bit integer in b is negative, and store the results in dst. Element in dst are zeroed out when the corresponding element in b is zero.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	IF b[i+7:i] &amp;lt; 0
		dst[i+7:i] := NEG(a[i+7:i])
	ELSE IF b[i+7:i] = 0
		dst[i+7:i] := 0
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4778</id>
<name>_MM256_SIN_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SIN(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4782</id>
<name>_MM256_SIN_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SIN(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4786</id>
<name>_MM256_SINCOS_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D_PTR mem_addr,__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the sine and cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, store the sine in dst, and store the cosine into memory at mem_addr.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SIN(a[i+63:i])
	MEM[mem_addr+i+63:mem_addr+i] := COS(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4790</id>
<name>_MM256_SINCOS_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256_PTR mem_addr,__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the sine and cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, store the sine in dst, and store the cosine into memory at mem_addr.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SIN(a[i+31:i])
	MEM[mem_addr+i+31:mem_addr+i] := COS(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4794</id>
<name>_MM256_SIND_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SIND(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4798</id>
<name>_MM256_SIND_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SIND(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4802</id>
<name>_MM256_SINH_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SINH(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4806</id>
<name>_MM256_SINH_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SINH(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4814</id>
<name>_MM256_SLL_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4823</id>
<name>_MM256_SLL_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4832</id>
<name>_MM256_SLL_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4844</id>
<name>_MM256_SLLI_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4853</id>
<name>_MM256_SLLI_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4862</id>
<name>_MM256_SLLI_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4869</id>
<name>_MM256_SLLI_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSLLDQ</instr>
<desc>Shift 128-bit lanes in a left by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;lt;&amp;lt; (tmp*8)
dst[255:128] := a[255:128] &amp;lt;&amp;lt; (tmp*8)
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4876</id>
<name>_MM256_SLLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4885</id>
<name>_MM256_SLLV_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4894</id>
<name>_MM256_SLLV_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4905</id>
<name>_MM256_SQRT_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4914</id>
<name>_MM256_SQRT_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4941</id>
<name>_MM256_SRA_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4950</id>
<name>_MM256_SRA_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4959</id>
<name>_MM256_SRA_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := SignBit
	ELSE
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4970</id>
<name>_MM256_SRAI_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4979</id>
<name>_MM256_SRAI_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4988</id>
<name>_MM256_SRAI_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := SignBit
	ELSE
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>4999</id>
<name>_MM256_SRAV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5008</id>
<name>_MM256_SRAV_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5017</id>
<name>_MM256_SRAV_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5026</id>
<name>_MM256_SRL_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5035</id>
<name>_MM256_SRL_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5044</id>
<name>_MM256_SRL_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5056</id>
<name>_MM256_SRLI_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5065</id>
<name>_MM256_SRLI_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5074</id>
<name>_MM256_SRLI_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5081</id>
<name>_MM256_SRLI_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,CONST_INT imm8</sign>
<instr>VPSRLDQ</instr>
<desc>Shift 128-bit lanes in a right by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;gt;&amp;gt; (tmp*8)
dst[255:128] := a[255:128] &amp;gt;&amp;gt; (tmp*8)
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5088</id>
<name>_MM256_SRLV_EPI16</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5097</id>
<name>_MM256_SRLV_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5106</id>
<name>_MM256_SRLV_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5121</id>
<name>_MM256_STORE_PD</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M256D a</sign>
<instr>VMOVAPD</instr>
<desc>Store 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from a into memory.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5128</id>
<name>_MM256_STORE_PS</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M256 a</sign>
<instr>VMOVAPS</instr>
<desc>Store 256-bits (composed of 8 packed single-precision (32-bit) floating-point elements) from a into memory.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5135</id>
<name>_MM256_STORE_SI256</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>__M256I_PTR mem_addr,__M256I a</sign>
<instr>VMOVDQA</instr>
<desc>Store 256-bits of integer data from a into memory.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5170</id>
<name>_MM256_STOREU_PD</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M256D a</sign>
<instr>VMOVUPD</instr>
<desc>Store 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from a into memory.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5176</id>
<name>_MM256_STOREU_PS</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M256 a</sign>
<instr>VMOVUPS</instr>
<desc>Store 256-bits (composed of 8 packed single-precision (32-bit) floating-point elements) from a into memory.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5182</id>
<name>_MM256_STOREU_SI256</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>__M256I_PTR mem_addr,__M256I a</sign>
<instr>VMOVDQU</instr>
<desc>Store 256-bits of integer data from a into memory.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5188</id>
<name>_MM256_STOREU2_M128</name>
<cpuid>AVX, SVML</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR hiaddr,FLOAT_PTR loaddr,__M256 a</sign>
<instr>NONE</instr>
<desc>Store the high and low 128-bit halves (each composed of 4 packed single-precision (32-bit) floating-point elements) from a into memory two different 128-bit locations.
	hiaddr and loaddr do not need to be aligned on any particular boundary.</desc>
<oper>MEM[loaddr+127:loaddr] := a[127:0]
MEM[hiaddr+127:hiaddr] := a[255:128]</oper>
</intrinsic>
<intrinsic>
<id>5189</id>
<name>_MM256_STOREU2_M128D</name>
<cpuid>AVX, SVML</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR hiaddr,DOUBLE_PTR loaddr,__M256D a</sign>
<instr>NONE</instr>
<desc>Store the high and low 128-bit halves (each composed of 2 packed double-precision (64-bit) floating-point elements) from a into memory two different 128-bit locations.
	hiaddr and loaddr do not need to be aligned on any particular boundary.</desc>
<oper>MEM[loaddr+127:loaddr] := a[127:0]
MEM[hiaddr+127:hiaddr] := a[255:128]</oper>
</intrinsic>
<intrinsic>
<id>5190</id>
<name>_MM256_STOREU2_M128I</name>
<cpuid>AVX, SVML</cpuid>
<ret>void</ret>
<sign>__M128I_PTR hiaddr,__M128I_PTR loaddr,__M256I a</sign>
<instr>NONE</instr>
<desc>Store the high and low 128-bit halves (each composed of integer data) from a into memory two different 128-bit locations.
	hiaddr and loaddr do not need to be aligned on any particular boundary.</desc>
<oper>MEM[loaddr+127:loaddr] := a[127:0]
MEM[hiaddr+127:hiaddr] := a[255:128]</oper>
</intrinsic>
<intrinsic>
<id>5192</id>
<name>_MM256_STREAM_LOAD_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I_CONST_PTR mem_addr</sign>
<instr>VMOVNTDQA</instr>
<desc>Load 256-bits of integer data from memory into dst using a non-temporal memory hint.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5195</id>
<name>_MM256_STREAM_PD</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>DOUBLE_PTR mem_addr,__M256D a</sign>
<instr>VMOVNTPD</instr>
<desc>Store 256-bits (composed of 4 packed double-precision (64-bit) floating-point elements) from a into memory using a non-temporal memory hint.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5199</id>
<name>_MM256_STREAM_PS</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>FLOAT_PTR mem_addr,__M256 a</sign>
<instr>VMOVNTPS</instr>
<desc>Store 256-bits (composed of 8 packed single-precision (32-bit) floating-point elements) from a into memory using a non-temporal memory hint.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5202</id>
<name>_MM256_STREAM_SI256</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign>__M256I_PTR mem_addr,__M256I a</sign>
<instr>VMOVNTDQ</instr>
<desc>Store 256-bits of integer data from a into memory using a non-temporal memory hint.
	mem_addr must be aligned on a 32-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+255:mem_addr] := a[255:0]</oper>
</intrinsic>
<intrinsic>
<id>5211</id>
<name>_MM256_SUB_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := a[i+15:i] - b[i+15:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5220</id>
<name>_MM256_SUB_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5229</id>
<name>_MM256_SUB_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5238</id>
<name>_MM256_SUB_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := a[i+7:i] - b[i+7:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5247</id>
<name>_MM256_SUB_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5259</id>
<name>_MM256_SUB_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5301</id>
<name>_MM256_SUBS_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5310</id>
<name>_MM256_SUBS_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5319</id>
<name>_MM256_SUBS_EPU16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5328</id>
<name>_MM256_SUBS_EPU8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5339</id>
<name>_MM256_SVML_CEIL_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a up to an integer value, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := CEIL(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5341</id>
<name>_MM256_SVML_CEIL_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a up to an integer value, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := CEIL(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5343</id>
<name>_MM256_SVML_FLOOR_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a down to an integer value, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := FLOOR(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5345</id>
<name>_MM256_SVML_FLOOR_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a down to an integer value, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := FLOOR(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5347</id>
<name>_MM256_SVML_ROUND_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a to the nearest integer value, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := ROUND(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5351</id>
<name>_MM256_SVML_ROUND_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a to the nearest integer value, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := ROUND(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5353</id>
<name>_MM256_SVML_SQRT_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. Note that this intrinsic is less efficient than _mm_sqrt_pd.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5355</id>
<name>_MM256_SVML_SQRT_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. Note that this intrinsic is less efficient than _mm_sqrt_ps.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5365</id>
<name>_MM256_TAN_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := TAN(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5369</id>
<name>_MM256_TAN_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := TAN(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5373</id>
<name>_MM256_TAND_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := TAND(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5377</id>
<name>_MM256_TAND_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := TAND(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5381</id>
<name>_MM256_TANH_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := TANH(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5385</id>
<name>_MM256_TANH_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := TANH(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5393</id>
<name>_MM256_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,__M256I c,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	FOR h := 0 to 31
		index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
		dst[i+h] := imm8[index[2:0]]
	ENDFOR
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5402</id>
<name>_MM256_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b,__M256I c,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	FOR h := 0 to 63
		index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
		dst[i+h] := imm8[index[2:0]]
	ENDFOR
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5411</id>
<name>_MM256_TEST_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTMW</instr>
<desc>Compute the bitwise AND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5417</id>
<name>_MM256_TEST_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTMD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5423</id>
<name>_MM256_TEST_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTMQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5429</id>
<name>_MM256_TEST_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTMB</instr>
<desc>Compute the bitwise AND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5434</id>
<name>_MM256_TESTC_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VTESTPD</instr>
<desc>Compute the bitwise AND of 256 bits (representing double-precision (64-bit) floating-point elements) in a and b, producing an intermediate 256-bit value, and set ZF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set CF to 0. Return the CF value.</desc>
<oper>tmp[255:0] := a[255:0] AND b[255:0]
IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[255:0] := (NOT a[255:0]) AND b[255:0]
IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5436</id>
<name>_MM256_TESTC_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VTESTPS</instr>
<desc>Compute the bitwise AND of 256 bits (representing single-precision (32-bit) floating-point elements) in a and b, producing an intermediate 256-bit value, and set ZF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set CF to 0. Return the CF value.</desc>
<oper>tmp[255:0] := a[255:0] AND b[255:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[255:0] := (NOT a[255:0]) AND b[255:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5438</id>
<name>_MM256_TESTC_SI256</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTEST</instr>
<desc>Compute the bitwise AND of 256 bits (representing integer data) in a and b, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero, otherwise set CF to 0. Return the CF value.</desc>
<oper>IF (a[255:0] AND b[255:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[255:0]) AND b[255:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN CF</oper>
</intrinsic>
<intrinsic>
<id>5442</id>
<name>_MM256_TESTN_EPI16_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask16</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTNMW</instr>
<desc>Compute the bitwise NAND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*16
	k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5448</id>
<name>_MM256_TESTN_EPI32_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTNMD</instr>
<desc>Compute the bitwise NAND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5454</id>
<name>_MM256_TESTN_EPI64_MASK</name>
<cpuid>AVX512F, AVX512VL</cpuid>
<ret>__mmask8</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTNMQ</instr>
<desc>Compute the bitwise NAND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0</oper>
</intrinsic>
<intrinsic>
<id>5460</id>
<name>_MM256_TESTN_EPI8_MASK</name>
<cpuid>AVX512BW, AVX512VL</cpuid>
<ret>__mmask32</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTESTNMB</instr>
<desc>Compute the bitwise NAND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5464</id>
<name>_MM256_TESTNZC_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VTESTPD</instr>
<desc>Compute the bitwise AND of 256 bits (representing double-precision (64-bit) floating-point elements) in a and b, producing an intermediate 256-bit value, and set ZF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>tmp[255:0] := a[255:0] AND b[255:0]
IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[255:0] := (NOT a[255:0]) AND b[255:0]
IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5466</id>
<name>_MM256_TESTNZC_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VTESTPS</instr>
<desc>Compute the bitwise AND of 256 bits (representing single-precision (32-bit) floating-point elements) in a and b, producing an intermediate 256-bit value, and set ZF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>tmp[255:0] := a[255:0] AND b[255:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255]  == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[255:0] := (NOT a[255:0]) AND b[255:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255]  == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5468</id>
<name>_MM256_TESTNZC_SI256</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTEST</instr>
<desc>Compute the bitwise AND of 256 bits (representing integer data) in a and b, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero, otherwise return 0.</desc>
<oper>IF (a[255:0] AND b[255:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[255:0]) AND b[255:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
IF (ZF == 0 &amp;&amp; CF == 0)
	RETURN 1
ELSE
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5470</id>
<name>_MM256_TESTZ_PD</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VTESTPD</instr>
<desc>Compute the bitwise AND of 256 bits (representing double-precision (64-bit) floating-point elements) in a and b, producing an intermediate 256-bit value, and set ZF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 64-bit element in the intermediate value is zero, otherwise set CF to 0. Return the ZF value.</desc>
<oper>tmp[255:0] := a[255:0] AND b[255:0]
IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[255:0] := (NOT a[255:0]) AND b[255:0]
IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5472</id>
<name>_MM256_TESTZ_PS</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VTESTPS</instr>
<desc>Compute the bitwise AND of 256 bits (representing single-precision (32-bit) floating-point elements) in a and b, producing an intermediate 256-bit value, and set ZF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, producing an intermediate value, and set CF to 1 if the sign bit of each 32-bit element in the intermediate value is zero, otherwise set CF to 0. Return the ZF value.</desc>
<oper>tmp[255:0] := a[255:0] AND b[255:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
tmp[255:0] := (NOT a[255:0]) AND b[255:0]
IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5474</id>
<name>_MM256_TESTZ_SI256</name>
<cpuid>AVX</cpuid>
<ret>int</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPTEST</instr>
<desc>Compute the bitwise AND of 256 bits (representing integer data) in a and b, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero, otherwise set CF to 0. Return the ZF value.</desc>
<oper>IF (a[255:0] AND b[255:0] == 0)
	ZF := 1
ELSE
	ZF := 0
FI
IF ((NOT a[255:0]) AND b[255:0] == 0)
	CF := 1
ELSE
	CF := 0
FI
RETURN ZF</oper>
</intrinsic>
<intrinsic>
<id>5479</id>
<name>_MM256_TRUNC_PD</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256d</ret>
<sign>__M256D a</sign>
<instr>NONE</instr>
<desc>Truncate the packed double-precision (64-bit) floating-point elements in a, and store the results as packed double-precision floating-point elements in dst. This intrinsic may generate the roundpd/vroundpd instruction.</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := TRUNCATE(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5483</id>
<name>_MM256_TRUNC_PS</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256</ret>
<sign>__M256 a</sign>
<instr>NONE</instr>
<desc>Truncate the packed single-precision (32-bit) floating-point elements in a, and store the results as packed single-precision floating-point elements in dst. This intrinsic may generate the roundps/vroundps instruction.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := TRUNCATE(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5505</id>
<name>_MM256_UDIV_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5507</id>
<name>_MM256_UDIVREM_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I_PTR mem_addr,__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, store the truncated results in dst, and store the remainders as packed unsigned 32-bit integers into memory at mem_addr.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
	MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5511</id>
<name>_MM256_UNDEFINED_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m256d with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5514</id>
<name>_MM256_UNDEFINED_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m256 with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5517</id>
<name>_MM256_UNDEFINED_SI256</name>
<cpuid>AVX</cpuid>
<ret>__m256i</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m256i with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5523</id>
<name>_MM256_UNPACKHI_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5532</id>
<name>_MM256_UNPACKHI_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5541</id>
<name>_MM256_UNPACKHI_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5550</id>
<name>_MM256_UNPACKHI_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5559</id>
<name>_MM256_UNPACKHI_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5571</id>
<name>_MM256_UNPACKHI_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5580</id>
<name>_MM256_UNPACKLO_EPI16</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5589</id>
<name>_MM256_UNPACKLO_EPI32</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5598</id>
<name>_MM256_UNPACKLO_EPI64</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5607</id>
<name>_MM256_UNPACKLO_EPI8</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5616</id>
<name>_MM256_UNPACKLO_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5628</id>
<name>_MM256_UNPACKLO_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5633</id>
<name>_MM256_UREM_EPI32</name>
<cpuid>AVX, SVML</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5661</id>
<name>_MM256_XOR_PD</name>
<cpuid>AVX</cpuid>
<ret>__m256d</ret>
<sign>__M256D a,__M256D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 3
	i := j*64
	dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5670</id>
<name>_MM256_XOR_PS</name>
<cpuid>AVX</cpuid>
<ret>__m256</ret>
<sign>__M256 a,__M256 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5675</id>
<name>_MM256_XOR_SI256</name>
<cpuid>AVX2</cpuid>
<ret>__m256i</ret>
<sign>__M256I a,__M256I b</sign>
<instr>VPXOR</instr>
<desc>Compute the bitwise XOR of 256 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[255:0] := (a[255:0] XOR b[255:0])
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>5692</id>
<name>_MM256_ZEROALL</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign></sign>
<instr>VZEROALL</instr>
<desc>Zero the contents of all XMM or YMM registers.</desc>
<oper>YMM0[MAX:0] := 0
YMM1[MAX:0] := 0
YMM2[MAX:0] := 0
YMM3[MAX:0] := 0
YMM4[MAX:0] := 0
YMM5[MAX:0] := 0
YMM6[MAX:0] := 0
YMM7[MAX:0] := 0
IF 64-bit mode
	YMM8[MAX:0] := 0
	YMM9[MAX:0] := 0
	YMM10[MAX:0] := 0
	YMM11[MAX:0] := 0
	YMM12[MAX:0] := 0
	YMM13[MAX:0] := 0
	YMM14[MAX:0] := 0
	YMM15[MAX:0] := 0
FI</oper>
</intrinsic>
<intrinsic>
<id>5693</id>
<name>_MM256_ZEROUPPER</name>
<cpuid>AVX</cpuid>
<ret>void</ret>
<sign></sign>
<instr>VZEROUPPER</instr>
<desc>Zero the upper 128 bits of all YMM registers; the lower 128-bits of the registers are unmodified.</desc>
<oper>YMM0[MAX:128] := 0
YMM1[MAX:128] := 0
YMM2[MAX:128] := 0
YMM3[MAX:128] := 0
YMM4[MAX:128] := 0
YMM5[MAX:128] := 0
YMM6[MAX:128] := 0
YMM7[MAX:128] := 0
IF 64-bit mode
	YMM8[MAX:128] := 0
	YMM9[MAX:128] := 0
	YMM10[MAX:128] := 0
	YMM11[MAX:128] := 0
	YMM12[MAX:128] := 0
	YMM13[MAX:128] := 0
	YMM14[MAX:128] := 0
	YMM15[MAX:128] := 0
FI</oper>
</intrinsic>
<intrinsic>
<id>6</id>
<name>_MM512_ABS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := ABS(a[i+15:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>15</id>
<name>_MM512_ABS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ABS(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>24</id>
<name>_MM512_ABS_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ABS(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>33</id>
<name>_MM512_ABS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst. </desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := ABS(a[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>36</id>
<name>_MM512_ABS_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v2</sign>
<instr>VPANDQ</instr>
<desc>Finds the absolute value of each packed double-precision (64-bit) floating-point element in v2, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ABS(v2[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>41</id>
<name>_MM512_ABS_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2</sign>
<instr>VPANDD</instr>
<desc>Finds the absolute value of each packed single-precision (32-bit) floating-point element in v2, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ABS(v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>45</id>
<name>_MM512_ACOS_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ACOS(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>49</id>
<name>_MM512_ACOS_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ACOS(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>53</id>
<name>_MM512_ACOSH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ACOSH(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>57</id>
<name>_MM512_ACOSH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ACOSH(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>59</id>
<name>_MM512_ADC_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k2,__M512I v3,__MMASK16_PTR k2_res</sign>
<instr>VPADCD</instr>
<desc>Performs element-by-element addition of packed 32-bit integers in v2 and v3 and the corresponding bit in k2, storing the result of the addition in dst and the result of the carry in k2_res.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k2_res[j]   := Carry(v2[i+31:i] + v3[i+31:i] + k2[j])
	dst[i+31:i] := v2[i+31:i] + v3[i+31:i] + k2[j]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>67</id>
<name>_MM512_ADD_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := a[i+15:i] + b[i+15:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>76</id>
<name>_MM512_ADD_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>85</id>
<name>_MM512_ADD_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>94</id>
<name>_MM512_ADD_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := a[i+7:i] + b[i+7:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>103</id>
<name>_MM512_ADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>115</id>
<name>_MM512_ADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>118</id>
<name>_MM512_ADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT rounding</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>121</id>
<name>_MM512_ADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT rounding</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] + b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>141</id>
<name>_MM512_ADDN_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v2,__M512D v3</sign>
<instr>VADDNPD</instr>
<desc>Performs element-by-element addition between packed double-precision (64-bit) floating-point elements in v2 and v3 and negates their sum, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>143</id>
<name>_MM512_ADDN_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2,__M512 v3</sign>
<instr>VADDNPS</instr>
<desc>Performs element-by-element addition between packed single-precision (32-bit) floating-point elements in v2 and v3 and negates their sum, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>145</id>
<name>_MM512_ADDN_ROUND_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v2,__M512D v3,INT rounding</sign>
<instr>VADDNPD</instr>
<desc>Performs element by element addition between packed double-precision (64-bit) floating-point elements in v2 and v3 and negates the sum, storing the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>147</id>
<name>_MM512_ADDN_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2,__M512 v3,INT rounding</sign>
<instr>VADDNPS</instr>
<desc>Performs element by element addition between packed single-precision (32-bit) floating-point elements in v2 and v3 and negates the sum, storing the result in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>155</id>
<name>_MM512_ADDS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>164</id>
<name>_MM512_ADDS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>173</id>
<name>_MM512_ADDS_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>182</id>
<name>_MM512_ADDS_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>189</id>
<name>_MM512_ADDSETC_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__M512I v3,__MMASK16_PTR k2_res</sign>
<instr>VPADDSETCD</instr>
<desc>Performs element-by-element addition of packed 32-bit integer elements in v2 and v3, storing the resultant carry in k2_res (carry flag) and the addition results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
	k2_res[j] := Carry(v2[i+31:i] + v3[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>191</id>
<name>_MM512_ADDSETS_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__M512I v3,__MMASK16_PTR sign</sign>
<instr>VPADDSETSD</instr>
<desc>Performs an element-by-element addition of packed 32-bit integer elements in v2 and v3, storing the results in dst and the sign of the sum in sign (sign flag).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
	sign[j] := v2[i+31:i] &amp; v3[i+31:i] &amp; 0x80000000
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>193</id>
<name>_MM512_ADDSETS_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2,__M512 v3,__MMASK16_PTR sign</sign>
<instr>VADDSETSPS</instr>
<desc>Performs an element-by-element addition of packed single-precision (32-bit) floating-point elements in v2 and v3, storing the results in dst and the sign of the sum in sign (sign flag).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
	sign[j] := v2[i+31:i] &amp; v3[i+31:i] &amp; 0x80000000
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>195</id>
<name>_MM512_ADDSETS_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2,__M512 v3,__MMASK16_PTR sign,INT rounding</sign>
<instr>VADDSETSPS</instr>
<desc>Performs an element-by-element addition of packed single-precision (32-bit) floating-point elements in v2 and v3, storing the results in dst and the sign of the sum in sign (sign flag).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
	sign[j] := v2[i+31:i] &amp; v3[i+31:i] &amp; 0x80000000
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>213</id>
<name>_MM512_ALIGNR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 128-byte immediate result, shift the result right by count 32-bit elements, and store the low 64 bytes (16 elements) in dst.</desc>
<oper>temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &amp;gt;&amp;gt; (32*count)
dst[511:0] := temp[511:0]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>222</id>
<name>_MM512_ALIGNR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 128-byte immediate result, shift the result right by count 64-bit elements, and store the low 64 bytes (8 elements) in dst.</desc>
<oper>temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &amp;gt;&amp;gt; (64*count)
dst[511:0] := temp[511:0]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>231</id>
<name>_MM512_ALIGNR_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst. </desc>
<oper>FOR j := 0 to 3
	i := j*128
	tmp[255:0] := ((a[i+127:i] &amp;lt;&amp;lt; 128) OR b[i+127:i]) &amp;gt;&amp;gt; (count[7:0]*8)
	dst[i+127:i] := tmp[127:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>240</id>
<name>_MM512_AND_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>247</id>
<name>_MM512_AND_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of 512 bits (composed of packed 64-bit integers) in a and b, and store the results in dst.</desc>
<oper>dst[511:0] := (a[511:0] AND b[511:0])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>256</id>
<name>_MM512_AND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>265</id>
<name>_MM512_AND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>270</id>
<name>_MM512_AND_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of 512 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[511:0] := (a[511:0] AND b[511:0])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>276</id>
<name>_MM512_ANDNOT_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>283</id>
<name>_MM512_ANDNOT_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of 512 bits (composed of packed 64-bit integers) in a and then AND with b, and store the results in dst.</desc>
<oper>dst[511:0] := ((NOT a[511:0]) AND b[511:0])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>292</id>
<name>_MM512_ANDNOT_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>301</id>
<name>_MM512_ANDNOT_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>306</id>
<name>_MM512_ANDNOT_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of 512 bits (representing integer data) in a and then AND with b, and store the result in dst.</desc>
<oper>dst[511:0] := ((NOT a[511:0]) AND b[511:0])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>310</id>
<name>_MM512_ASIN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ASIN(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>314</id>
<name>_MM512_ASIN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ASIN(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>318</id>
<name>_MM512_ASINH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ASINH(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>322</id>
<name>_MM512_ASINH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ASINH(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>326</id>
<name>_MM512_ATAN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ATAN(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>330</id>
<name>_MM512_ATAN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ATAN(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>334</id>
<name>_MM512_ATAN2_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>338</id>
<name>_MM512_ATAN2_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>342</id>
<name>_MM512_ATANH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ATANH(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>346</id>
<name>_MM512_ATANH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperblic tangent of packed single-precision (32-bit) floating-point elements in a, and store the results in dst expressed in radians.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ATANH(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>354</id>
<name>_MM512_AVG_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>363</id>
<name>_MM512_AVG_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>435</id>
<name>_MM512_BROADCAST_F32X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTF32X2</instr>
<desc>Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 2)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>441</id>
<name>_MM512_BROADCAST_F32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTF32X4</instr>
<desc>Broadcast the 4 packed single-precision (32-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 4)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>444</id>
<name>_MM512_BROADCAST_F32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M256 a</sign>
<instr>VBROADCASTF32X8</instr>
<desc>Broadcast the 8 packed single-precision (32-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 8)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>450</id>
<name>_MM512_BROADCAST_F64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M128D a</sign>
<instr>VBROADCASTF64X2</instr>
<desc>Broadcast the 2 packed double-precision (64-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 2)*64
	dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>453</id>
<name>_MM512_BROADCAST_F64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256D a</sign>
<instr>VBROADCASTF64X4</instr>
<desc>Broadcast the 4 packed double-precision (64-bit) floating-point elements from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 4)*64
	dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>462</id>
<name>_MM512_BROADCAST_I32X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of "dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 2)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>468</id>
<name>_MM512_BROADCAST_I32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI32X4</instr>
<desc>Broadcast the 4 packed 32-bit integers from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 4)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>471</id>
<name>_MM512_BROADCAST_I32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VBROADCASTI32X8</instr>
<desc>Broadcast the 8 packed 32-bit integers from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 8)*32
	dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>477</id>
<name>_MM512_BROADCAST_I64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VBROADCASTI64X2</instr>
<desc>Broadcast the 2 packed 64-bit integers from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 2)*64
	dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>480</id>
<name>_MM512_BROADCAST_I64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VBROADCASTI64X4</instr>
<desc>Broadcast the 4 packed 64-bit integers from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 4)*64
	dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>494</id>
<name>_MM512_BROADCASTB_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>503</id>
<name>_MM512_BROADCASTD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>508</id>
<name>_MM512_BROADCASTMB_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k</sign>
<instr>VPBROADCASTMB2Q</instr>
<desc>Broadcast the low 8-bits from input mask k to all 64-bit elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ZeroExtend(k[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>511</id>
<name>_MM512_BROADCASTMW_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k</sign>
<instr>VPBROADCASTMW2D</instr>
<desc>Broadcast the low 16-bits from input mask k to all 32-bit elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ZeroExtend(k[15:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>518</id>
<name>_MM512_BROADCASTQ_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>525</id>
<name>_MM512_BROADCASTSD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M128D a</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>535</id>
<name>_MM512_BROADCASTSS_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>544</id>
<name>_MM512_BROADCASTW_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>548</id>
<name>_MM512_BSLLI_EPI128</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VPSLLDQ</instr>
<desc>Shift 128-bit lanes in a left by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;lt;&amp;lt; (tmp*8)
dst[255:128] := a[255:128] &amp;lt;&amp;lt; (tmp*8)
dst[383:256] := a[383:256] &amp;lt;&amp;lt; (tmp*8)
dst[511:384] := a[511:384] &amp;lt;&amp;lt; (tmp*8)
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>551</id>
<name>_MM512_BSRLI_EPI128</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VPSRLDQ</instr>
<desc>Shift 128-bit lanes in a right by imm8 bytes while shifting in zeros, and store the results in dst.</desc>
<oper>tmp := imm8[7:0]
IF tmp &amp;gt; 15
	tmp := 16
FI
dst[127:0] := a[127:0] &amp;gt;&amp;gt; (tmp*8)
dst[255:128] := a[255:128] &amp;gt;&amp;gt; (tmp*8)
dst[383:256] := a[383:256] &amp;gt;&amp;gt; (tmp*8)
dst[511:384] := a[511:384] &amp;gt;&amp;gt; (tmp*8)
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>561</id>
<name>_MM512_CASTPD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512D a</sign>
<instr></instr>
<desc>Cast vector of type __m512d to type __m512.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>564</id>
<name>_MM512_CASTPD_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512D a</sign>
<instr></instr>
<desc>Cast vector of type __m512d to type __m512i.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>566</id>
<name>_MM512_CASTPD128_PD512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M128D a</sign>
<instr></instr>
<desc>Cast vector of type __m128d to type __m512d; the upper 384 bits of the result are undefined. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>568</id>
<name>_MM512_CASTPD256_PD512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256D a</sign>
<instr></instr>
<desc>Cast vector of type __m256d to type __m512d; the upper 256 bits of the result are undefined. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>569</id>
<name>_MM512_CASTPD512_PD128</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512D a</sign>
<instr></instr>
<desc>Cast vector of type __m512d to type __m128d. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>570</id>
<name>_MM512_CASTPD512_PD256</name>
<cpuid>AVX512F</cpuid>
<ret>__m256d</ret>
<sign>__M512D a</sign>
<instr></instr>
<desc>Cast vector of type __m512d to type __m256d. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>573</id>
<name>_MM512_CASTPS_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512 a</sign>
<instr></instr>
<desc>Cast vector of type __m512 to type __m512d.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>576</id>
<name>_MM512_CASTPS_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512 a</sign>
<instr></instr>
<desc>Cast vector of type __m512 to type __m512i.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>578</id>
<name>_MM512_CASTPS128_PS512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M128 a</sign>
<instr></instr>
<desc>Cast vector of type __m128 to type __m512; the upper 384 bits of the result are undefined. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>580</id>
<name>_MM512_CASTPS256_PS512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M256 a</sign>
<instr></instr>
<desc>Cast vector of type __m256 to type __m512; the upper 256 bits of the result are undefined. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>581</id>
<name>_MM512_CASTPS512_PS128</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M512 a</sign>
<instr></instr>
<desc>Cast vector of type __m512 to type __m128. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>582</id>
<name>_MM512_CASTPS512_PS256</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M512 a</sign>
<instr></instr>
<desc>Cast vector of type __m512 to type __m256. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>586</id>
<name>_MM512_CASTSI128_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr></instr>
<desc>Cast vector of type __m128i to type __m512i; the upper 384 bits of the result are undefined. 
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>590</id>
<name>_MM512_CASTSI256_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr></instr>
<desc>Cast vector of type __m256i to type __m512i; the upper 256 bits of the result are undefined.
	 This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>591</id>
<name>_MM512_CASTSI512_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512I a</sign>
<instr></instr>
<desc>Cast vector of type __m512i to type __m512d.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>592</id>
<name>_MM512_CASTSI512_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512I a</sign>
<instr></instr>
<desc>Cast vector of type __m512i to type __m512.
	This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>593</id>
<name>_MM512_CASTSI512_SI128</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr></instr>
<desc>Cast vector of type __m512i to type __m128i.
	 This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>594</id>
<name>_MM512_CASTSI512_SI256</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr></instr>
<desc>Cast vector of type __m512i to type __m256i.
	 This intrinsic is only used for compilation and does not generate any instructions, thus it has zero latency.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>599</id>
<name>_MM512_CBRT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := CubeRoot(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>603</id>
<name>_MM512_CBRT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := CubeRoot(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>607</id>
<name>_MM512_CDFNORM_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := CDFNormal(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>611</id>
<name>_MM512_CDFNORM_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := CDFNormal(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>615</id>
<name>_MM512_CDFNORMINV_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := InverseCDFNormal(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>619</id>
<name>_MM512_CDFNORMINV_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := InverseCDFNormal(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>623</id>
<name>_MM512_CEIL_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a up to an integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := CEIL(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>627</id>
<name>_MM512_CEIL_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a up to an integer value, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := CEIL(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>643</id>
<name>_MM512_CMP_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>649</id>
<name>_MM512_CMP_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>655</id>
<name>_MM512_CMP_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>661</id>
<name>_MM512_CMP_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>667</id>
<name>_MM512_CMP_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>673</id>
<name>_MM512_CMP_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>679</id>
<name>_MM512_CMP_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>685</id>
<name>_MM512_CMP_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>693</id>
<name>_MM512_CMP_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>701</id>
<name>_MM512_CMP_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>703</id>
<name>_MM512_CMP_ROUND_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>705</id>
<name>_MM512_CMP_ROUND_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>723</id>
<name>_MM512_CMPEQ_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>731</id>
<name>_MM512_CMPEQ_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPEQD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>739</id>
<name>_MM512_CMPEQ_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPEQQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>747</id>
<name>_MM512_CMPEQ_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>753</id>
<name>_MM512_CMPEQ_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>759</id>
<name>_MM512_CMPEQ_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>765</id>
<name>_MM512_CMPEQ_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>771</id>
<name>_MM512_CMPEQ_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>774</id>
<name>_MM512_CMPEQ_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] == b[i+63:i]) ? 1 : 0
ENDFOR	
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>780</id>
<name>_MM512_CMPEQ_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for equality, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] == b[i+31:i]) ? 1 : 0
ENDFOR	
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>795</id>
<name>_MM512_CMPGE_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>801</id>
<name>_MM512_CMPGE_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>807</id>
<name>_MM512_CMPGE_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>813</id>
<name>_MM512_CMPGE_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>819</id>
<name>_MM512_CMPGE_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>825</id>
<name>_MM512_CMPGE_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>831</id>
<name>_MM512_CMPGE_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>837</id>
<name>_MM512_CMPGE_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>849</id>
<name>_MM512_CMPGT_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>857</id>
<name>_MM512_CMPGT_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>865</id>
<name>_MM512_CMPGT_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPGTQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>873</id>
<name>_MM512_CMPGT_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>879</id>
<name>_MM512_CMPGT_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>885</id>
<name>_MM512_CMPGT_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>891</id>
<name>_MM512_CMPGT_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>897</id>
<name>_MM512_CMPGT_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>917</id>
<name>_MM512_CMPLE_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>923</id>
<name>_MM512_CMPLE_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>929</id>
<name>_MM512_CMPLE_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>935</id>
<name>_MM512_CMPLE_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>941</id>
<name>_MM512_CMPLE_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>947</id>
<name>_MM512_CMPLE_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>953</id>
<name>_MM512_CMPLE_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>959</id>
<name>_MM512_CMPLE_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>962</id>
<name>_MM512_CMPLE_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] &amp;lt;= b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>965</id>
<name>_MM512_CMPLE_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] &amp;lt;= b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>974</id>
<name>_MM512_CMPLT_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>981</id>
<name>_MM512_CMPLT_EPI32_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>982</id>
<name>_MM512_CMPLT_EPI32_MASK</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPLTD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>989</id>
<name>_MM512_CMPLT_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>996</id>
<name>_MM512_CMPLT_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1002</id>
<name>_MM512_CMPLT_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1008</id>
<name>_MM512_CMPLT_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1014</id>
<name>_MM512_CMPLT_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1020</id>
<name>_MM512_CMPLT_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1023</id>
<name>_MM512_CMPLT_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] &amp;lt; b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1026</id>
<name>_MM512_CMPLT_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] &amp;lt; b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1034</id>
<name>_MM512_CMPNEQ_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1040</id>
<name>_MM512_CMPNEQ_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1046</id>
<name>_MM512_CMPNEQ_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1052</id>
<name>_MM512_CMPNEQ_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1058</id>
<name>_MM512_CMPNEQ_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1064</id>
<name>_MM512_CMPNEQ_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1070</id>
<name>_MM512_CMPNEQ_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1076</id>
<name>_MM512_CMPNEQ_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1079</id>
<name>_MM512_CMPNEQ_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] != b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1082</id>
<name>_MM512_CMPNEQ_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] != b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1095</id>
<name>_MM512_CMPNLE_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := !(a[i+63:i] &amp;lt;= b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1098</id>
<name>_MM512_CMPNLE_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-less-than-or-equal, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := !(a[i+31:i] &amp;lt;= b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1103</id>
<name>_MM512_CMPNLT_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := !(a[i+63:i] &amp;lt; b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1106</id>
<name>_MM512_CMPNLT_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-less-than, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := !(a[i+31:i] &amp;lt; b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1111</id>
<name>_MM512_CMPORD_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b to see if neither is NaN, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] != NaN AND b[i+63:i] != NaN) ? 1 : 0 
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1114</id>
<name>_MM512_CMPORD_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b to see if neither is NaN, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] != NaN AND b[i+31:i] != NaN) ? 1 : 0 
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1119</id>
<name>_MM512_CMPUNORD_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b to see if either is NaN, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := (a[i+63:i] == NaN OR b[i+63:i] == NaN) ? 1 : 0 
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1122</id>
<name>_MM512_CMPUNORD_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b to see if either is NaN, and store the results in mask vector k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := (a[i+31:i] == NaN OR b[i+31:i] == NaN) ? 1 : 0 
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1182</id>
<name>_MM512_CONFLICT_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit. Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	FOR k := 0 to j-1
		m := k*32
		dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
	ENDFOR
	dst[i+31:i+j] := 0
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1191</id>
<name>_MM512_CONFLICT_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit. Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	FOR k := 0 to j-1
		m := k*64
		dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
	ENDFOR
	dst[i+63:i+j] := 0
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1196</id>
<name>_MM512_COS_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := COS(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1200</id>
<name>_MM512_COS_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := COS(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1204</id>
<name>_MM512_COSD_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := COSD(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1208</id>
<name>_MM512_COSD_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := COSD(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1212</id>
<name>_MM512_COSH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := COSH(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1216</id>
<name>_MM512_COSH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := COSH(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1228</id>
<name>_MM512_CVT_ROUNDEPI32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512I a,INT rounding</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1231</id>
<name>_MM512_CVT_ROUNDEPI64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512I a,INT rounding</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1234</id>
<name>_MM512_CVT_ROUNDEPI64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M512I a,INT rounding</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1237</id>
<name>_MM512_CVT_ROUNDEPU32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512I a,INT rounding</sign>
<instr>VCVTUDQ2PS</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1240</id>
<name>_MM512_CVT_ROUNDEPU64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512I a,INT rounding</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1243</id>
<name>_MM512_CVT_ROUNDEPU64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M512I a,INT rounding</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1249</id>
<name>_MM512_CVT_ROUNDPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1252</id>
<name>_MM512_CVT_ROUNDPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1255</id>
<name>_MM512_CVT_ROUNDPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1258</id>
<name>_MM512_CVT_ROUNDPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1261</id>
<name>_MM512_CVT_ROUNDPD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1264</id>
<name>_MM512_CVT_ROUNDPD_PSLO</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512D v2,INT rounding</sign>
<instr>VCVTPD2PS</instr>
<desc>Performs element-by-element conversion of packed double-precision (64-bit) floating-point elements in v2 to packed single-precision (32-bit) floating-point elements, storing the results in dst. Results are written to the lower half of dst, and the upper half locations are set to '0'.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k := j*32
	dst[k+31:k] := Float64ToFloat32(v2[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1266</id>
<name>_MM512_CVT_ROUNDPH_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M256I a,INT sae</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	m := j*16
	dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1269</id>
<name>_MM512_CVT_ROUNDPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1272</id>
<name>_MM512_CVT_ROUNDPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a,INT rounding</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1275</id>
<name>_MM512_CVT_ROUNDPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1278</id>
<name>_MM512_CVT_ROUNDPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a,INT rounding</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1281</id>
<name>_MM512_CVT_ROUNDPS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256 a,INT sae</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1288</id>
<name>_MM512_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 32*j
	dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1323</id>
<name>_MM512_CVTEPI16_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 16*j
	dst[i+31:i] := SignExtend(a[k+15:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1332</id>
<name>_MM512_CVTEPI16_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 16*j
	dst[i+63:i] := SignExtend(a[k+15:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1341</id>
<name>_MM512_CVTEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1353</id>
<name>_MM512_CVTEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 16*j
	dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1362</id>
<name>_MM512_CVTEPI32_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[i+63:i] := SignExtend(a[k+31:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1371</id>
<name>_MM512_CVTEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 8*j
	dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1380</id>
<name>_MM512_CVTEPI32_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1389</id>
<name>_MM512_CVTEPI32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1398</id>
<name>_MM512_CVTEPI32LO_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512I v2</sign>
<instr>VCVTDQ2PD</instr>
<desc>Performs element-by-element conversion of the lower half of packed 32-bit integer elements in v2 to packed double-precision (64-bit) floating-point elements, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	dst[l+63:l] := Int32ToFloat64(v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1406</id>
<name>_MM512_CVTEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 16*j
	dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1415</id>
<name>_MM512_CVTEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1424</id>
<name>_MM512_CVTEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 8*j
	dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1433</id>
<name>_MM512_CVTEPI64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1442</id>
<name>_MM512_CVTEPI64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M512I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1460</id>
<name>_MM512_CVTEPI8_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	l := j*16
	dst[l+15:l] := SignExtend(a[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1469</id>
<name>_MM512_CVTEPI8_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 8*j
	dst[i+31:i] := SignExtend(a[k+7:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1478</id>
<name>_MM512_CVTEPI8_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 8*j
	dst[i+63:i] := SignExtend(a[k+7:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1487</id>
<name>_MM512_CVTEPU16_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 16*j
	dst[i+31:i] := ZeroExtend(a[k+15:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1496</id>
<name>_MM512_CVTEPU16_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 16*j
	dst[i+63:i] := ZeroExtend(a[k+15:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1505</id>
<name>_MM512_CVTEPU32_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[i+63:i] := ZeroExtend(a[k+31:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1514</id>
<name>_MM512_CVTEPU32_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1517</id>
<name>_MM512_CVTEPU32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512I a</sign>
<instr>VCVTUDQ2PS</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1520</id>
<name>_MM512_CVTEPU32LO_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512I v2</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Performs element-by-element conversion of the lower half of packed 32-bit unsigned integer elements in v2 to packed double-precision (64-bit) floating-point elements, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k := j*64
	dst[k+63:k] := UInt32ToFloat64(v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1528</id>
<name>_MM512_CVTEPU64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1537</id>
<name>_MM512_CVTEPU64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M512I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1546</id>
<name>_MM512_CVTEPU8_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M256I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*8
	l := j*16
	dst[l+15:l] := ZeroExtend(a[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1555</id>
<name>_MM512_CVTEPU8_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 8*j
	dst[i+31:i] := ZeroExtend(a[k+7:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1564</id>
<name>_MM512_CVTEPU8_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 byte sof a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 8*j
	dst[i+63:i] := ZeroExtend(a[k+7:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1567</id>
<name>_MM512_CVTFXPNT_ROUND_ADJUSTEPI32_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512I v2,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VCVTFXPNTDQ2PS</instr>
<desc>Performs element-by-element conversion of packed 32-bit integer elements in v2 to packed single-precision (32-bit) floating-point elements and performing an optional exponent adjust using expadj, storing the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := Int32ToFloat32(v2[i+31:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
	_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
	_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
	_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1568</id>
<name>_MM512_CVTFXPNT_ROUND_ADJUSTEPU32_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512I v2,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VCVTFXPNTUDQ2PS</instr>
<desc>Performs element-by-element conversion of packed 32-bit unsigned integer elements in v2 to packed single-precision (32-bit) floating-point elements and performing an optional exponent adjust using expadj, storing the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := UInt32ToFloat32(v2[i+31:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
	_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i] * 2**4
	_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i] * 2**5
	_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i] * 2**8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1570</id>
<name>_MM512_CVTFXPNT_ROUND_ADJUSTPS_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512 v2,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VCVTFXPNTPS2DQ</instr>
<desc>Performs element-by-element conversion of packed single-precision (32-bit) floating-point elements in v2 to packed 32-bit integer elements and performs an optional exponent adjust using expadj, storing the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := Float32ToInt32(v2[i+31:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
	_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i] * 2**4
	_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i] * 2**5
	_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i] * 2**8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1571</id>
<name>_MM512_CVTFXPNT_ROUND_ADJUSTPS_EPU32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512 v2,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VCVTFXPNTPS2UDQ</instr>
<desc>Performs element-by-element conversion of packed single-precision (32-bit) floating-point elements in v2 to packed 32-bit unsigned integer elements and performing an optional exponent adjust using expadj, storing the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := Float32ToUInt32(v2[i+31:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i]  0
	_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i]  4
	_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i]  5
	_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i]  8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i]  16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i]  24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i]  31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i]  32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1572</id>
<name>_MM512_CVTFXPNT_ROUNDPD_EPI32LO</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512D v2,INT rounding</sign>
<instr>VCVTFXPNTPD2DQ</instr>
<desc>Performs an element-by-element conversion of elements in packed double-precision (64-bit) floating-point vector v2 to 32-bit integer elements, storing them in the lower half of dst. The elements in the upper half of dst are set to 0.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k := j*32
	dst[k+31:k] := Float64ToInt32(v2[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1574</id>
<name>_MM512_CVTFXPNT_ROUNDPD_EPU32LO</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512D v2,INT rounding</sign>
<instr>VCVTFXPNTPD2UDQ</instr>
<desc>Performs element-by-element conversion of packed double-precision (64-bit) floating-point elements in v2 to packed 32-bit unsigned integer elements, storing the results in dst. Results are written to the lower half of dst, and the upper half locations are set to '0'.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k := j*32
	dst[k+31:k] := Float64ToInt32(v2[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1587</id>
<name>_MM512_CVTPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1596</id>
<name>_MM512_CVTPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1605</id>
<name>_MM512_CVTPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1614</id>
<name>_MM512_CVTPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1624</id>
<name>_MM512_CVTPD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M512D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1627</id>
<name>_MM512_CVTPD_PSLO</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512D v2</sign>
<instr>VCVTPD2PS</instr>
<desc>Performs an element-by-element conversion of packed double-precision (64-bit) floating-point elements in v2 to single-precision (32-bit) floating-point elements and stores them in dst. The elements are stored in the lower half of the results vector, while the remaining upper half locations are set to 0.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k := j*32
	dst[k+31:k] := Float64ToFloat32(v2[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1635</id>
<name>_MM512_CVTPH_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M256I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	m := j*16
	dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1649</id>
<name>_MM512_CVTPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1658</id>
<name>_MM512_CVTPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1667</id>
<name>_MM512_CVTPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1676</id>
<name>_MM512_CVTPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1681</id>
<name>_MM512_CVTPS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256 a</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1690</id>
<name>_MM512_CVTPS_PH</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 32*j
	dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1696</id>
<name>_MM512_CVTPSLO_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512 v2</sign>
<instr>VCVTPS2PD</instr>
<desc>Performs element-by-element conversion of the lower half of packed single-precision (32-bit) floating-point elements in v2 to packed double-precision (64-bit) floating-point elements, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	k := j*64
	dst[k+63:k] := Float32ToFloat64(v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1717</id>
<name>_MM512_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1729</id>
<name>_MM512_CVTSEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 16*j
	dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1738</id>
<name>_MM512_CVTSEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 8*j
	dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1753</id>
<name>_MM512_CVTSEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 16*j
	dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1762</id>
<name>_MM512_CVTSEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1771</id>
<name>_MM512_CVTSEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 8*j
	dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1810</id>
<name>_MM512_CVTT_ROUNDPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a,INT sae</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1813</id>
<name>_MM512_CVTT_ROUNDPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a,INT sae</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst. Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1816</id>
<name>_MM512_CVTT_ROUNDPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a,INT sae</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1819</id>
<name>_MM512_CVTT_ROUNDPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a,INT sae</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst. Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1822</id>
<name>_MM512_CVTT_ROUNDPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a,INT sae</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := 32*i
	dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1825</id>
<name>_MM512_CVTT_ROUNDPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a,INT sae</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst. Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1828</id>
<name>_MM512_CVTT_ROUNDPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a,INT sae</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := 32*i
	dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1831</id>
<name>_MM512_CVTT_ROUNDPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a,INT sae</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst. Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1853</id>
<name>_MM512_CVTTPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1862</id>
<name>_MM512_CVTTPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1871</id>
<name>_MM512_CVTTPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	k := 64*j
	dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1880</id>
<name>_MM512_CVTTPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1890</id>
<name>_MM512_CVTTPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1899</id>
<name>_MM512_CVTTPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1908</id>
<name>_MM512_CVTTPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1917</id>
<name>_MM512_CVTTPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M256 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1944</id>
<name>_MM512_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1956</id>
<name>_MM512_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 16*j
	dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1965</id>
<name>_MM512_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	k := 8*j
	dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1980</id>
<name>_MM512_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 16*j
	dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1989</id>
<name>_MM512_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 32*j
	dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1998</id>
<name>_MM512_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	k := 8*j
	dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2016</id>
<name>_MM512_DBSAD_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst.
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected from within 128-bit lanes according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>FOR j := 0 to 3
	i := j*128
	tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
	tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
	tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
	tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
ENDFOR

FOR j := 0 to 7
	i := j*64
	dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2023</id>
<name>_MM512_DIV_EPI16</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 16-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2026</id>
<name>_MM512_DIV_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2030</id>
<name>_MM512_DIV_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 64-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2033</id>
<name>_MM512_DIV_EPI8</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 8-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 63
	i := 8*j
	dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2036</id>
<name>_MM512_DIV_EPU16</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 16-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2039</id>
<name>_MM512_DIV_EPU32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2043</id>
<name>_MM512_DIV_EPU64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 64-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2046</id>
<name>_MM512_DIV_EPU8</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 8-bit integers in a by packed elements in b, and store the truncated results in dst.</desc>
<oper>FOR j := 0 to 63
	i := 8*j
	dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2053</id>
<name>_MM512_DIV_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	dst[i+63:i] := a[i+63:i] / b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2062</id>
<name>_MM512_DIV_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := a[i+31:i] / b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2065</id>
<name>_MM512_DIV_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT rounding</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, =and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	dst[i+63:i] := a[i+63:i] / b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2068</id>
<name>_MM512_DIV_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT rounding</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := a[i+31:i] / b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2090</id>
<name>_MM512_ERF_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ERF(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2094</id>
<name>_MM512_ERF_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ERF(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2098</id>
<name>_MM512_ERFC_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := 1.0 - ERF(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2102</id>
<name>_MM512_ERFC_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := 1.0 - ERF(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2106</id>
<name>_MM512_ERFCINV_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2110</id>
<name>_MM512_ERFCINV_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2114</id>
<name>_MM512_ERFINV_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := 1.0 / ERF(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2118</id>
<name>_MM512_ERFINV_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := 1.0 / ERF(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2122</id>
<name>_MM512_EXP_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := e^(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2126</id>
<name>_MM512_EXP_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := e^(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2130</id>
<name>_MM512_EXP10_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := 10^(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2134</id>
<name>_MM512_EXP10_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := 10^(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2138</id>
<name>_MM512_EXP2_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := 2^(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2142</id>
<name>_MM512_EXP2_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := 2^(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2144</id>
<name>_MM512_EXP223_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512I v2</sign>
<instr>VEXP223PS</instr>
<desc>Approximates the base-2 exponent of the packed single-precision (32-bit) floating-point elements in v2 with eight bits for sign and magnitude and 24 bits for the fractional part. Results are stored in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := exp223(v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2146</id>
<name>_MM512_EXP2A23_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VEXP2PD</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-23.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2149</id>
<name>_MM512_EXP2A23_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VEXP2PS</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-23.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2152</id>
<name>_MM512_EXP2A23_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VEXP2PD</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-23. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2155</id>
<name>_MM512_EXP2A23_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VEXP2PS</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-23. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2208</id>
<name>_MM512_EXPM1_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, subtract one from each element, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := e^(a[i+63:i]) - 1.0
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2212</id>
<name>_MM512_EXPM1_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, subtract one from each element, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := e^(a[i+31:i]) - 1.0
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2214</id>
<name>_MM512_EXTLOAD_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mt,_MM_UPCONV_EPI32_ENUM conv,_MM_BROADCAST32_ENUM bc,INT hint</sign>
<instr>VMOVDQA32, VBROADCASTI32X4, VPBROADCASTD</instr>
<desc>Depending on bc, loads 1, 4, or 16 elements of type and size determined by conv from memory address mt and converts all elements to 32-bit integer elements, storing the results in dst. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 15
	i := j*32
	CASE bc OF
	_MM_BROADCAST32_NONE:
		CASE conv OF
		_MM_UPCONV_EPI32_NONE:
			n	 := j*32
			dst[i+31:i] := addr[n+31:n]
		_MM_UPCONV_EPI32_UINT8:
			n	 := j*8
			dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_SINT8:
			n	 := j*8
			dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_UINT16:
			n	 := j*16
			dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
		_MM_UPCONV_EPI32_SINT16:
			n	 := j*16
			dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
		ESAC
	_MM_BROADCAST_1X16:
		CASE conv OF
		_MM_UPCONV_EPI32_NONE:
			n	 := j*32
			dst[i+31:i] := addr[31:0]
		_MM_UPCONV_EPI32_UINT8:
			n	 := j*8
			dst[i+31:i] := UInt8ToInt32(addr[7:0])
		_MM_UPCONV_EPI32_SINT8:
			n	 := j*8
			dst[i+31:i] := SInt8ToInt32(addr[7:0])
		_MM_UPCONV_EPI32_UINT16:
			n	 := j*16
			dst[i+31:i] := UInt16ToInt32(addr[15:0])
		_MM_UPCONV_EPI32_SINT16:
			n	 := j*16
			dst[i+31:i] := SInt16ToInt32(addr[15:0])
		ESAC
	_MM_BROADCAST_4X16:
		mod := j%4
		CASE conv OF
		_MM_UPCONV_EPI32_NONE:
			n := mod*32
			dst[i+31:i] := addr[n+31:n]
		_MM_UPCONV_EPI32_UINT8:
			n := mod*8
			dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_SINT8:
			n := mod*8
			dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_UINT16:
			n := mod*16
			dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
		_MM_UPCONV_EPI32_SINT16:
			n := mod*16
			dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
		ESAC
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2216</id>
<name>_MM512_EXTLOAD_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mt,_MM_UPCONV_EPI64_ENUM conv,_MM_BROADCAST64_ENUM bc,INT hint</sign>
<instr>VMOVDQA64, VBROADCASTI64X4, VPBROADCASTQ</instr>
<desc>Depending on bc, loads 1, 4, or 8 elements of type and size determined by conv from memory address mt and converts all elements to 64-bit integer elements, storing the results in dst. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 7
	i := j*64
	CASE bc OF
	_MM_BROADCAST64_NONE:
		CASE conv OF
		_MM_UPCONV_EPI64_NONE:
			n := j*64
			dst[i+63:i] := addr[n+63:n]
		ESAC
	_MM_BROADCAST_1X8:
		CASE conv OF
		_MM_UPCONV_EPI64_NONE:
			n := j*64
			dst[i+63:i] := addr[63:0]
		ESAC
	_MM_BROADCAST_4X8:
		mod := j%4
		CASE conv OF
		_MM_UPCONV_EPI64_NONE:
			n := mod*64
			dst[i+63:i] := addr[n+63:n]
		ESAC
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2218</id>
<name>_MM512_EXTLOAD_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>VOID_CONST_PTR mt,_MM_UPCONV_PD_ENUM conv,_MM_BROADCAST64_ENUM bc,INT hint</sign>
<instr>VMOVAPD, VBROADCASTF64X4, VBROADCASTSD</instr>
<desc>Depending on bc, loads 1, 4, or 8 elements of type and size determined by conv from memory address mt and converts all elements to double-precision (64-bit) floating-point elements, storing the results in dst. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 7
	i := j*64
	CASE bc OF
	_MM_BROADCAST64_NONE:
		CASE conv OF
		_MM_UPCONV_PD_NONE:
			n := j*64
			dst[i+63:i] := addr[n+63:n]
		ESAC
	_MM_BROADCAST_1X8:
		CASE conv OF
		_MM_UPCONV_PD_NONE:
			n := j*64
			dst[i+63:i] := addr[63:0]
		ESAC
	_MM_BROADCAST_4X8:
		mod := j%4
		CASE conv OF
		_MM_UPCONV_PD_NONE:
			n := mod*64
			dst[i+63:i] := addr[n+63:n]
		ESAC
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2220</id>
<name>_MM512_EXTLOAD_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>VOID_CONST_PTR mt,_MM_UPCONV_PS_ENUM conv,_MM_BROADCAST32_ENUM bc,INT hint</sign>
<instr>VMOVAPS, VBROADCASTF32X4, VBROADCASTSS</instr>
<desc>Depending on bc, loads 1, 4, or 16 elements of type and size determined by conv from memory address mt and converts all elements to single-precision (32-bit) floating-point elements, storing the results in dst. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 15
	i := j*32
	CASE bc OF
	_MM_BROADCAST32_NONE:
		CASE conv OF
		_MM_UPCONV_PS_NONE:
			n	 := j*32
			dst[i+31:i] := addr[n+31:n]
		_MM_UPCONV_PS_FLOAT16:
			n	 := j*16
			dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_UINT8:
			n	 := j*8
			dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_SINT8:
			n	 := j*8
			dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_UINT16:
			n	 := j*16
			dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_SINT16:
			n	 := j*16
			dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
		ESAC
	_MM_BROADCAST_1X16:
		CASE conv OF
		_MM_UPCONV_PS_NONE:
			n	 := j*32
			dst[i+31:i] := addr[31:0]
		_MM_UPCONV_PS_FLOAT16:
			n	 := j*16
			dst[i+31:i] := Float16ToFloat32(addr[15:0])
		_MM_UPCONV_PS_UINT8:
			n	 := j*8
			dst[i+31:i] := UInt8ToFloat32(addr[7:0])
		_MM_UPCONV_PS_SINT8:
			n	 := j*8
			dst[i+31:i] := SInt8ToFloat32(addr[7:0])
		_MM_UPCONV_PS_UINT16:
			n	 := j*16
			dst[i+31:i] := UInt16ToFloat32(addr[15:0])
		_MM_UPCONV_PS_SINT16:
			n	 := j*16
			dst[i+31:i] := SInt16ToFloat32(addr[15:0])
		ESAC
	_MM_BROADCAST_4X16:
		mod := j%4
		CASE conv OF
		_MM_UPCONV_PS_NONE:
			n := mod*32
			dst[i+31:i] := addr[n+31:n]
		_MM_UPCONV_PS_FLOAT16:
			n := mod*16
			dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_UINT8:
			n := mod*8
			dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_SINT8:
			n := mod*8
			dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_UINT16:
			n := mod*16
			dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_SINT16:
			n := mod*16
			dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
		ESAC
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2222</id>
<name>_MM512_EXTLOADUNPACKHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt,_MM_UPCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHD</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*upSize % 64) == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*32
		dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2224</id>
<name>_MM512_EXTLOADUNPACKHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt,_MM_UPCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHQ</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*upSize) == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*64
		dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2226</id>
<name>_MM512_EXTLOADUNPACKHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,VOID_CONST_PTR mt,_MM_UPCONV_PD_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHPD</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed double-precision (64-bit) floating-point values in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE: RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE: RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*upSize) % 64 == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*64
		dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2228</id>
<name>_MM512_EXTLOADUNPACKHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,VOID_CONST_PTR mt,_MM_UPCONV_PS_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHPS</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
	_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
	_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*upSize % 64) == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*32
		dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2230</id>
<name>_MM512_EXTLOADUNPACKLO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt,_MM_UPCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLD</instr>
<desc>Loads the low-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
	loadOffset := loadOffset + 1
	IF (mt + loadOffset * upSize) % 64 == 0
		break
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2232</id>
<name>_MM512_EXTLOADUNPACKLO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt,_MM_UPCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLQ</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
	loadOffset := loadOffset + 1
	IF (addr + loadOffset*upSize % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2234</id>
<name>_MM512_EXTLOADUNPACKLO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,VOID_CONST_PTR mt,_MM_UPCONV_PD_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLPD</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed double-precision (64-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE:	RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE:	RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
	loadOffset := loadOffset + 1
	IF (mt + loadOffset * upSize) % 64 == 0
		break
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2236</id>
<name>_MM512_EXTLOADUNPACKLO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,VOID_CONST_PTR mt,_MM_UPCONV_PS_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLPS</instr>
<desc>Loads the low-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. hint indicates to the processor whether the loaded data is non-temporal.</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
	_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
	_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = MEM[mt]
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
	loadOffset := loadOffset + 1
	IF (mt + loadOffset * upSize) % 64 == 0
		break
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2238</id>
<name>_MM512_EXTPACKSTOREHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHD</instr>
<desc>Down-converts and stores packed 32-bit integer elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*32
		tmp := DOWNCONVERT(v1[i+31:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		4: MEM[storeAddr] := tmp[31:0]
		2: MEM[storeAddr] := tmp[15:0]
		1: MEM[storeAddr] := tmp[7:0]
		ESAC
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2240</id>
<name>_MM512_EXTPACKSTOREHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHQ</instr>
<desc>Down-converts and stores packed 64-bit integer elements of v1 into a quadword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN 8
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*64
		tmp := DOWNCONVERT(v1[i+63:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		8: MEM[storeAddr] := tmp[63:0]
		ESAC
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2242</id>
<name>_MM512_EXTPACKSTOREHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHPD</instr>
<desc>Down-converts and stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN 8
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*64
		tmp := DOWNCONVERT(v1[i+63:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		8: MEM[storeAddr] := tmp[63:0]
		ESAC
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2244</id>
<name>_MM512_EXTPACKSTOREHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHPS</instr>
<desc>Down-converts and stores packed single-precision (32-bit) floating-point elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
	_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
	_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:	   RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*32
		tmp := DOWNCONVERT(v1[i+31:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		4: MEM[storeAddr] := tmp[31:0]
		2: MEM[storeAddr] := tmp[15:0]
		1: MEM[storeAddr] := tmp[7:0]
		ESAC
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2246</id>
<name>_MM512_EXTPACKSTORELO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VPACKSTORELD</instr>
<desc>Down-converts and stores packed 32-bit integer elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 15
	i := j*32
	tmp := DOWNCONVERT(v1[i+31:i], conv)
	storeAddr := addr + storeOffset * downSize
	CASE downSize OF
	4: MEM[storeAddr] := tmp[31:0]
	2: MEM[storeAddr] := tmp[15:0]
	1: MEM[storeAddr] := tmp[7:0]
	ESAC
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset * downSize) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2248</id>
<name>_MM512_EXTPACKSTORELO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VPACKSTORELQ</instr>
<desc>Down-converts and stores packed 64-bit integer elements of v1 into a quadword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN 8
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	i := j*63
	tmp := DOWNCONVERT(v1[i+63:i], conv)
	storeAddr := addr + storeOffset * downSize
	CASE downSize OF
	8: MEM[storeAddr] := tmp[63:0]
	ESAC
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset * downSize) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2250</id>
<name>_MM512_EXTPACKSTORELO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT hint</sign>
<instr>VPACKSTORELPD</instr>
<desc>Down-converts and stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN 8
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	i := j*63
	tmp := DOWNCONVERT(v1[i+63:i], conv)
	storeAddr := addr + storeOffset * downSize
	CASE downSize OF
	8: MEM[storeAddr] := tmp[63:0]
	ESAC
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset * downSize) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2252</id>
<name>_MM512_EXTPACKSTORELO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT hint</sign>
<instr>VPACKSTORELPS</instr>
<desc>Down-converts and stores packed single-precision (32-bit) floating-point elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
	_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
	_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:	   RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 15
	i := j*32
	tmp := DOWNCONVERT(v1[i+31:i], conv)
	storeAddr := addr + storeOffset * downSize
	CASE downSize OF
	4: MEM[storeAddr] := tmp[31:0]
	2: MEM[storeAddr] := tmp[15:0]
	1: MEM[storeAddr] := tmp[7:0]
	ESAC
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset * downSize) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2270</id>
<name>_MM512_EXTRACTF32X4_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M512 a,INT imm8</sign>
<instr>VEXTRACTF32X4</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2273</id>
<name>_MM512_EXTRACTF32X8_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M512 a,INT imm8</sign>
<instr>VEXTRACTF32X8</instr>
<desc>Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2279</id>
<name>_MM512_EXTRACTF64X2_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M512D a,INT imm8</sign>
<instr>VEXTRACTF64X2</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2282</id>
<name>_MM512_EXTRACTF64X4_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m256d</ret>
<sign>__M512D a,INT imm8</sign>
<instr>VEXTRACTF64X4</instr>
<desc>Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2289</id>
<name>_MM512_EXTRACTI32X4_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VEXTRACTI32X4</instr>
<desc>Extract 128 bits (composed of 4 packed 32-bit integers) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2292</id>
<name>_MM512_EXTRACTI32X8_EPI32</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VEXTRACTI32X8</instr>
<desc>Extract 256 bits (composed of 8 packed 32-bit integers) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2298</id>
<name>_MM512_EXTRACTI64X2_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VEXTRACTI64X2</instr>
<desc>Extract 128 bits (composed of 2 packed 64-bit integers) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2301</id>
<name>_MM512_EXTRACTI64X4_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VEXTRACTI64X4</instr>
<desc>Extract 256 bits (composed of 4 packed 64-bit integers) from a, selected with imm8, and store the result in dst.</desc>
<oper>CASE imm8[7:0] of
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2304</id>
<name>_MM512_EXTSTORE_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v,_MM_DOWNCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VMOVDQA32</instr>
<desc>Downconverts packed 32-bit integer elements stored in v to a smaller type depending on conv and stores them in memory location mt. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 15
	i := j*32
	CASE conv OF
	_MM_DOWNCONV_EPI32_NONE:
		addr[i+31:i] := v[i+31:i]
	_MM_DOWNCONV_EPI32_UINT8:
		n := j*8
		addr[n+7:n] := Int32ToUInt8(v[i+31:i])
	_MM_DOWNCONV_EPI32_SINT8:
		n := j*8
		addr[n+7:n] := Int32ToSInt8(v[i+31:i])
	_MM_DOWNCONV_EPI32_UINT16:
		n := j*16
		addr[n+15:n] := Int32ToUInt16(v[i+31:i])
	_MM_DOWNCONV_EPI32_SINT16:
		n := j*16
		addr[n+15:n] := Int32ToSInt16(v[i+31:i])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2306</id>
<name>_MM512_EXTSTORE_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v,_MM_DOWNCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VMOVDQA64</instr>
<desc>Downconverts packed 64-bit integer elements stored in v to a smaller type depending on conv and stores them in memory location mt. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 7
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v[i+63:i]
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2308</id>
<name>_MM512_EXTSTORE_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v,_MM_DOWNCONV_PD_ENUM conv,INT hint</sign>
<instr>VMOVAPD</instr>
<desc>Downconverts packed double-precision (64-bit) floating-point elements stored in v to a smaller type depending on conv and stores them in memory location mt. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 7
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_PS_NONE:
		addr[i+63:i] := v[i+63:i]
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2310</id>
<name>_MM512_EXTSTORE_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v,_MM_DOWNCONV_PS_ENUM conv,INT hint</sign>
<instr>VMOVAPS</instr>
<desc>Downconverts packed single-precision (32-bit) floating-point elements stored in v to a smaller type depending on conv and stores them in memory location mt. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]		
FOR j := 0 to 15
	i := j*32
	CASE conv OF
	_MM_DOWNCONV_PS_NONE:
		addr[i+31:i] := v[i+31:i]
	_MM_DOWNCONV_PS_FLOAT16:
		n := j*16
		addr[n+15:n] := Float32ToFloat16(v[i+31:i])
	_MM_DOWNCONV_PS_UINT8:
		n := j*8
		addr[n+7:n] := Float32ToUInt8(v[i+31:i])
	_MM_DOWNCONV_PS_SINT8:
		n := j*8
		addr[n+7:n] := Float32ToSInt8(v[i+31:i])
	_MM_DOWNCONV_PS_UINT16:
		n := j*16
		addr[n+15:n] := Float32ToUInt16(v[i+31:i])
	_MM_DOWNCONV_PS_SINT16:
		n := j*16
		addr[n+15:n] := Float32ToSInt16(v[i+31:i])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2318</id>
<name>_MM512_FIXUPIMM_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2327</id>
<name>_MM512_FIXUPIMM_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2330</id>
<name>_MM512_FIXUPIMM_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2333</id>
<name>_MM512_FIXUPIMM_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst. imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2348</id>
<name>_MM512_FIXUPNAN_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v1,__M512D v2,__M512I v3</sign>
<instr>VFIXUPNANPD</instr>
<desc>Fixes up NaN's from packed double-precision (64-bit) floating-point elements in v1 and v2, storing the results in dst and storing the quietized NaN's from v1 in v3.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := FixupNaNs(v1[i+63:i], v2[i+63:i])
	v3[i+63:i] := QuietizeNaNs(v1[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2350</id>
<name>_MM512_FIXUPNAN_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v1,__M512 v2,__M512I v3</sign>
<instr>VFIXUPNANPS</instr>
<desc>Fixes up NaN's from packed single-precision (32-bit) floating-point elements in v1 and v2, storing the results in dst and storing the quietized NaN's from v1 in v3.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FixupNaNs(v1[i+31:i], v2[i+31:i])
	v3[i+31:i] := QuietizeNaNs(v1[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2354</id>
<name>_MM512_FLOOR_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a down to an integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := FLOOR(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2358</id>
<name>_MM512_FLOOR_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a down to an integer value, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FLOOR(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2362</id>
<name>_MM512_FMADD_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,__M512I c</sign>
<instr>VPMADD231D</instr>
<desc>Multiply packed 32-bit integer elements in a and b, add the intermediate result to packed elements in c and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2373</id>
<name>_MM512_FMADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2385</id>
<name>_MM512_FMADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2389</id>
<name>_MM512_FMADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,INT rounding</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2393</id>
<name>_MM512_FMADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,INT rounding</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2411</id>
<name>_MM512_FMADD233_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMADD233D</instr>
<desc>Multiply packed 32-bit integer elements in each 4-element set of a and by element 1 of the corresponding 4-element set from b, add the intermediate result to element 0 of the corresponding 4-element set from b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	base := (j &amp; ~0x3) * 32
	scale[31:0] := b[base+63:base+32]
	bias[31:0]  := b[base+31:base]
	dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2413</id>
<name>_MM512_FMADD233_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VFMADD233PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in each 4-element set of a and by element 1 of the corresponding 4-element set from b, add the intermediate result to element 0 of the corresponding 4-element set from b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	base := (j &amp; ~0x3) * 32
	scale[31:0] := b[base+63:base+32]
	bias[31:0]  := b[base+31:base]
	dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2415</id>
<name>_MM512_FMADD233_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT rounding</sign>
<instr>VFMADD233PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in each 4-element set of a and by element 1 of the corresponding 4-element set from b, add the intermediate result to element 0 of the corresponding 4-element set from b, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	base := (j &amp; ~0x3) * 32
	scale[31:0] := b[base+63:base+32]
	bias[31:0]  := b[base+31:base]
	dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2425</id>
<name>_MM512_FMADDSUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2437</id>
<name>_MM512_FMADDSUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2441</id>
<name>_MM512_FMADDSUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2445</id>
<name>_MM512_FMADDSUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2457</id>
<name>_MM512_FMSUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2469</id>
<name>_MM512_FMSUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2473</id>
<name>_MM512_FMSUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,INT rounding</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2477</id>
<name>_MM512_FMSUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,INT rounding</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2503</id>
<name>_MM512_FMSUBADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2515</id>
<name>_MM512_FMSUBADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2519</id>
<name>_MM512_FMSUBADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF (j is even) 
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2523</id>
<name>_MM512_FMSUBADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF (j is even) 
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2535</id>
<name>_MM512_FNMADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2547</id>
<name>_MM512_FNMADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst. 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2551</id>
<name>_MM512_FNMADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,INT rounding</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst.
	 Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2555</id>
<name>_MM512_FNMADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,INT rounding</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst.  
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2581</id>
<name>_MM512_FNMSUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2593</id>
<name>_MM512_FNMSUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2597</id>
<name>_MM512_FNMSUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,INT rounding</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst.  
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2601</id>
<name>_MM512_FNMSUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,INT rounding</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2623</id>
<name>_MM512_FPCLASS_PD_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__M512D a,INT imm8</sign>
<instr>VFPCLASSPD</instr>
<desc>Test packed double-precision (64-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2629</id>
<name>_MM512_FPCLASS_PS_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask16</ret>
<sign>__M512 a,INT imm8</sign>
<instr>VFPCLASSPS</instr>
<desc>Test packed single-precision (32-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k.
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2653</id>
<name>_MM512_GETEXP_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2662</id>
<name>_MM512_GETEXP_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2665</id>
<name>_MM512_GETEXP_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2668</id>
<name>_MM512_GETEXP_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2689</id>
<name>_MM512_GETMANT_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2698</id>
<name>_MM512_GETMANT_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2701</id>
<name>_MM512_GETMANT_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2704</id>
<name>_MM512_GETMANT_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2719</id>
<name>_MM512_GMAX_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VGMAXPD</instr>
<desc>Determines the maximum of each pair of corresponding elements in packed double-precision (64-bit) floating-point elements in a and b, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := FpMax(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2721</id>
<name>_MM512_GMAX_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VGMAXPS</instr>
<desc>Determines the maximum of each pair of corresponding elements in packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FpMax(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2723</id>
<name>_MM512_GMAXABS_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VGMAXABSPS</instr>
<desc>Determines the maximum of the absolute elements of each pair of corresponding elements of packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2725</id>
<name>_MM512_GMIN_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VGMINPD</instr>
<desc>Determines the minimum of each pair of corresponding elements in packed double-precision (64-bit) floating-point elements in a and b, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := FpMin(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2727</id>
<name>_MM512_GMIN_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VGMINPS</instr>
<desc>Determines the minimum of each pair of corresponding elements in packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FpMin(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2757</id>
<name>_MM512_HYPOT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2761</id>
<name>_MM512_HYPOT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2763</id>
<name>_MM512_I32EXTGATHER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>VPGATHERDD</instr>
<desc>Up-converts 16 memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv to 32-bit integer elements and stores them in dst.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:
		dst[i+31:i] := addr[i+31:i]
	_MM_UPCONV_EPI32_UINT8:
		n := j*7
		dst[i+31:i] := UInt8ToUInt32(addr[n+7:n])
	_MM_UPCONV_EPI32_SINT8:
		n := j*7
		dst[i+31:i] := Int8ToInt32(addr[n+7:n])
	_MM_UPCONV_EPI32_UINT16:
		n := j*16
		dst[i+31:i] := UInt16ToUInt32(addr[n+15:n])
	_MM_UPCONV_EPI32_SINT16:
		n := j*16
		dst[i+31:i] := Int16ToInt32(addr[n+15:n])
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2765</id>
<name>_MM512_I32EXTGATHER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VGATHERDPS</instr>
<desc>Up-converts 16 memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv to single-precision (32-bit) floating-point elements and stores them in dst.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_UPCONV_PS_NONE:
		dst[i+31:i] := addr[i+31:i]
	_MM_UPCONV_PS_FLOAT16:
		n := j*16
		dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
	_MM_UPCONV_PS_UINT8:
		n := j*8
		dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
	_MM_UPCONV_PS_SINT8:
		n := j*8
		dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
	_MM_UPCONV_PS_UINT16:
		n := j*16
		dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
	_MM_UPCONV_PS_SINT16:
		n := j*16
		dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2767</id>
<name>_MM512_I32EXTSCATTER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>VPSCATTERDD</instr>
<desc>Down-converts 16 packed 32-bit integer elements in v1 using conv and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_DOWNCONV_EPI32_NONE:
		addr[i+31:i] := v1[i+31:i]
	_MM_DOWNCONV_EPI32_UINT8:
		n := j*8
		addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
	_MM_DOWNCONV_EPI32_SINT8:
		n := j*8
		addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
	_MM_DOWNCONV_EPI32_UINT16:
		n := j*16
		addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
	_MM_DOWNCONV_EPI32_SINT16:
		n := j*16
		addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2769</id>
<name>_MM512_I32EXTSCATTER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT scale,INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Down-converts 16 packed single-precision (32-bit) floating-point elements in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_DOWNCONV_PS_NONE:
		n := j*32
		addr[i+31:i] := v1[n+31:n]
	_MM_DOWNCONV_PS_FLOAT16:
		i := j*16
		addr[i+15:i] := Float32ToFloat16(v1[n+31:n])
	_MM_DOWNCONV_PS_UINT8:
		i := j*8
		addr[i+7:i] := Float32ToUInt8(v1[n+31:n])
	_MM_DOWNCONV_PS_SINT8:
		i := j*8
		addr[i+7:i] := Float32ToSInt8(v1[n+31:n])
	_MM_DOWNCONV_PS_UINT16:
		i := j*8
		addr[i+15:i] := Float32ToUInt16(v1[n+31:n])
	_MM_DOWNCONV_PS_SINT16:
		i := j*8
		addr[i+15:i] := Float32ToSInt16(v1[n+31:n])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2777</id>
<name>_MM512_I32GATHER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2785</id>
<name>_MM512_I32GATHER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M256I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	m := j*32
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2793</id>
<name>_MM512_I32GATHER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M256I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	m := j*32
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2801</id>
<name>_MM512_I32GATHER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2803</id>
<name>_MM512_I32LOEXTGATHER_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>VPGATHERDQ</instr>
<desc>Up-converts 8 double-precision (64-bit) memory locations starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale using conv to 64-bit integer elements and stores them in dst.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2805</id>
<name>_MM512_I32LOEXTGATHER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>VGATHERDPD</instr>
<desc>Up-converts 8 double-precision (64-bit) floating-point elements in memory locations starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale using conv to 64-bit floating-point elements and stores them in dst.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2807</id>
<name>_MM512_I32LOEXTSCATTER_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>VPSCATTERDQ</instr>
<desc>Down-converts 8 packed 64-bit integer elements in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2809</id>
<name>_MM512_I32LOEXTSCATTER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>VSCATTERDPD</instr>
<desc>Down-converts 8 packed double-precision (64-bit) floating-point elements in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_PD_NONE: addr[i+63:i] := v1[i+63:i]
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2811</id>
<name>_MM512_I32LOGATHER_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Loads 8 64-bit integer elements from memory starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale and stores them in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	addr := MEM[mv + index[j] * scale]
	dst[i+63:i] := addr[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2813</id>
<name>_MM512_I32LOGATHER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>Loads 8 double-precision (64-bit) floating-point elements stored at memory locations starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale them in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	addr := MEM[mv + index[j] * scale]
	dst[i+63:i] := addr[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2815</id>
<name>_MM512_I32LOSCATTER_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512I v1,INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Stores 8 packed 64-bit integer elements located in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	addr[i+63:i] := v1[k+63:j]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2817</id>
<name>_MM512_I32LOSCATTER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512D v1,INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Stores 8 packed double-precision (64-bit) floating-point elements in v1 and to memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	addr[i+63:i] := v1[k+63:j]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2823</id>
<name>_MM512_I32SCATTER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,__M512I a,INT scale</sign>
<instr>VPSCATTERDD</instr>
<desc>Scatter 32-bit integers from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2829</id>
<name>_MM512_I32SCATTER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M512I a,INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Scatter 64-bit integers from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2835</id>
<name>_MM512_I32SCATTER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,__M512D a,INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2841</id>
<name>_MM512_I32SCATTER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,__M512 a,INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2843</id>
<name>_MM512_I64EXTGATHER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 single-precision (32-bit) memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to 32-bit integer elements and stores them in dst. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:
		dst[i+31:i] := addr[i+31:i]
	_MM_UPCONV_EPI32_UINT8:
		n := j*8
		dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
	_MM_UPCONV_EPI32_SINT8:
		n := j*8
		dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
	_MM_UPCONV_EPI32_UINT16:
		n := j*16
		dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
	_MM_UPCONV_EPI32_SINT16:
		n := j*16
		dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
	ESAC
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2845</id>
<name>_MM512_I64EXTGATHER_EPI64</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 double-precision (64-bit) memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to 64-bit integer elements and stores them in dst. hint indicates to the processor whether the load is non-temporal.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	addr := MEM[mv + index[j] * scale]
	CASE conv OF
	_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2847</id>
<name>_MM512_I64EXTGATHER_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 double-precision (64-bit) floating-point elements stored in memory starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to 64-bit floating-point elements and stores them in dst. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2849</id>
<name>_MM512_I64EXTGATHER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to single-precision (32-bit) floating-point elements and stores them in the lower half of dst. hint indicates to the processor whether the load is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_UPCONV_PS_NONE:
		dst[i+31:i] := addr[i+31:i]
	_MM_UPCONV_PS_FLOAT16:
		n := j*16
		dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
	_MM_UPCONV_PS_UINT8:
		n := j*8
		dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
	_MM_UPCONV_PS_SINT8:
		n := j*8
		dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
	_MM_UPCONV_PS_UINT16:
		n := j*16
		dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
	_MM_UPCONV_PS_SINT16:
		n := j*16
		dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
	ESAC
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2851</id>
<name>_MM512_I64EXTSCATTER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts the low 8 packed 32-bit integer elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_EPI32_NONE:
		addr[i+31:i] := v1[i+31:i]
	_MM_DOWNCONV_EPI32_UINT8:
		n := j*8
		addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
	_MM_DOWNCONV_EPI32_SINT8:
		n := j*8
		addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
	_MM_DOWNCONV_EPI32_UINT16:
		n := j*16
		addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
	_MM_DOWNCONV_EPI32_SINT16:
		n := j*16
		addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2853</id>
<name>_MM512_I64EXTSCATTER_EPI64</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts 8 packed 64-bit integer elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. hint indicates to the processor whether the load is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2855</id>
<name>_MM512_I64EXTSCATTER_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts 8 packed double-precision (64-bit) floating-point elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_EPI64_NONE:
		addr[i+63:i] := v1[i+63:i]
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2857</id>
<name>_MM512_I64EXTSCATTER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts 8 packed single-precision (32-bit) floating-point elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE conv OF
	_MM_DOWNCONV_PS_NONE:
		addr[i+31:i] := v1[i+31:i]
	_MM_DOWNCONV_PS_FLOAT16:
		n := j*16
		addr[n+15:n] := Float32ToFloat16(v1[i+31:i])
	_MM_DOWNCONV_PS_UINT8:
		n := j*8
		addr[n+7:n] := Float32ToUInt8(v1[i+31:i])
	_MM_DOWNCONV_PS_SINT8:
		n := j*8
		addr[n+7:n] := Float32ToSInt8(v1[i+31:i])
	_MM_DOWNCONV_PS_UINT16:
		n := j*16
		addr[n+15:n] := Float32ToUInt16(v1[i+31:i])
	_MM_DOWNCONV_PS_SINT16:
		n := j*16
		addr[n+15:n] := Float32ToSInt16(v1[i+31:i])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2865</id>
<name>_MM512_I64GATHER_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2867</id>
<name>_MM512_I64GATHER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>NONE</instr>
<desc>Loads 8 32-bit integer memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale to dst.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	dst[i+31:i] := addr[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2875</id>
<name>_MM512_I64GATHER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2883</id>
<name>_MM512_I64GATHER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2891</id>
<name>_MM512_I64GATHER_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst. scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2893</id>
<name>_MM512_I64GATHER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>NONE</instr>
<desc>Loads 8 single-precision (32-bit) floating-point memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale to dst.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	dst[i+31:i] := addr[i+31:i]
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2899</id>
<name>_MM512_I64SCATTER_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,__M256I a,INT scale</sign>
<instr>VPSCATTERQD</instr>
<desc>Scatter 32-bit integers from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2901</id>
<name>_MM512_I64SCATTER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512I v1,INT scale</sign>
<instr>NONE</instr>
<desc>Stores 8 packed 32-bit integer elements in v1 in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	addr := MEM[mv + index[j] * scale]
	addr[i+31:i] := v1[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2907</id>
<name>_MM512_I64SCATTER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,__M512I a,INT scale</sign>
<instr>VPSCATTERQQ</instr>
<desc>Scatter 64-bit integers from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2913</id>
<name>_MM512_I64SCATTER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,__M512D a,INT scale</sign>
<instr>VSCATTERQPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2919</id>
<name>_MM512_I64SCATTER_PS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,__M256 a,INT scale</sign>
<instr>VSCATTERQPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2921</id>
<name>_MM512_I64SCATTER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,__M512 v,INT scale</sign>
<instr>NONE</instr>
<desc>Stores 8 packed single-precision (32-bit) floating-point elements in v in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale.</desc>
<oper>FOR j := 0 to 7
	i := j*32
	addr := MEM[mv + index[j] * scale]
	addr[i+31:i] := v[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2943</id>
<name>_MM512_INSERTF32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M128 b,INT imm8</sign>
<instr>VINSERTF32X4</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2946</id>
<name>_MM512_INSERTF32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M256 b,INT imm8</sign>
<instr>VINSERTF32X8</instr>
<desc>Copy a to dst, then insert 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from b into dst at the location specified by imm8.
	</desc>
<oper>dst[511:0] := a[511:0]
CASE (imm8[7:0]) OF
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2952</id>
<name>_MM512_INSERTF64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M128D b,INT imm8</sign>
<instr>VINSERTF64X2</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE imm8[7:0] of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2955</id>
<name>_MM512_INSERTF64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M256D b,INT imm8</sign>
<instr>VINSERTF64X4</instr>
<desc>Copy a to dst, then insert 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE (imm8[0]) of
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2962</id>
<name>_MM512_INSERTI32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I b,INT imm8</sign>
<instr>VINSERTI32X4</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 4 packed 32-bit integers) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2965</id>
<name>_MM512_INSERTI32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M256I b,INT imm8</sign>
<instr>VINSERTI32X8</instr>
<desc>Copy a to dst, then insert 256 bits (composed of 8 packed 32-bit integers) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE imm8[7:0] of
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2971</id>
<name>_MM512_INSERTI64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I b,INT imm8</sign>
<instr>VINSERTI64X2</instr>
<desc>Copy a to dst, then insert 128 bits (composed of 2 packed 64-bit integers) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE imm8[7:0] of
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2974</id>
<name>_MM512_INSERTI64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M256I b,INT imm8</sign>
<instr>VINSERTI64X4</instr>
<desc>Copy a to dst, then insert 256 bits (composed of 4 packed 64-bit integers) from b into dst at the location specified by imm8.</desc>
<oper>dst[511:0] := a[511:0]
CASE (imm8[7:0]) OF
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2977</id>
<name>_MM512_INT2MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>INT mask</sign>
<instr>KMOVW</instr>
<desc>Converts integer mask into bitmask, storing the result in dst.</desc>
<oper>dst := mask[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2978</id>
<name>_MM512_INT2MASK</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>INT mask</sign>
<instr>KMOV</instr>
<desc>Converts integer mask into bitmask, storing the result in dst.</desc>
<oper>dst := mask[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2986</id>
<name>_MM512_INVSQRT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := InvSQRT(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2990</id>
<name>_MM512_INVSQRT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := InvSQRT(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2994</id>
<name>_MM512_KAND</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KANDW</instr>
<desc>Compute the bitwise AND of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := a[15:0] AND b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2995</id>
<name>_MM512_KAND</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KAND</instr>
<desc>Compute the bitwise AND of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := a[15:0] AND b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2996</id>
<name>_MM512_KANDN</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KANDN</instr>
<desc>Compute the bitwise NOT of 16-bit masks a and then AND with b, and store the result in k.</desc>
<oper>k[15:0] := (NOT a[15:0]) AND b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2997</id>
<name>_MM512_KANDN</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KANDNW</instr>
<desc>Compute the bitwise NOT of 16-bit masks a and then AND with b, and store the result in k.</desc>
<oper>k[15:0] := (NOT a[15:0]) AND b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2998</id>
<name>_MM512_KANDNR</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KANDNR</instr>
<desc>Performs a bitwise AND operation between NOT of k2 and k1, storing the result in dst.</desc>
<oper>dst[15:0] := NOT(k2[15:0]) &amp; k1[15:0]</oper>
</intrinsic>
<intrinsic>
<id>2999</id>
<name>_MM512_KCONCATHI_64</name>
<cpuid>KNCNI</cpuid>
<ret>__int64</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KCONCATH</instr>
<desc>Packs masks k1 and k2 into the high 32 bits of dst. The rest of dst is set to 0.</desc>
<oper>dst[63:48] := k1[15:0]
dst[47:32] := k2[15:0]
dst[31:0]  := 0</oper>
</intrinsic>
<intrinsic>
<id>3000</id>
<name>_MM512_KCONCATLO_64</name>
<cpuid>KNCNI</cpuid>
<ret>__int64</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KCONCATL</instr>
<desc>Packs masks k1 and k2 into the low 32 bits of dst. The rest of dst is set to 0.</desc>
<oper>dst[31:16] := k1[15:0]
dst[15:0]  := k2[15:0]
dst[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3001</id>
<name>_MM512_KEXTRACT_64</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__INT64 a,CONST_INT b</sign>
<instr>KEXTRACT</instr>
<desc>Extracts 16-bit value b from 64-bit integer a, storing the result in dst.</desc>
<oper>CASE b of
0: dst[15:0] := a[63:48]
1: dst[15:0] := a[47:32]
2: dst[15:0] := a[31:16]
3: dst[15:0] := a[15:0]
ESAC
dst[MAX:15] := 0</oper>
</intrinsic>
<intrinsic>
<id>3002</id>
<name>_MM512_KMERGE2L1H</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KMERGE2L1H</instr>
<desc>Move the high element from k1 to the low element of k1, and insert the low element of k2 into the high element of k1.</desc>
<oper>tmp[7:0] := k1[15:8]
k1[15:8] := k2[7:0]
k1[7:0]  := tmp[7:0]</oper>
</intrinsic>
<intrinsic>
<id>3003</id>
<name>_MM512_KMERGE2L1L</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KMERGE2L1L</instr>
<desc>Insert the low element of k2 into the high element of k1.</desc>
<oper>k1[15:8] := k2[7:0]</oper>
</intrinsic>
<intrinsic>
<id>3004</id>
<name>_MM512_KMOV</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a</sign>
<instr>KMOVW</instr>
<desc>Copy 16-bit mask a to k.</desc>
<oper>k[15:0] := a[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3005</id>
<name>_MM512_KMOV</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a</sign>
<instr>KMOV</instr>
<desc>Copy 16-bit mask a to k.</desc>
<oper>k[15:0] := a[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3006</id>
<name>_MM512_KMOVLHB</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KMERGE2L1L</instr>
<desc>Inserts the low byte of mask k2 into the high byte of dst, and copies the low byte of k1 to the low byte of dst.</desc>
<oper>dst[7:0] := k1[7:0]
dst[15:8] := k2[7:0]</oper>
</intrinsic>
<intrinsic>
<id>3007</id>
<name>_MM512_KNOT</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a</sign>
<instr>KNOT</instr>
<desc>Compute the bitwise NOT of 16-bit mask a, and store the result in k.</desc>
<oper>k[15:0] := NOT a[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3008</id>
<name>_MM512_KNOT</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a</sign>
<instr>KNOTW</instr>
<desc>Compute the bitwise NOT of 16-bit mask a, and store the result in k.</desc>
<oper>k[15:0] := NOT a[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3009</id>
<name>_MM512_KOR</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KORW</instr>
<desc>Compute the bitwise OR of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := a[15:0] OR b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3010</id>
<name>_MM512_KOR</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KOR</instr>
<desc>Compute the bitwise OR of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := a[15:0] OR b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3011</id>
<name>_MM512_KORTESTC</name>
<cpuid>KNCNI</cpuid>
<ret>int</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KORTEST</instr>
<desc>Performs bitwise OR between k1 and k2, storing the result in dst. CF flag is set if dst consists of all 1's.</desc>
<oper>dst[15:0] := k1[15:0] | k2[15:0]
IF PopCount(dst[15:0]) = 16
	SetCF()
FI</oper>
</intrinsic>
<intrinsic>
<id>3012</id>
<name>_MM512_KORTESTC</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KORTESTW</instr>
<desc>Performs bitwise OR between k1 and k2, storing the result in dst. CF flag is set if dst consists of all 1's.</desc>
<oper>dst[15:0] := k1[15:0] | k2[15:0]
IF PopCount(dst[15:0]) = 16
	SetCF()
FI</oper>
</intrinsic>
<intrinsic>
<id>3013</id>
<name>_MM512_KORTESTZ</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KORTESTW</instr>
<desc>Performs bitwise OR between k1 and k2, storing the result in dst. ZF flag is set if dst is 0.</desc>
<oper>dst[15:0] := k1[15:0] | k2[15:0]
IF dst = 0
	SetZF()
FI</oper>
</intrinsic>
<intrinsic>
<id>3014</id>
<name>_MM512_KORTESTZ</name>
<cpuid>KNCNI</cpuid>
<ret>int</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KORTEST</instr>
<desc>Performs bitwise OR between k1 and k2, storing the result in dst. ZF flag is set if dst is 0.</desc>
<oper>dst[15:0] := k1[15:0] | k2[15:0]
IF dst = 0
	SetZF()
FI</oper>
</intrinsic>
<intrinsic>
<id>3015</id>
<name>_MM512_KSWAPB</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__MMASK16 k2</sign>
<instr>KMERGE2L1H</instr>
<desc>Moves high byte from k2 to low byte of k1, and moves low byte of k2 to high byte of k1.</desc>
<oper>tmp[7:0] := k2[15:8]
k2[15:8] := k1[7:0]
k1[7:0]  := tmp[7:0]

tmp[7:0] := k2[7:0]
k2[7:0]  := k1[15:8]
k1[15:8] := tmp[7:0]</oper>
</intrinsic>
<intrinsic>
<id>3016</id>
<name>_MM512_KUNPACKB</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KUNPCKBW</instr>
<desc>Unpack and interleave 8 bits from masks a and b, and store the 16-bit result in k.</desc>
<oper>k[7:0] := b[7:0]
k[15:8] := a[7:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3017</id>
<name>_MM512_KUNPACKD</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 a,__MMASK64 b</sign>
<instr>KUNPCKDQ</instr>
<desc>Unpack and interleave 32 bits from masks a and b, and store the 64-bit result in k.</desc>
<oper>k[31:0] := a[31:0]
k[63:32] := b[31:0]
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3018</id>
<name>_MM512_KUNPACKW</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 a,__MMASK32 b</sign>
<instr>KUNPCKWD</instr>
<desc>Unpack and interleave 16 bits from masks a and b, and store the 32-bit result in k.</desc>
<oper>k[15:0] := a[15:0]
k[31:16] := b[15:0]
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3019</id>
<name>_MM512_KXNOR</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KXNOR</instr>
<desc>Compute the bitwise XNOR of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := NOT (a[15:0] XOR b[15:0])
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3020</id>
<name>_MM512_KXNOR</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KXNORW</instr>
<desc>Compute the bitwise XNOR of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := NOT (a[15:0] XOR b[15:0])
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3021</id>
<name>_MM512_KXOR</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KXORW</instr>
<desc>Compute the bitwise XOR of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := a[15:0] XOR b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3022</id>
<name>_MM512_KXOR</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 a,__MMASK16 b</sign>
<instr>KXOR</instr>
<desc>Compute the bitwise XOR of 16-bit masks a and b, and store the result in k.</desc>
<oper>k[15:0] := a[15:0] XOR b[15:0]
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3030</id>
<name>_MM512_LOAD_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load 512-bits (composed of 16 packed 32-bit integers) from memory into dst. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3037</id>
<name>_MM512_LOAD_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load 512-bits (composed of 8 packed 64-bit integers) from memory into dst. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3046</id>
<name>_MM512_LOAD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from memory into dst. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3056</id>
<name>_MM512_LOAD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from memory into dst. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3065</id>
<name>_MM512_LOAD_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load 512-bits of integer data from memory into dst. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3112</id>
<name>_MM512_LOADU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from memory into dst. 
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3121</id>
<name>_MM512_LOADU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from memory into dst. 
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3130</id>
<name>_MM512_LOADU_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load 512-bits of integer data from memory into dst.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3136</id>
<name>_MM512_LOADUNPACKHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHD</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64 and expands them into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*4 % 64) == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*32
		tmp := MEM[addr + loadOffset*4]
		dst[i+31:i] := tmp[i+31:i]
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3138</id>
<name>_MM512_LOADUNPACKHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHQ</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64 and expands them into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*8) == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*64
		tmp := MEM[addr + loadOffset*8]
		dst[i+63:i] := tmp[i+63:i]
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3140</id>
<name>_MM512_LOADUNPACKHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHPD</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64 and expands them into packed double-precision (64-bit) floating-point values in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*8) % 64 == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*64
		tmp := MEM[addr + loadOffset*8]
		dst[i+63:i] := tmp[i+63:i]
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3142</id>
<name>_MM512_LOADUNPACKHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHPS</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64 and expands them into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF (addr + (loadOffset + 1)*4 % 64) == 0
			foundNext64BytesBoundary := true
		FI
	ELSE
		i := j*32
		tmp := MEM[addr + loadOffset*4]
		dst[i+31:i] := tmp[i+31:i]
	FI
	loadOffset := loadOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3144</id>
<name>_MM512_LOADUNPACKLO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLD</instr>
<desc>Loads the low-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt and expanded into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 15
	i := j*32
	tmp := MEM[addr + loadOffset*4]
	dst[i+31:i] := tmp[i+31:i]
	loadOffset := loadOffset + 1
	IF (mt + loadOffset * 4) % 64 == 0
		break
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3146</id>
<name>_MM512_LOADUNPACKLO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLQ</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt and expands them into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 7
	i := j*64
	tmp := MEM[addr + loadOffset*8]
	dst[i+63:i] := tmp[i+63:i]
	loadOffset := loadOffset + 1
	IF (addr + loadOffset*8 % 64) == 0
		break
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3148</id>
<name>_MM512_LOADUNPACKLO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLPD</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt and expands them into packed double-precision (64-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 7
	i := j*64
	tmp := MEM[addr + loadOffset*8]
	dst[i+63:i] := tmp[i+63:i]
	loadOffset := loadOffset + 1
	IF ((addr + 8*loadOffset) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3150</id>
<name>_MM512_LOADUNPACKLO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLPS</instr>
<desc>Loads the low-64-byte-aligned portion of the doubleword stream starting at element-aligned address mt and expanded into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src.</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 15
	i := j*32
	tmp := MEM[addr + loadOffset*4]
	dst[i+31:i] := tmp[i+31:i]
	loadOffset := loadOffset + 1
	IF (mt + loadOffset * 4) % 64 == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3154</id>
<name>_MM512_LOG_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ln(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3158</id>
<name>_MM512_LOG_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ln(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3162</id>
<name>_MM512_LOG10_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := log10(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3166</id>
<name>_MM512_LOG10_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := log10(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3170</id>
<name>_MM512_LOG1P_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ln(1.0 + a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3174</id>
<name>_MM512_LOG1P_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ln(1.0 + a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3178</id>
<name>_MM512_LOG2_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the base-2 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := log2(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3182</id>
<name>_MM512_LOG2_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VLOG2PS</instr>
<desc>Compute the base-2 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := log2(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3184</id>
<name>_MM512_LOG2AE23_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VLOG2PS</instr>
<desc>Compute the base-2 logarithm of packed single-precision (32-bit) floating-point elements in a with absolute error of 2^(-23) and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := Log2ae23(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3188</id>
<name>_MM512_LOGB_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3192</id>
<name>_MM512_LOGB_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision floating-point number representing the integer exponent, and store the results in dst. This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3202</id>
<name>_MM512_LZCNT_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	tmp := 31
	dst[i+31:i] := 0
	DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
		tmp := tmp - 1
		dst[i+31:i] := dst[i+31:i] + 1
	OD
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3211</id>
<name>_MM512_LZCNT_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	tmp := 63
	dst[i+63:i] := 0
	DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
		tmp := tmp - 1
		dst[i+63:i] := dst[i+63:i] + 1
	OD
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3222</id>
<name>_MM512_MADD_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3232</id>
<name>_MM512_MADD52HI_EPU64</name>
<cpuid>AVX512IFMA52</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,__M512I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
	dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3241</id>
<name>_MM512_MADD52LO_EPU64</name>
<cpuid>AVX512IFMA52</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,__M512I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
	dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3250</id>
<name>_MM512_MADDUBS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Vertically multiply each unsigned 8-bit integer from a with the corresponding signed 8-bit integer from b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>7</id>
<name>_MM512_MASK_ABS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ABS(a[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>16</id>
<name>_MM512_MASK_ABS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>25</id>
<name>_MM512_MASK_ABS_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>34</id>
<name>_MM512_MASK_ABS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := ABS(a[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>37</id>
<name>_MM512_MASK_ABS_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D v2</sign>
<instr>VPANDQ</instr>
<desc>Finds the absolute value of each packed double-precision (64-bit) floating-point element in v2, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(v2[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>42</id>
<name>_MM512_MASK_ABS_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2</sign>
<instr>VPANDD</instr>
<desc>Finds the absolute value of each packed single-precision (32-bit) floating-point element in v2, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(v2[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>46</id>
<name>_MM512_MASK_ACOS_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ACOS(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>50</id>
<name>_MM512_MASK_ACOS_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ACOS(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>54</id>
<name>_MM512_MASK_ACOSH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ACOSH(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>58</id>
<name>_MM512_MASK_ACOSH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ACOSH(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>60</id>
<name>_MM512_MASK_ADC_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k1,__MMASK16 k2,__M512I v3,__MMASK16_PTR k2_res</sign>
<instr>VPADCD</instr>
<desc>Performs element-by-element addition of packed 32-bit integers in v2 and v3 and the corresponding bit in k2, storing the result of the addition in dst and the result of the carry in k2_res using writemask k1 (elements are copied from v2 when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k2_res[j]   := Carry(v2[i+31:i] + v3[i+31:i] + k2[j])
		dst[i+31:i] := v2[i+31:i] + v3[i+31:i] + k2[j]
	ELSE
		dst[i+31:i] := v2[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>68</id>
<name>_MM512_MASK_ADD_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] + b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>77</id>
<name>_MM512_MASK_ADD_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>86</id>
<name>_MM512_MASK_ADD_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>95</id>
<name>_MM512_MASK_ADD_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] + b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>104</id>
<name>_MM512_MASK_ADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>116</id>
<name>_MM512_MASK_ADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>119</id>
<name>_MM512_MASK_ADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>122</id>
<name>_MM512_MASK_ADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>142</id>
<name>_MM512_MASK_ADDN_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D v2,__M512D v3</sign>
<instr>VADDNPD</instr>
<desc>Performs element-by-element addition between packed double-precision (64-bit) floating-point elements in v2 and v3 and negates their sum, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>144</id>
<name>_MM512_MASK_ADDN_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2,__M512 v3</sign>
<instr>VADDNPS</instr>
<desc>Performs element-by-element addition between packed single-precision (32-bit) floating-point elements in v2 and v3 and negates their sum, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>146</id>
<name>_MM512_MASK_ADDN_ROUND_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D v2,__M512D v3,INT rounding</sign>
<instr>VADDNPD</instr>
<desc>Performs element by element addition between packed double-precision (64-bit) floating-point elements in v2 and v3 and negates the sum, storing the result in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>148</id>
<name>_MM512_MASK_ADDN_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2,__M512 v3,INT rounding</sign>
<instr>VADDNPS</instr>
<desc>Performs element by element addition between packed single-precision (32-bit) floating-point elements in v2 and v3 and negates the sum, storing the result in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>156</id>
<name>_MM512_MASK_ADDS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>165</id>
<name>_MM512_MASK_ADDS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>174</id>
<name>_MM512_MASK_ADDS_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>183</id>
<name>_MM512_MASK_ADDS_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>190</id>
<name>_MM512_MASK_ADDSETC_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k,__MMASK16 k_old,__M512I v3,__MMASK16_PTR k2_res</sign>
<instr>VPADDSETCD</instr>
<desc>Performs element-by-element addition of packed 32-bit integer elements in v2 and v3, storing the resultant carry in k2_res (carry flag) and the addition results in dst using writemask k (elements are copied from v2 and k_old when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
	ELSE
		dst[i+31:i] := v2[i+31:i]
		k2_res[j] := k_old[j]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>192</id>
<name>_MM512_MASK_ADDSETS_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I v2,__M512I v3,__MMASK16_PTR sign</sign>
<instr>VPADDSETSD</instr>
<desc>Performs an element-by-element addition of packed 32-bit integer elements in v2 and v3, storing the results in dst and the sign of the sum in sign (sign flag). Results are stored using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
		sign[j] := v2[i+31:i] &amp; v3[i+31:i] &amp; 0x80000000
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>194</id>
<name>_MM512_MASK_ADDSETS_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2,__M512 v3,__MMASK16_PTR sign</sign>
<instr>VADDSETSPS</instr>
<desc>Performs an element-by-element addition of packed single-precision (32-bit) floating-point elements in v2 and v3, storing the results in dst and the sign of the sum in sign (sign flag). Results are stored using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
		sign[j] := v2[i+31:i] &amp; v3[i+31:i] &amp; 0x80000000
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>196</id>
<name>_MM512_MASK_ADDSETS_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2,__M512 v3,__MMASK16_PTR sign,INT rounding</sign>
<instr>VADDSETSPS</instr>
<desc>Performs an element-by-element addition of packed single-precision (32-bit) floating-point elements in v2 and v3, storing the results in dst and the sign of the sum in sign (sign flag). Results are stored using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
		sign[j] := v2[i+31:i] &amp; v3[i+31:i] &amp; 0x80000000
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>214</id>
<name>_MM512_MASK_ALIGNR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 128-byte immediate result, shift the result right by count 32-bit elements, and store the low 64 bytes (16 elements) in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &amp;gt;&amp;gt; (32*count)
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := temp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>223</id>
<name>_MM512_MASK_ALIGNR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 128-byte immediate result, shift the result right by count 64-bit elements, and store the low 64 bytes (8 elements) in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &amp;gt;&amp;gt; (64*count)
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := temp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>232</id>
<name>_MM512_MASK_ALIGNR_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 3
	i := j*128
	tmp[255:0] := ((a[i+127:i] &amp;lt;&amp;lt; 128) OR b[i+127:i]) &amp;gt;&amp;gt; (count[7:0]*8)
	tmp_dst[i+127:i] := tmp[127:0]
ENDFOR

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>241</id>
<name>_MM512_MASK_AND_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I v2,__M512I v3</sign>
<instr>VPANDD</instr>
<desc>Performs element-by-element bitwise AND between packed 32-bit integer elements of v2 and v3, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v2[i+31:i] &amp; v3[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>248</id>
<name>_MM512_MASK_AND_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] AND b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>257</id>
<name>_MM512_MASK_AND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>266</id>
<name>_MM512_MASK_AND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>277</id>
<name>_MM512_MASK_ANDNOT_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>284</id>
<name>_MM512_MASK_ANDNOT_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>293</id>
<name>_MM512_MASK_ANDNOT_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>302</id>
<name>_MM512_MASK_ANDNOT_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>311</id>
<name>_MM512_MASK_ASIN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ASIN(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>315</id>
<name>_MM512_MASK_ASIN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ASIN(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>319</id>
<name>_MM512_MASK_ASINH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ASINH(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>323</id>
<name>_MM512_MASK_ASINH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ASINH(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>327</id>
<name>_MM512_MASK_ATAN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a, and store the results in dst expressed in radians using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ATAN(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>331</id>
<name>_MM512_MASK_ATAN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ATAN(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>335</id>
<name>_MM512_MASK_ATAN2_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed double-precision (64-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 
	i := j*64
	IF k[j]
		dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>339</id>
<name>_MM512_MASK_ATAN2_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>NONE</instr>
<desc>Compute the inverse tangent of packed single-precision (32-bit) floating-point elements in a divided by packed elements in b, and store the results in dst expressed in radians using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>343</id>
<name>_MM512_MASK_ATANH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a, and store the results in dst expressed in radians using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ATANH(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>347</id>
<name>_MM512_MASK_ATANH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ATANH(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>355</id>
<name>_MM512_MASK_AVG_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>364</id>
<name>_MM512_MASK_AVG_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>388</id>
<name>_MM512_MASK_BLEND_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPBLENDMW</instr>
<desc>Blend packed 16-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := b[i+15:i]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>393</id>
<name>_MM512_MASK_BLEND_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPBLENDMD</instr>
<desc>Blend packed 32-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>396</id>
<name>_MM512_MASK_BLEND_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPBLENDMQ</instr>
<desc>Blend packed 64-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>399</id>
<name>_MM512_MASK_BLEND_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPBLENDMB</instr>
<desc>Blend packed 8-bit integers from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := b[i+7:i]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>404</id>
<name>_MM512_MASK_BLEND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VBLENDMPD</instr>
<desc>Blend packed double-precision (64-bit) floating-point elements from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := b[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>409</id>
<name>_MM512_MASK_BLEND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VBLENDMPS</instr>
<desc>Blend packed single-precision (32-bit) floating-point elements from a and b using control mask k, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := b[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>436</id>
<name>_MM512_MASK_BROADCAST_F32X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M128 a</sign>
<instr>VBROADCASTF32X2</instr>
<desc>Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>442</id>
<name>_MM512_MASK_BROADCAST_F32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M128 a</sign>
<instr>VBROADCASTF32X4</instr>
<desc>Broadcast the 4 packed single-precision (32-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>445</id>
<name>_MM512_MASK_BROADCAST_F32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M256 a</sign>
<instr>VBROADCASTF32X8</instr>
<desc>Broadcast the 8 packed single-precision (32-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 8)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>451</id>
<name>_MM512_MASK_BROADCAST_F64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTF64X2</instr>
<desc>Broadcast the 2 packed double-precision (64-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := src[n+63:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>454</id>
<name>_MM512_MASK_BROADCAST_F64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M256D a</sign>
<instr>VBROADCASTF64X4</instr>
<desc>Broadcast the 4 packed double-precision (64-bit) floating-point elements from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 4)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := src[n+63:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>463</id>
<name>_MM512_MASK_BROADCAST_I32X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[n+31:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>469</id>
<name>_MM512_MASK_BROADCAST_I32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M128I a</sign>
<instr>VBROADCASTI32X4</instr>
<desc>Broadcast the 4 packed 32-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[n+31:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>472</id>
<name>_MM512_MASK_BROADCAST_I32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M256I a</sign>
<instr>VBROADCASTI32X8</instr>
<desc>Broadcast the 8 packed 32-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 8)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := src[n+31:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>478</id>
<name>_MM512_MASK_BROADCAST_I64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI64X2</instr>
<desc>Broadcast the 2 packed 64-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := src[n+63:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>481</id>
<name>_MM512_MASK_BROADCAST_I64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256I a</sign>
<instr>VBROADCASTI64X4</instr>
<desc>Broadcast the 4 packed 64-bit integers from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 4)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := src[n+63:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>495</id>
<name>_MM512_MASK_BROADCASTB_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>504</id>
<name>_MM512_MASK_BROADCASTD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>519</id>
<name>_MM512_MASK_BROADCASTQ_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>526</id>
<name>_MM512_MASK_BROADCASTSD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>536</id>
<name>_MM512_MASK_BROADCASTSS_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>545</id>
<name>_MM512_MASK_BROADCASTW_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>600</id>
<name>_MM512_MASK_CBRT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := CubeRoot(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>604</id>
<name>_MM512_MASK_CBRT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cube root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := CubeRoot(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>608</id>
<name>_MM512_MASK_CDFNORM_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := CDFNormal(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>612</id>
<name>_MM512_MASK_CDFNORM_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := CDFNormal(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>616</id>
<name>_MM512_MASK_CDFNORMINV_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed double-precision (64-bit) floating-point elements in a using the normal distribution, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := InverseCDFNormal(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>620</id>
<name>_MM512_MASK_CDFNORMINV_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse cumulative distribution function of packed single-precision (32-bit) floating-point elements in a using the normal distribution, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := InverseCDFNormal(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>624</id>
<name>_MM512_MASK_CEIL_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a up to an integer value, and store the results as packed double-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := CEIL(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>628</id>
<name>_MM512_MASK_CEIL_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a up to an integer value, and store the results as packed single-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := CEIL(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>644</id>
<name>_MM512_MASK_CMP_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>650</id>
<name>_MM512_MASK_CMP_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>656</id>
<name>_MM512_MASK_CMP_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>662</id>
<name>_MM512_MASK_CMP_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>668</id>
<name>_MM512_MASK_CMP_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>674</id>
<name>_MM512_MASK_CMP_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>680</id>
<name>_MM512_MASK_CMP_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b,CONST_MM_CMPINT_ENUM imm8</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>686</id>
<name>_MM512_MASK_CMP_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b based on the comparison operand specified by imm8, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NEQ
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>694</id>
<name>_MM512_MASK_CMP_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>702</id>
<name>_MM512_MASK_CMP_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>704</id>
<name>_MM512_MASK_CMP_ROUND_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>706</id>
<name>_MM512_MASK_CMP_ROUND_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b,CONST_INT imm8,CONST_INT sae</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b based on the comparison operand specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>CASE (imm8[7:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>724</id>
<name>_MM512_MASK_CMPEQ_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>732</id>
<name>_MM512_MASK_CMPEQ_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPEQD</instr>
<desc>Compare packed 32-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>740</id>
<name>_MM512_MASK_CMPEQ_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPEQQ</instr>
<desc>Compare packed 64-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>748</id>
<name>_MM512_MASK_CMPEQ_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>754</id>
<name>_MM512_MASK_CMPEQ_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>760</id>
<name>_MM512_MASK_CMPEQ_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>766</id>
<name>_MM512_MASK_CMPEQ_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for equality, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>772</id>
<name>_MM512_MASK_CMPEQ_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>775</id>
<name>_MM512_MASK_CMPEQ_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := (a[i+63:i] == b[i+63:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR	
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>781</id>
<name>_MM512_MASK_CMPEQ_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for equality, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>y
FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := (a[i+31:i] == b[i+31:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR		
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>796</id>
<name>_MM512_MASK_CMPGE_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>802</id>
<name>_MM512_MASK_CMPGE_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>808</id>
<name>_MM512_MASK_CMPGE_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>814</id>
<name>_MM512_MASK_CMPGE_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>820</id>
<name>_MM512_MASK_CMPGE_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>826</id>
<name>_MM512_MASK_CMPGE_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>832</id>
<name>_MM512_MASK_CMPGE_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>838</id>
<name>_MM512_MASK_CMPGE_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>850</id>
<name>_MM512_MASK_CMPGT_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>858</id>
<name>_MM512_MASK_CMPGT_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPGTD</instr>
<desc>Compare packed 32-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>866</id>
<name>_MM512_MASK_CMPGT_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPGTQ</instr>
<desc>Compare packed 64-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>874</id>
<name>_MM512_MASK_CMPGT_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for greater-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>880</id>
<name>_MM512_MASK_CMPGT_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;gt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>886</id>
<name>_MM512_MASK_CMPGT_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;gt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>892</id>
<name>_MM512_MASK_CMPGT_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for greater-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;gt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>898</id>
<name>_MM512_MASK_CMPGT_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for greater-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;gt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>918</id>
<name>_MM512_MASK_CMPLE_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>924</id>
<name>_MM512_MASK_CMPLE_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>930</id>
<name>_MM512_MASK_CMPLE_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>936</id>
<name>_MM512_MASK_CMPLE_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>942</id>
<name>_MM512_MASK_CMPLE_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt;= b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>948</id>
<name>_MM512_MASK_CMPLE_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt; b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>954</id>
<name>_MM512_MASK_CMPLE_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt;= b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>960</id>
<name>_MM512_MASK_CMPLE_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt;= b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>963</id>
<name>_MM512_MASK_CMPLE_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := (a[i+63:i] &amp;lt;= b[i+63:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>966</id>
<name>_MM512_MASK_CMPLE_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := (a[i+31:i] &amp;lt;= b[i+31:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>975</id>
<name>_MM512_MASK_CMPLT_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>983</id>
<name>_MM512_MASK_CMPLT_EPI32_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>984</id>
<name>_MM512_MASK_CMPLT_EPI32_MASK</name>
<cpuid>KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPLTD</instr>
<desc>Compare packed 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>990</id>
<name>_MM512_MASK_CMPLT_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>997</id>
<name>_MM512_MASK_CMPLT_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1003</id>
<name>_MM512_MASK_CMPLT_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] &amp;lt; b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1009</id>
<name>_MM512_MASK_CMPLT_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for less-than-or-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] &amp;lt;= b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1015</id>
<name>_MM512_MASK_CMPLT_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for less-than, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] &amp;lt; b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1021</id>
<name>_MM512_MASK_CMPLT_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] &amp;lt; b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1024</id>
<name>_MM512_MASK_CMPLT_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := (a[i+63:i] &amp;lt; b[i+63:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1027</id>
<name>_MM512_MASK_CMPLT_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := (a[i+31:i] &amp;lt; b[i+31:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1035</id>
<name>_MM512_MASK_CMPNEQ_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPW</instr>
<desc>Compare packed 16-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1041</id>
<name>_MM512_MASK_CMPNEQ_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPD</instr>
<desc>Compare packed 32-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1047</id>
<name>_MM512_MASK_CMPNEQ_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPQ</instr>
<desc>Compare packed 64-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1053</id>
<name>_MM512_MASK_CMPNEQ_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPB</instr>
<desc>Compare packed 8-bit integers in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1059</id>
<name>_MM512_MASK_CMPNEQ_EPU16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>1065</id>
<name>_MM512_MASK_CMPNEQ_EPU32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1071</id>
<name>_MM512_MASK_CMPNEQ_EPU64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b for not-equal, and store the results in mask vector k1 using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1077</id>
<name>_MM512_MASK_CMPNEQ_EPU8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPCMPUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1080</id>
<name>_MM512_MASK_CMPNEQ_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := (a[i+63:i] != b[i+63:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1083</id>
<name>_MM512_MASK_CMPNEQ_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := (a[i+31:i] != b[i+31:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1096</id>
<name>_MM512_MASK_CMPNLE_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := !(a[i+63:i] &amp;lt;= b[i+63:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1099</id>
<name>_MM512_MASK_CMPNLE_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-less-than-or-equal, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := !(a[i+31:i] &amp;lt;= b[i+31:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1104</id>
<name>_MM512_MASK_CMPNLT_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b for not-less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := !(a[i+63:i] &amp;lt; b[i+63:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1107</id>
<name>_MM512_MASK_CMPNLT_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b for not-less-than, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := !(a[i+31:i] &amp;lt; b[i+31:i]) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1112</id>
<name>_MM512_MASK_CMPORD_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b to see if neither is NaN, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := (a[i+63:i] != NaN AND b[i+63:i] != NaN) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1115</id>
<name>_MM512_MASK_CMPORD_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b to see if neither is NaN, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := (a[i+31:i] != NaN AND b[i+31:i] != NaN) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1120</id>
<name>_MM512_MASK_CMPUNORD_PD_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,__M512D b</sign>
<instr>VCMPPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b to see if either is NaN, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := (a[i+63:i] == NaN OR b[i+63:i] == NaN) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>1123</id>
<name>_MM512_MASK_CMPUNORD_PS_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,__M512 b</sign>
<instr>VCMPPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b to see if either is NaN, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := (a[i+31:i] == NaN OR b[i+31:i] == NaN) ? 1 : 0
	ELSE 
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>1144</id>
<name>_MM512_MASK_COMPRESS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1150</id>
<name>_MM512_MASK_COMPRESS_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1156</id>
<name>_MM512_MASK_COMPRESS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1162</id>
<name>_MM512_MASK_COMPRESS_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in writemask k) to dst, and pass through the remaining elements from src.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1166</id>
<name>_MM512_MASK_COMPRESSSTOREU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 32
m := base_addr
FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1169</id>
<name>_MM512_MASK_COMPRESSSTOREU_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 64
m := base_addr
FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1172</id>
<name>_MM512_MASK_COMPRESSSTOREU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 64
m := base_addr
FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1175</id>
<name>_MM512_MASK_COMPRESSSTOREU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>size := 32
m := base_addr
FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1183</id>
<name>_MM512_MASK_CONFLICT_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit using writemask k (elements are copied from src when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[i]
		FOR l := 0 to j-1
			m := l*32
			dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
		ENDFOR
		dst[i+31:i+j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1192</id>
<name>_MM512_MASK_CONFLICT_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit using writemask k (elements are copied from src when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		FOR l := 0 to j-1
			m := l*64
			dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
		ENDFOR
		dst[i+63:i+j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1197</id>
<name>_MM512_MASK_COS_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := COS(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1201</id>
<name>_MM512_MASK_COS_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := COS(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1205</id>
<name>_MM512_MASK_COSD_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := COSD(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1209</id>
<name>_MM512_MASK_COSD_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the cosine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := COSD(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1213</id>
<name>_MM512_MASK_COSH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := COSH(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1217</id>
<name>_MM512_MASK_COSH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic cosine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := COSH(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1229</id>
<name>_MM512_MASK_CVT_ROUNDEPI32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I a,INT rounding</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1232</id>
<name>_MM512_MASK_CVT_ROUNDEPI64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1235</id>
<name>_MM512_MASK_CVT_ROUNDEPI64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1238</id>
<name>_MM512_MASK_CVT_ROUNDEPU32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I a,INT rounding</sign>
<instr>VCVTUDQ2PS</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1241</id>
<name>_MM512_MASK_CVT_ROUNDEPU64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1244</id>
<name>_MM512_MASK_CVT_ROUNDEPU64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1250</id>
<name>_MM512_MASK_CVT_ROUNDPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1253</id>
<name>_MM512_MASK_CVT_ROUNDPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1256</id>
<name>_MM512_MASK_CVT_ROUNDPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1259</id>
<name>_MM512_MASK_CVT_ROUNDPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1262</id>
<name>_MM512_MASK_CVT_ROUNDPD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1265</id>
<name>_MM512_MASK_CVT_ROUNDPD_PSLO</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK8 k,__M512D v2,INT rounding</sign>
<instr>VCVTPD2PS</instr>
<desc>Performs element-by-element conversion of packed double-precision (64-bit) floating-point elements in v2 to packed single-precision (32-bit) floating-point elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). Results are written to the lower half of dst, and the upper half locations are set to '0'.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Float64ToFloat32(v2[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1267</id>
<name>_MM512_MASK_CVT_ROUNDPH_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M256I a,INT sae</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1270</id>
<name>_MM512_MASK_CVT_ROUNDPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1273</id>
<name>_MM512_MASK_CVT_ROUNDPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	 Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	 </desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1276</id>
<name>_MM512_MASK_CVT_ROUNDPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1279</id>
<name>_MM512_MASK_CVT_ROUNDPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1282</id>
<name>_MM512_MASK_CVT_ROUNDPS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M256 a,INT sae</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1289</id>
<name>_MM512_MASK_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1324</id>
<name>_MM512_MASK_CVTEPI16_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M256I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	l := j*16
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1333</id>
<name>_MM512_MASK_CVTEPI16_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1342</id>
<name>_MM512_MASK_CVTEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M512I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1346</id>
<name>_MM512_MASK_CVTEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK32 k,__M512I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1354</id>
<name>_MM512_MASK_CVTEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M512I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1363</id>
<name>_MM512_MASK_CVTEPI32_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1372</id>
<name>_MM512_MASK_CVTEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M512I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1381</id>
<name>_MM512_MASK_CVTEPI32_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M256I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	IF k[j]
		dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
	ELSE
		dst[m+63:m] := src[m+63:m]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1390</id>
<name>_MM512_MASK_CVTEPI32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1394</id>
<name>_MM512_MASK_CVTEPI32_STOREU_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1397</id>
<name>_MM512_MASK_CVTEPI32_STOREU_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1399</id>
<name>_MM512_MASK_CVTEPI32LO_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I v2</sign>
<instr>VCVTDQ2PD</instr>
<desc>Performs element-by-element conversion of the lower half of packed 32-bit integer elements in v2 to packed double-precision (64-bit) floating-point elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	n := j*64
	IF k[j]
		dst[k+63:k] := Int32ToFloat64(v2[i+31:i])
	ELSE
		dst[n+63:n] := src[n+63:n]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1407</id>
<name>_MM512_MASK_CVTEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1416</id>
<name>_MM512_MASK_CVTEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1425</id>
<name>_MM512_MASK_CVTEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1434</id>
<name>_MM512_MASK_CVTEPI64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1443</id>
<name>_MM512_MASK_CVTEPI64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1447</id>
<name>_MM512_MASK_CVTEPI64_STOREU_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1450</id>
<name>_MM512_MASK_CVTEPI64_STOREU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1453</id>
<name>_MM512_MASK_CVTEPI64_STOREU_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1461</id>
<name>_MM512_MASK_CVTEPI8_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M256I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := SignExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1470</id>
<name>_MM512_MASK_CVTEPI8_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1479</id>
<name>_MM512_MASK_CVTEPI8_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1488</id>
<name>_MM512_MASK_CVTEPU16_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M256I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1497</id>
<name>_MM512_MASK_CVTEPU16_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1506</id>
<name>_MM512_MASK_CVTEPU32_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1515</id>
<name>_MM512_MASK_CVTEPU32_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M256I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1518</id>
<name>_MM512_MASK_CVTEPU32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I a</sign>
<instr>VCVTUDQ2PS</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1521</id>
<name>_MM512_MASK_CVTEPU32LO_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I v2</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Performs element-by-element conversion of the lower half of 32-bit unsigned integer elements in v2 to packed double-precision (64-bit) floating-point elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[l+63:l] := UInt32ToFloat64(v2[i+31:i])
	ELSE
		dst[l+63:l] := src[l+63:l]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1529</id>
<name>_MM512_MASK_CVTEPU64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1538</id>
<name>_MM512_MASK_CVTEPU64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1547</id>
<name>_MM512_MASK_CVTEPU8_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M256I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := ZeroExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1556</id>
<name>_MM512_MASK_CVTEPU8_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1565</id>
<name>_MM512_MASK_CVTEPU8_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1569</id>
<name>_MM512_MASK_CVTFXPNT_ROUND_ADJUSTEPU32_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I v2,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VCVTFXPNTUDQ2PS</instr>
<desc>Performs element-by-element conversion of packed 32-bit unsigned integer elements in v2 to packed single-precision (32-bit) floating-point elements and performing an optional exponent adjust using expadj, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Int32ToFloat32(v2[i+31:i])
		CASE expadj OF
		_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
		_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i] * 2**4
		_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i] * 2**5
		_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i] * 2**8
		_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
		_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
		_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
		_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1573</id>
<name>_MM512_MASK_CVTFXPNT_ROUNDPD_EPI32LO</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D v2,INT rounding</sign>
<instr>VCVTFXPNTPD2DQ</instr>
<desc>Performs an element-by-element conversion of elements in packed double-precision (64-bit) floating-point vector v2 to 32-bit integer elements, storing them in the lower half of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The elements in the upper half of dst are set to 0.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Float64ToInt32(v2[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1575</id>
<name>_MM512_MASK_CVTFXPNT_ROUNDPD_EPU32LO</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D v2,INT rounding</sign>
<instr>VCVTFXPNTPD2UDQ</instr>
<desc>Performs element-by-element conversion of packed double-precision (64-bit) floating-point elements in v2 to packed 32-bit unsigned integer elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). Results are written to the lower half of dst, and the upper half locations are set to '0'.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Float64ToInt32(v2[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1588</id>
<name>_MM512_MASK_CVTPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1597</id>
<name>_MM512_MASK_CVTPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1606</id>
<name>_MM512_MASK_CVTPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1615</id>
<name>_MM512_MASK_CVTPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1625</id>
<name>_MM512_MASK_CVTPD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1628</id>
<name>_MM512_MASK_CVTPD_PSLO</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK8 k,__M512D v2</sign>
<instr>VCVTPD2PS</instr>
<desc>Performs an element-by-element conversion of packed double-precision (64-bit) floating-point elements in v2 to single-precision (32-bit) floating-point elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The elements are stored in the lower half of the results vector, while the remaining upper half locations are set to 0.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Float64ToFloat32(v2[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1636</id>
<name>_MM512_MASK_CVTPH_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M256I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1650</id>
<name>_MM512_MASK_CVTPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1659</id>
<name>_MM512_MASK_CVTPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1668</id>
<name>_MM512_MASK_CVTPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1677</id>
<name>_MM512_MASK_CVTPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1682</id>
<name>_MM512_MASK_CVTPS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1691</id>
<name>_MM512_MASK_CVTPS_PH</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1697</id>
<name>_MM512_MASK_CVTPSLO_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512 v2</sign>
<instr>VCVTPS2PD</instr>
<desc>Performs element-by-element conversion of the lower half of packed single-precision (32-bit) floating-point elements in v2 to packed double-precision (64-bit) floating-point elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[l+63:l] := Float32ToFloat64(v2[i+31:i])
	ELSE
		dst[l+63:l] := src[l+63:l]:
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1718</id>
<name>_MM512_MASK_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M512I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1722</id>
<name>_MM512_MASK_CVTSEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK32 k,__M512I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1730</id>
<name>_MM512_MASK_CVTSEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M512I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1739</id>
<name>_MM512_MASK_CVTSEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M512I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1743</id>
<name>_MM512_MASK_CVTSEPI32_STOREU_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1746</id>
<name>_MM512_MASK_CVTSEPI32_STOREU_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1754</id>
<name>_MM512_MASK_CVTSEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1763</id>
<name>_MM512_MASK_CVTSEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1772</id>
<name>_MM512_MASK_CVTSEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1776</id>
<name>_MM512_MASK_CVTSEPI64_STOREU_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1779</id>
<name>_MM512_MASK_CVTSEPI64_STOREU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1782</id>
<name>_MM512_MASK_CVTSEPI64_STOREU_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1811</id>
<name>_MM512_MASK_CVTT_ROUNDPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1814</id>
<name>_MM512_MASK_CVTT_ROUNDPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1817</id>
<name>_MM512_MASK_CVTT_ROUNDPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1820</id>
<name>_MM512_MASK_CVTT_ROUNDPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1823</id>
<name>_MM512_MASK_CVTT_ROUNDPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a,INT sae</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions. </desc>
<oper>FOR j := 0 to 15
	i := 32*i
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1826</id>
<name>_MM512_MASK_CVTT_ROUNDPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a,INT sae</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1829</id>
<name>_MM512_MASK_CVTT_ROUNDPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a,INT sae</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := 32*i
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1832</id>
<name>_MM512_MASK_CVTT_ROUNDPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a,INT sae</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1854</id>
<name>_MM512_MASK_CVTTPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1863</id>
<name>_MM512_MASK_CVTTPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1872</id>
<name>_MM512_MASK_CVTTPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1881</id>
<name>_MM512_MASK_CVTTPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1891</id>
<name>_MM512_MASK_CVTTPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1900</id>
<name>_MM512_MASK_CVTTPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1909</id>
<name>_MM512_MASK_CVTTPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed double-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1918</id>
<name>_MM512_MASK_CVTTPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1945</id>
<name>_MM512_MASK_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK32 k,__M512I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1949</id>
<name>_MM512_MASK_CVTUSEPI16_STOREU_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK32 k,__M512I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1957</id>
<name>_MM512_MASK_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK16 k,__M512I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1966</id>
<name>_MM512_MASK_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK16 k,__M512I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1970</id>
<name>_MM512_MASK_CVTUSEPI32_STOREU_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed 16-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1973</id>
<name>_MM512_MASK_CVTUSEPI32_STOREU_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>1981</id>
<name>_MM512_MASK_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := src[l+15:l]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1990</id>
<name>_MM512_MASK_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := src[l+31:l]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1999</id>
<name>_MM512_MASK_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := src[l+7:l]
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2003</id>
<name>_MM512_MASK_CVTUSEPI64_STOREU_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed 16-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2006</id>
<name>_MM512_MASK_CVTUSEPI64_STOREU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed 32-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2009</id>
<name>_MM512_MASK_CVTUSEPI64_STOREU_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed 8-bit integers with unsigned saturation, and store the active results (those with their respective bit set in writemask k) to unaligned memory at base_addr.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2017</id>
<name>_MM512_MASK_DBSAD_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected from within 128-bit lanes according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>FOR j := 0 to 3
	i := j*128
	tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
	tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
	tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
	tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
ENDFOR

FOR j := 0 to 7
	i := j*64
	tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2027</id>
<name>_MM512_MASK_DIV_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the truncated results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2040</id>
<name>_MM512_MASK_DIV_EPU32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the truncated results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2054</id>
<name>_MM512_MASK_DIV_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2063</id>
<name>_MM512_MASK_DIV_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2066</id>
<name>_MM512_MASK_DIV_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VDIVPD</instr>
<desc>
	Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2069</id>
<name>_MM512_MASK_DIV_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VDIVPS</instr>
<desc>
	Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2091</id>
<name>_MM512_MASK_ERF_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ERF(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2095</id>
<name>_MM512_MASK_ERF_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ERF(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2099</id>
<name>_MM512_MASK_ERFC_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := 1.0 - ERF(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2103</id>
<name>_MM512_MASK_ERFC_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := 1.0 - ERF(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2107</id>
<name>_MM512_MASK_ERFCINV_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2111</id>
<name>_MM512_MASK_ERFCINV_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse complementary error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2115</id>
<name>_MM512_MASK_ERFINV_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := 1.0 / ERF(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2119</id>
<name>_MM512_MASK_ERFINV_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse error function of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := 1.0 / ERF(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2123</id>
<name>_MM512_MASK_EXP_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := e^(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2127</id>
<name>_MM512_MASK_EXP_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := e^(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2131</id>
<name>_MM512_MASK_EXP10_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := 10^(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2135</id>
<name>_MM512_MASK_EXP10_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 10 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := 10^(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2139</id>
<name>_MM512_MASK_EXP2_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := 2^(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2143</id>
<name>_MM512_MASK_EXP2_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := 2^(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2145</id>
<name>_MM512_MASK_EXP223_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I v2</sign>
<instr>VEXP223PS</instr>
<desc>Approximates the base-2 exponent of the packed single-precision (32-bit) floating-point elements in v2 with eight bits for sign and magnitude and 24 bits for the fractional part. Results are stored in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := exp223(v2[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2147</id>
<name>_MM512_MASK_EXP2A23_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D src</sign>
<instr>VEXP2PD</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
	ELSE
		dst[i+63:i] := src[i+63:i];
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2150</id>
<name>_MM512_MASK_EXP2A23_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VEXP2PS</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
	ELSE
		dst[i*31:i] := src[i*31:i];
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2153</id>
<name>_MM512_MASK_EXP2A23_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D src,INT rounding</sign>
<instr>VEXP2PD</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
	ELSE
		dst[i+63:i] := src[i+63:i];
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2156</id>
<name>_MM512_MASK_EXP2A23_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VEXP2PS</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
	ELSE
		dst[i*31:i] := src[i*31:i];
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2162</id>
<name>_MM512_MASK_EXPAND_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2168</id>
<name>_MM512_MASK_EXPAND_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2174</id>
<name>_MM512_MASK_EXPAND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2180</id>
<name>_MM512_MASK_EXPAND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2186</id>
<name>_MM512_MASK_EXPANDLOADU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2192</id>
<name>_MM512_MASK_EXPANDLOADU_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2198</id>
<name>_MM512_MASK_EXPANDLOADU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2204</id>
<name>_MM512_MASK_EXPANDLOADU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2209</id>
<name>_MM512_MASK_EXPM1_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed double-precision (64-bit) floating-point elements in a, subtract one from each element, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := e^(a[i+63:i]) - 1.0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2213</id>
<name>_MM512_MASK_EXPM1_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of e raised to the power of packed single-precision (32-bit) floating-point elements in a, subtract one from each element, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := e^(a[i+31:i]) - 1.0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2215</id>
<name>_MM512_MASK_EXTLOAD_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mt,_MM_UPCONV_EPI32_ENUM conv,_MM_BROADCAST32_ENUM bc,INT hint</sign>
<instr>VMOVDQA32, VBROADCASTI32X4, VPBROADCASTD</instr>
<desc>Depending on bc, loads 1, 4, or 16 elements of type and size determined by conv from memory address mt and converts all elements to 32-bit integer elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		CASE bc OF
		_MM_BROADCAST32_NONE:
			CASE conv OF
			_MM_UPCONV_EPI32_NONE:
				n	 := j*32
				dst[i+31:i] := addr[n+31:n]
			_MM_UPCONV_EPI32_UINT8:
				n	 := j*8
				dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
			_MM_UPCONV_EPI32_SINT8:
				n	 := j*8
				dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
			_MM_UPCONV_EPI32_UINT16:
				n	 := j*16
				dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
			_MM_UPCONV_EPI32_SINT16:
				n	 := j*16
				dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
			ESAC
		_MM_BROADCAST_1X16:
			CASE conv OF
			_MM_UPCONV_EPI32_NONE:
				n	 := j*32
				dst[i+31:i] := addr[31:0]
			_MM_UPCONV_EPI32_UINT8:
				n	 := j*8
				dst[i+31:i] := UInt8ToInt32(addr[7:0])
			_MM_UPCONV_EPI32_SINT8:
				n	 := j*8
				dst[i+31:i] := SInt8ToInt32(addr[7:0])
			_MM_UPCONV_EPI32_UINT16:
				n	 := j*16
				dst[i+31:i] := UInt16ToInt32(addr[15:0])
			_MM_UPCONV_EPI32_SINT16:
				n	 := j*16
				dst[i+31:i] := SInt16ToInt32(addr[15:0])
			ESAC
		_MM_BROADCAST_4X16:
			mod := j%4
			CASE conv OF
			_MM_UPCONV_EPI32_NONE:
				n := mod*32
				dst[i+31:i] := addr[n+31:n]
			_MM_UPCONV_EPI32_UINT8:
				n := mod*8
				dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
			_MM_UPCONV_EPI32_SINT8:
				n := mod*8
				dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
			_MM_UPCONV_EPI32_UINT16:
				n := mod*16
				dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
			_MM_UPCONV_EPI32_SINT16:
				n := mod*16
				dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
			ESAC
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2217</id>
<name>_MM512_MASK_EXTLOAD_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mt,_MM_UPCONV_EPI64_ENUM conv,_MM_BROADCAST64_ENUM bc,INT hint</sign>
<instr>VMOVDQA64, VBROADCASTI64X4, VPBROADCASTQ</instr>
<desc>Depending on bc, loads 1, 4, or 8 elements of type and size determined by conv from memory address mt and converts all elements to 64-bit integer elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		CASE bc OF
		_MM_BROADCAST64_NONE:
			CASE conv OF
			_MM_UPCONV_EPI64_NONE:
				n := j*64
				dst[i+63:i] := addr[n+63:n]
			ESAC
		_MM_BROADCAST_1X8:
			CASE conv OF
			_MM_UPCONV_EPI64_NONE:
				n := j*64
				dst[i+63:i] := addr[63:0]
			ESAC
		_MM_BROADCAST_4X8:
			mod := j%4
			CASE conv OF
			_MM_UPCONV_EPI64_NONE:
				n := mod*64
				dst[i+63:i] := addr[n+63:n]
			ESAC
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2219</id>
<name>_MM512_MASK_EXTLOAD_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mt,_MM_UPCONV_PD_ENUM conv,_MM_BROADCAST64_ENUM bc,INT hint</sign>
<instr>VMOVAPD, VBROADCASTF64X4, VBROADCASTSD</instr>
<desc>Depending on bc, loads 1, 4, or 8 elements of type and size determined by conv from memory address mt and converts all elements to double-precision (64-bit) floating-point elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		CASE bc OF
		_MM_BROADCAST64_NONE:
			CASE conv OF
			_MM_UPCONV_PD_NONE:
				n := j*64
				dst[i+63:i] := addr[n+63:n]
			ESAC
		_MM_BROADCAST_1X8:
			CASE conv OF
			_MM_UPCONV_PD_NONE:
				n := j*64
				dst[i+63:i] := addr[63:0]
			ESAC
		_MM_BROADCAST_4X8:
			mod := j%4
			CASE conv OF
			_MM_UPCONV_PD_NONE:
				n := mod*64
				dst[i+63:i] := addr[n+63:n]
			ESAC
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2221</id>
<name>_MM512_MASK_EXTLOAD_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mt,_MM_UPCONV_PS_ENUM conv,_MM_BROADCAST32_ENUM bc,INT hint</sign>
<instr>VMOVAPS, VBROADCASTF32X4, VBROADCASTSS</instr>
<desc>Depending on bc, loads 1, 4, or 16 elements of type and size determined by conv from memory address mt and converts all elements to single-precision (32-bit) floating-point elements, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr = MEM[mt]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		CASE bc OF
		_MM_BROADCAST32_NONE:
			CASE conv OF
			_MM_UPCONV_PS_NONE:
				n	 := j*32
				dst[i+31:i] := addr[n+31:n]
			_MM_UPCONV_PS_FLOAT16:
				n	 := j*16
				dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
			_MM_UPCONV_PS_UINT8:
				n	 := j*8
				dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
			_MM_UPCONV_PS_SINT8:
				n	 := j*8
				dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
			_MM_UPCONV_PS_UINT16:
				n	 := j*16
				dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
			_MM_UPCONV_PS_SINT16:
				n	 := j*16
				dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
			ESAC
		_MM_BROADCAST_1X16:
			CASE conv OF
			_MM_UPCONV_PS_NONE:
				n	 := j*32
				dst[i+31:i] := addr[31:0]
			_MM_UPCONV_PS_FLOAT16:
				n	 := j*16
				dst[i+31:i] := Float16ToFloat32(addr[15:0])
			_MM_UPCONV_PS_UINT8:
				n	 := j*8
				dst[i+31:i] := UInt8ToFloat32(addr[7:0])
			_MM_UPCONV_PS_SINT8:
				n	 := j*8
				dst[i+31:i] := SInt8ToFloat32(addr[7:0])
			_MM_UPCONV_PS_UINT16:
				n	 := j*16
				dst[i+31:i] := UInt16ToFloat32(addr[15:0])
			_MM_UPCONV_PS_SINT16:
				n	 := j*16
				dst[i+31:i] := SInt16ToFloat32(addr[15:0])
			ESAC
		_MM_BROADCAST_4X16:
			mod := j%4
			CASE conv OF
			_MM_UPCONV_PS_NONE:
				n := mod*32
				dst[i+31:i] := addr[n+31:n]
			_MM_UPCONV_PS_FLOAT16:
				n := mod*16
				dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
			_MM_UPCONV_PS_UINT8:
				n := mod*8
				dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
			_MM_UPCONV_PS_SINT8:
				n := mod*8
				dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
			_MM_UPCONV_PS_UINT16:
				n := mod*16
				dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
			_MM_UPCONV_PS_SINT16:
				n := mod*16
				dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
			ESAC
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2223</id>
<name>_MM512_MASK_EXTLOADUNPACKHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mt,_MM_UPCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHD</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*upSize % 64) == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*32
			dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2225</id>
<name>_MM512_MASK_EXTLOADUNPACKHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mt,_MM_UPCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHQ</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*upSize) == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*64
			dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2227</id>
<name>_MM512_MASK_EXTLOADUNPACKHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mt,_MM_UPCONV_PD_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHPD</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed double-precision (64-bit) floating-point values in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE: RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE: RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*upSize) % 64 == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*64
			dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2229</id>
<name>_MM512_MASK_EXTLOADUNPACKHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mt,_MM_UPCONV_PS_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKHPS</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64, up-converted depending on the value of conv, and expanded into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
	_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
	_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
upSize := UPCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*upSize % 64) == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*32
			dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2231</id>
<name>_MM512_MASK_EXTLOADUNPACKLO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mt,_MM_UPCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLD</instr>
<desc>Loads the low-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 15
	IF k[j]
		i := j*32
		dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
		loadOffset := loadOffset + 1
		IF (mt + loadOffset * upSize) % 64 == 0
			break
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2233</id>
<name>_MM512_MASK_EXTLOADUNPACKLO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mt,_MM_UPCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLQ</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quadwords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_EPI64_NONE:   RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	IF k[j]
		i := j*64
		dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
		loadOffset := loadOffset + 1
		IF (addr + loadOffset*upSize % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2235</id>
<name>_MM512_MASK_EXTLOADUNPACKLO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mt,_MM_UPCONV_PD_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLPD</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed double-precision (64-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elemenst are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE:	RETURN MEM[addr + 8*offset]
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PD_NONE:	RETURN 8
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	IF k[j]
		i := j*64
		dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
		loadOffset := loadOffset + 1
		IF (mt + loadOffset * upSize) % 64 == 0
			break
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2237</id>
<name>_MM512_MASK_EXTLOADUNPACKLO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mt,_MM_UPCONV_PS_ENUM conv,INT hint</sign>
<instr>VLOADUNPACKLPS</instr>
<desc>Loads the low-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt, up-converted depending on the value of conv, and expanded into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. hint indicates to the processor whether the loaded data is non-temporal. Elements are copied to dst according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>UPCONVERT(address, offset, convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:    RETURN MEM[addr + 4*offset]
	_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
	_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
	_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
	_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
	ESAC
}

UPCONVERTSIZE(convertTo) {
	CASE conv OF
	_MM_UPCONV_PS_NONE:	   RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

dst[511:0] := src[511:0]
loadOffset := 0
upSize := UPCONVERTSIZE(conv)
addr = MEM[mt]
FOR j := 0 to 15
	IF k[j]
		i := j*32
		dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
		loadOffset := loadOffset + 1
		IF (mt + loadOffset * upSize) % 64 == 0
			break
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2239</id>
<name>_MM512_MASK_EXTPACKSTOREHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHD</instr>
<desc>Down-converts and stores packed 32-bit integer elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped when the corresonding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*32
			tmp := DOWNCONVERT(v1[i+31:i], conv)
			storeAddr := addr + storeOffset * downSize
			CASE downSize OF
			4: MEM[storeAddr] := tmp[31:0]
			2: MEM[storeAddr] := tmp[15:0]
			1: MEM[storeAddr] := tmp[7:0]
			ESAC
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2241</id>
<name>_MM512_MASK_EXTPACKSTOREHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHQ</instr>
<desc>Down-converts and stores packed 64-bit integer elements of v1 into a quadword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (mt-64)). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped when the corresonding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN 8
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*64
			tmp := DOWNCONVERT(v1[i+63:i], conv)
			storeAddr := addr + storeOffset * downSize
			CASE downSize OF
			8: MEM[storeAddr] := tmp[63:0]
			ESAC
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2243</id>
<name>_MM512_MASK_EXTPACKSTOREHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHPD</instr>
<desc>Down-converts and stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN 8
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*64
			tmp := DOWNCONVERT(v1[i+63:i], conv)
			storeAddr := addr + storeOffset * downSize
			CASE downSize OF
			8: MEM[storeAddr] := tmp[63:0]
			ESAC
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2245</id>
<name>_MM512_MASK_EXTPACKSTOREHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT hint</sign>
<instr>VPACKSTOREHPS</instr>
<desc>Down-converts and stores packed single-precision (32-bit) floating-point elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
	_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
	_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:    RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

storeOffset := 0
foundNext64BytesBoundary := false
downSize := DOWNCONVERTSIZE(conv)
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*32
			tmp := DOWNCONVERT(v1[i+31:i], conv)
			storeAddr := addr + storeOffset * downSize
			CASE downSize OF
			4: MEM[storeAddr] := tmp[31:0]
			2: MEM[storeAddr] := tmp[15:0]
			1: MEM[storeAddr] := tmp[7:0]
			ESAC
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2247</id>
<name>_MM512_MASK_EXTPACKSTORELO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VPACKSTORELD</instr>
<desc>Down-converts and stores packed 32-bit integer elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal. Elements are written to memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
	_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI32_NONE:   RETURN 4
	_MM_UPCONV_EPI32_UINT8:  RETURN 1
	_MM_UPCONV_EPI32_SINT8:  RETURN 1
	_MM_UPCONV_EPI32_UINT16: RETURN 2
	_MM_UPCONV_EPI32_SINT16: RETURN 2
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 15
	IF k[j]
		i := j*32
		tmp := DOWNCONVERT(v1[i+31:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		4: MEM[storeAddr] := tmp[31:0]
		2: MEM[storeAddr] := tmp[15:0]
		1: MEM[storeAddr] := tmp[7:0]
		ESAC
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset * downSize) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2249</id>
<name>_MM512_MASK_EXTPACKSTORELO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VPACKSTORELQ</instr>
<desc>Down-converts and stores packed 64-bit integer elements of v1 into a quadword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped whent he corresponding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_EPI64_NONE: RETURN 8
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	IF k[j]
		i := j*63
		tmp := DOWNCONVERT(v1[i+63:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		8: MEM[storeAddr] := tmp[63:0]
		ESAC
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset * downSize) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2251</id>
<name>_MM512_MASK_EXTPACKSTORELO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT hint</sign>
<instr>VPACKSTORELPD</instr>
<desc>Down-converts and stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PD_NONE: RETURN 8
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 7
	IF k[j]
		i := j*63
		tmp := DOWNCONVERT(v1[i+63:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		8: MEM[storeAddr] := tmp[63:0]
		ESAC
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset * downSize) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2253</id>
<name>_MM512_MASK_EXTPACKSTORELO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT hint</sign>
<instr>VPACKSTORELPS</instr>
<desc>Down-converts and stores packed single-precision (32-bit) floating-point elements of v1 into a byte/word/doubleword stream according to conv at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). hint indicates to the processor whether the data is non-temporal. Elements are stored to memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>DOWNCONVERT(element, convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
	_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
	_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
	_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
	_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
	_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
	ESAC
}

DOWNCONVERTSIZE(convertTo) {
	CASE converTo OF
	_MM_UPCONV_PS_NONE:    RETURN 4
	_MM_UPCONV_PS_FLOAT16: RETURN 2
	_MM_UPCONV_PS_UINT8:   RETURN 1
	_MM_UPCONV_PS_SINT8:   RETURN 1
	_MM_UPCONV_PS_UINT16:  RETURN 2
	_MM_UPCONV_PS_SINT16:  RETURN 2
	ESAC
}

storeOffset := 0
downSize := DOWNCONVERTSIZE(conv)
addr = mt
FOR j := 0 to 15
	IF k[j]
		i := j*32
		tmp := DOWNCONVERT(v1[i+31:i], conv)
		storeAddr := addr + storeOffset * downSize
		CASE downSize OF
		4: MEM[storeAddr] := tmp[31:0]
		2: MEM[storeAddr] := tmp[15:0]
		1: MEM[storeAddr] := tmp[7:0]
		ESAC
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset * downSize) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2271</id>
<name>_MM512_MASK_EXTRACTF32X4_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__M128 src,__MMASK8 k,__M512 a,INT imm8</sign>
<instr>VEXTRACTF32X4</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2274</id>
<name>_MM512_MASK_EXTRACTF32X8_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512 a,INT imm8</sign>
<instr>VEXTRACTF32X8</instr>
<desc>Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2280</id>
<name>_MM512_MASK_EXTRACTF64X2_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128D src,__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VEXTRACTF64X2</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2283</id>
<name>_MM512_MASK_EXTRACTF64X4_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m256d</ret>
<sign>__M256D src,__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VEXTRACTF64X4</instr>
<desc>Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2290</id>
<name>_MM512_MASK_EXTRACTI32X4_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI32X4</instr>
<desc>Extract 128 bits (composed of 4 packed 32-bit integers) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2293</id>
<name>_MM512_MASK_EXTRACTI32X8_EPI32</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI32X8</instr>
<desc>Extract 256 bits (composed of 8 packed 32-bit integers) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2299</id>
<name>_MM512_MASK_EXTRACTI64X2_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__M128I src,__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI64X2</instr>
<desc>Extract 128 bits (composed of 2 packed 64-bit integers) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2302</id>
<name>_MM512_MASK_EXTRACTI64X4_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI64X4</instr>
<desc>Extract 256 bits (composed of 4 packed 64-bit integers) from a, selected with imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2305</id>
<name>_MM512_MASK_EXTSTORE_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512I v,_MM_DOWNCONV_EPI32_ENUM conv,INT hint</sign>
<instr>VMOVDQA32</instr>
<desc>Downconverts packed 32-bit integer elements stored in v to a smaller type depending on conv and stores them in memory location mt (elements in mt are unaltered when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_EPI32_NONE:
			addr[i+31:i] := v[i+31:i]
		_MM_DOWNCONV_EPI32_UINT8:
			n := j*8
			addr[n+7:n] := Int32ToUInt8(v[i+31:i])
		_MM_DOWNCONV_EPI32_SINT8:
			n := j*8
			addr[n+7:n] := Int32ToSInt8(v[i+31:i])
		_MM_DOWNCONV_EPI32_UINT16:
			n := j*16
			addr[n+15:n] := Int32ToUInt16(v[i+31:i])
		_MM_DOWNCONV_EPI32_SINT16:
			n := j*16
			addr[n+15:n] := Int32ToSInt16(v[i+31:i])
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2307</id>
<name>_MM512_MASK_EXTSTORE_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512I v,_MM_DOWNCONV_EPI64_ENUM conv,INT hint</sign>
<instr>VMOVDQA64</instr>
<desc>Downconverts packed 64-bit integer elements stored in v to a smaller type depending on conv and stores them in memory location mt (elements in mt are unaltered when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v[i+63:i]
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2309</id>
<name>_MM512_MASK_EXTSTORE_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512D v,_MM_DOWNCONV_PD_ENUM conv,INT hint</sign>
<instr>VMOVAPD</instr>
<desc>Downconverts packed double-precision (64-bit) floating-point elements stored in v to a smaller type depending on conv and stores them in memory location mt (elements in mt are unaltered when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>addr := MEM[mt]		
FOR j := 0 to 7
	i := j*64
	CASE conv OF
	_MM_DOWNCONV_PD_NONE:
		IF k[j]
			mt[i+63:i] := v[i+63:i]
		FI
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2311</id>
<name>_MM512_MASK_EXTSTORE_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512 v,_MM_DOWNCONV_PS_ENUM conv,INT hint</sign>
<instr>VMOVAPS</instr>
<desc>Downconverts packed single-precision (32-bit) floating-point elements stored in v to a smaller type depending on conv and stores them in memory location mt using writemask k (elements are not written to memory when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_PS_NONE:
			mt[i+31:i] := v[i+31:i]
		_MM_DOWNCONV_PS_FLOAT16:
			n := j*16
			mt[n+15:n] := Float32ToFloat16(v[i+31:i])
		_MM_DOWNCONV_PS_UINT8:
			n := j*8
			mt[n+7:n] := Float32ToUInt8(v[i+31:i])
		_MM_DOWNCONV_PS_SINT8:
			n := j*8
			mt[n+7:n] := Float32ToSInt8(v[i+31:i])
		_MM_DOWNCONV_PS_UINT16:
			n := j*16
			mt[n+15:n] := Float32ToUInt16(v[i+31:i])
		_MM_DOWNCONV_PS_SINT16:
			n := j*16
			mt[n+15:n] := Float32ToSInt16(v[i+31:i])
		ESAC
	 FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2319</id>
<name>_MM512_MASK_FIXUPIMM_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2328</id>
<name>_MM512_MASK_FIXUPIMM_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2331</id>
<name>_MM512_MASK_FIXUPIMM_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2334</id>
<name>_MM512_MASK_FIXUPIMM_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2349</id>
<name>_MM512_MASK_FIXUPNAN_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v1,__MMASK8 k,__M512D v2,__M512I v3</sign>
<instr>VFIXUPNANPD</instr>
<desc>Fixes up NaN's from packed double-precision (64-bit) floating-point elements in v1 and v2, storing the results in dst using writemask k (only elements whose corresponding mask bit is set are used in the computation). Quietized NaN's from v1 are stored in v3.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FixupNaNs(v1[i+63:i], v2[i+63:i])
		v3[i+63:i] := QuietizeNaNs(v1[i+63:i])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2351</id>
<name>_MM512_MASK_FIXUPNAN_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v1,__MMASK16 k,__M512 v2,__M512I v3</sign>
<instr>VFIXUPNANPS</instr>
<desc>Fixes up NaN's from packed single-precision (32-bit) floating-point elements in v1 and v2, storing the results in dst using writemask k (only elements whose corresponding mask bit is set are used in the computation). Quietized NaN's from v1 are stored in v3.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FixupNaNs(v1[i+31:i], v2[i+31:i])
		v3[i+31:i] := QuietizeNaNs(v1[i+31:i])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2355</id>
<name>_MM512_MASK_FLOOR_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a down to an integer value, and store the results as packed double-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FLOOR(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2359</id>
<name>_MM512_MASK_FLOOR_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a down to an integer value, and store the results as packed single-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FLOOR(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2363</id>
<name>_MM512_MASK_FMADD_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK16 k,__M512I b,__M512I c</sign>
<instr>VPMADD231D</instr>
<desc>Multiply packed 32-bit integer elements in a and b, add the intermediate result to packed elements in c and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2374</id>
<name>_MM512_MASK_FMADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2386</id>
<name>_MM512_MASK_FMADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2390</id>
<name>_MM512_MASK_FMADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c,INT rounding</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2394</id>
<name>_MM512_MASK_FMADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c,INT rounding</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2412</id>
<name>_MM512_MASK_FMADD233_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMADD233D</instr>
<desc>Multiply packed 32-bit integer elements in each 4-element set of a and by element 1 of the corresponding 4-element set from b, add the intermediate result to element 0 of the corresponding 4-element set from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		base := (j &amp; ~0x3) * 32
		scale[31:0] := b[base+63:base+32]
		bias[31:0]  := b[base+31:base]
		dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2414</id>
<name>_MM512_MASK_FMADD233_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VFMADD233PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in each 4-element set of a and by element 1 of the corresponding 4-element set from b, add the intermediate result to element 0 of the corresponding 4-element set from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		base := (j &amp; ~0x3) * 32
		scale[31:0] := b[base+63:base+32]
		bias[31:0]  := b[base+31:base]
		dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2416</id>
<name>_MM512_MASK_FMADD233_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VFMADD233PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in each 4-element set of a and by element 1 of the corresponding 4-element set from b, add the intermediate result to element 0 of the corresponding 4-element set from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		base := (j &amp; ~0x3) * 32
		scale[31:0] := b[base+63:base+32]
		bias[31:0]  := b[base+31:base]
		dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2426</id>
<name>_MM512_MASK_FMADDSUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2438</id>
<name>_MM512_MASK_FMADDSUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2442</id>
<name>_MM512_MASK_FMADDSUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2446</id>
<name>_MM512_MASK_FMADDSUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2458</id>
<name>_MM512_MASK_FMSUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2470</id>
<name>_MM512_MASK_FMSUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2474</id>
<name>_MM512_MASK_FMSUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c,INT rounding</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2478</id>
<name>_MM512_MASK_FMSUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c,INT rounding</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2504</id>
<name>_MM512_MASK_FMSUBADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2516</id>
<name>_MM512_MASK_FMSUBADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2520</id>
<name>_MM512_MASK_FMSUBADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2524</id>
<name>_MM512_MASK_FMSUBADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2536</id>
<name>_MM512_MASK_FNMADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2548</id>
<name>_MM512_MASK_FNMADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2552</id>
<name>_MM512_MASK_FNMADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c,INT rounding</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2556</id>
<name>_MM512_MASK_FNMADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c,INT rounding</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2582</id>
<name>_MM512_MASK_FNMSUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2594</id>
<name>_MM512_MASK_FNMSUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2598</id>
<name>_MM512_MASK_FNMSUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512D b,__M512D c,INT rounding</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2602</id>
<name>_MM512_MASK_FNMSUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512 b,__M512 c,INT rounding</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2624</id>
<name>_MM512_MASK_FPCLASS_PD_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512D a,INT imm8</sign>
<instr>VFPCLASSPD</instr>
<desc>Test packed double-precision (64-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2630</id>
<name>_MM512_MASK_FPCLASS_PS_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512 a,INT imm8</sign>
<instr>VFPCLASSPS</instr>
<desc>Test packed single-precision (32-bit) floating-point elements in a for special categories specified by imm8, and store the results in mask vector k using zeromask k1 (elements are zeroed out when the corresponding mask bit is not set).
	imm can be a combination of:    0x01 // QNaN
    0x02 // Positive Zero
    0x04 // Negative Zero
    0x08 // Positive Infinity
    0x10 // Negative Infinity
    0x20 // Denormal
    0x40 // Negative
    0x80 // SNaN
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2654</id>
<name>_MM512_MASK_GETEXP_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2663</id>
<name>_MM512_MASK_GETEXP_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2666</id>
<name>_MM512_MASK_GETEXP_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2669</id>
<name>_MM512_MASK_GETEXP_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2690</id>
<name>_MM512_MASK_GETMANT_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2699</id>
<name>_MM512_MASK_GETMANT_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2702</id>
<name>_MM512_MASK_GETMANT_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2705</id>
<name>_MM512_MASK_GETMANT_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2720</id>
<name>_MM512_MASK_GMAX_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VGMAXPD</instr>
<desc>Determines the maximum of each pair of corresponding elements of packed double-precision (64-bit) floating-point elements in a and b, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FpMax(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2722</id>
<name>_MM512_MASK_GMAX_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VGMAXPS</instr>
<desc>Determines the maximum of each pair of corresponding elements of packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FpMax(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2724</id>
<name>_MM512_MASK_GMAXABS_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VGMAXABSPS</instr>
<desc>Determines the maximum of the absolute elements of each pair of corresponding elements of packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2726</id>
<name>_MM512_MASK_GMIN_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VGMINPD</instr>
<desc>Determines the maximum of each pair of corresponding elements of packed double-precision (64-bit) floating-point elements in a and b, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FpMin(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2728</id>
<name>_MM512_MASK_GMIN_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VGMINPS</instr>
<desc>Determines the maximum of each pair of corresponding elements of packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FpMin(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2758</id>
<name>_MM512_MASK_HYPOT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2762</id>
<name>_MM512_MASK_HYPOT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>NONE</instr>
<desc>Compute the length of the hypotenous of a right triangle, with the lengths of the other two sides of the triangle stored as packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2764</id>
<name>_MM512_MASK_I32EXTGATHER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>VPGATHERDD</instr>
<desc>Up-converts 16 single-precision (32-bit) memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv to 32-bit integer elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_UPCONV_EPI32_NONE:
			dst[i+31:i] := addr[i+31:i]
		_MM_UPCONV_EPI32_UINT8:
			n := j*7
			dst[i+31:i] := UInt8ToUInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_SINT8:
			n := j*7
			dst[i+31:i] := Int8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_UINT16:
			n := j*16
			dst[i+31:i] := UInt16ToUInt32(addr[n+15:n])
		_MM_UPCONV_EPI32_SINT16:
			n := j*16
			dst[i+31:i] := Int16ToInt32(addr[n+15:n])
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2766</id>
<name>_MM512_MASK_I32EXTGATHER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VGATHERDPS</instr>
<desc>Up-converts 16 single-precision (32-bit) memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv to single-precision (32-bit) floating-point elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_UPCONV_PS_NONE:
			dst[i+31:i] := addr[i+31:i]
		_MM_UPCONV_PS_FLOAT16:
			n := j*16
			dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_UINT8:
			n := j*8
			dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_SINT8:
			n := j*8
			dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_UINT16:
			n := j*16
			dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_SINT16:
			n := j*16
			dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2768</id>
<name>_MM512_MASK_I32EXTSCATTER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK16 k,__M512I index,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>VPSCATTERDD</instr>
<desc>Down-converts 16 packed 32-bit integer elements in v1 using conv and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale. Elements are written using writemask k (elements are only written when the corresponding mask bit is set; otherwise, elements are left unchanged in memory). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_EPI32_NONE:
			addr[i+31:i] := v1[i+31:i]
		_MM_DOWNCONV_EPI32_UINT8:
			n := j*8
			addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
		_MM_DOWNCONV_EPI32_SINT8:
			n := j*8
			addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
		_MM_DOWNCONV_EPI32_UINT16:
			n := j*16
			addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
		_MM_DOWNCONV_EPI32_SINT16:
			n := j*16
			addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
		ESAC
	FI 
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2770</id>
<name>_MM512_MASK_I32EXTSCATTER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK16 k,__M512I index,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VSCATTERDPS</instr>
<desc>Down-converts 16 packed single-precision (32-bit) floating-point elements in v1 according to conv and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using writemask k (elements are written only when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		CASE conv OF
		_MM_DOWNCONV_PS_NONE:
			n := j*32
			addr[i+31:i] := v1[n+31:n]
		_MM_DOWNCONV_PS_FLOAT16:
			i := j*16
			addr[i+15:i] := Float32ToFloat16(v1[n+31:n])
		_MM_DOWNCONV_PS_UINT8:
			i := j*8
			addr[i+7:i] := Float32ToUInt8(v1[n+31:n])
		_MM_DOWNCONV_PS_SINT8:
			i := j*8
			addr[i+7:i] := Float32ToSInt8(v1[n+31:n])
		_MM_DOWNCONV_PS_UINT16:
			i := j*8
			addr[i+15:i] := Float32ToUInt16(v1[n+31:n])
		_MM_DOWNCONV_PS_SINT16:
			i := j*8
			addr[i+15:i] := Float32ToSInt16(v1[n+31:n])
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2778</id>
<name>_MM512_MASK_I32GATHER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERDD</instr>
<desc>
	Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:16] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2786</id>
<name>_MM512_MASK_I32GATHER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	m := j*32
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2794</id>
<name>_MM512_MASK_I32GATHER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M256I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	m := j*32
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2802</id>
<name>_MM512_MASK_I32GATHER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERDPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:16] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2804</id>
<name>_MM512_MASK_I32LOEXTGATHER_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>VPGATHERDQ</instr>
<desc>Up-converts 8 double-precision (64-bit) memory locations starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale using conv to 64-bit integer elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	IF k[j]
		CASE conv OF
		_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2806</id>
<name>_MM512_MASK_I32LOEXTGATHER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>VGATHERDPD</instr>
<desc>Up-converts 8 double-precision (64-bit) floating-point elements in memory locations starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale using conv to 64-bit floating-point elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	IF k[j]
		CASE conv OF
		_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2808</id>
<name>_MM512_MASK_I32LOEXTSCATTER_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>VPSCATTERDQ</instr>
<desc>Down-converts 8 packed 64-bit integer elements in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv. Only those elements whose corresponding mask bit is set in writemask k are written to memory.</desc>
<oper>FOR j := 0 to 7
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		i := j*64
		CASE conv OF
		_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2810</id>
<name>_MM512_MASK_I32LOEXTSCATTER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>VSCATTERDPD</instr>
<desc>Down-converts 8 packed double-precision (64-bit) floating-point elements in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using conv. Only those elements whose corresponding mask bit is set in writemask k are written to memory.</desc>
<oper>FOR j := 0 to 7
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		i := j*64
		CASE conv OF
		_MM_DOWNCONV_PD_NONE: addr[i+63:i] := v1[i+63:i]
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2812</id>
<name>_MM512_MASK_I32LOGATHER_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>VPGATHERDQ</instr>
<desc>Loads 8 64-bit integer elements from memory starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		dst[i+63:i] := addr[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2814</id>
<name>_MM512_MASK_I32LOGATHER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>VGATHERDPD</instr>
<desc>Loads 8 double-precision (64-bit) floating-point elements from memory starting at location mv at packed 32-bit integer indices stored in the lower half of index scaled by scale into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		dst[i+63:i] := addr[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2816</id>
<name>_MM512_MASK_I32LOSCATTER_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512I v1,INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Stores 8 packed 64-bit integer elements located in v1 and stores them in memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale using writemask k (elements whose corresponding mask bit is not set are not written to memory).</desc>
<oper>FOR j := 0 to 7
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		addr[i+63:i] := v1[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2818</id>
<name>_MM512_MASK_I32LOSCATTER_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512D v1,INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Stores 8 packed double-precision (64-bit) floating-point elements in v1 to memory locations starting at location mv at packed 32-bit integer indices stored in index scaled by scale. Only those elements whose corresponding mask bit is set in writemask k are written to memory.</desc>
<oper>FOR j := 0 to 7
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		i := j*64
		addr[i+63:i] := v1[k+63:j]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2824</id>
<name>_MM512_MASK_I32SCATTER_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I vindex,__M512I a,INT scale</sign>
<instr>VPSCATTERDD</instr>
<desc>Scatter 32-bit integers from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2830</id>
<name>_MM512_MASK_I32SCATTER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M512I a,INT scale</sign>
<instr>VPSCATTERDQ</instr>
<desc>Scatter 64-bit integers from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2836</id>
<name>_MM512_MASK_I32SCATTER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M256I vindex,__M512D a,INT scale</sign>
<instr>VSCATTERDPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2842</id>
<name>_MM512_MASK_I32SCATTER_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK16 k,__M512I vindex,__M512 a,INT scale</sign>
<instr>VSCATTERDPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>2844</id>
<name>_MM512_MASK_I64EXTGATHER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 single-precision (32-bit) memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to 32-bit integer elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_UPCONV_EPI32_NONE:
			dst[i+31:i] := addr[i+31:i]
		_MM_UPCONV_EPI32_UINT8:
			n := j*8
			dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_SINT8:
			n := j*8
			dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
		_MM_UPCONV_EPI32_UINT16:
			n := j*16
			dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
		_MM_UPCONV_EPI32_SINT16:
			n := j*16
			dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2846</id>
<name>_MM512_MASK_I64EXTGATHER_EPI64</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 double-precision (64-bit) memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to 64-bit integer elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the load is non-temporal.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		CASE conv OF
		_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2848</id>
<name>_MM512_MASK_I64EXTGATHER_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 double-precision (64-bit) floating-point elements stored in memory starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to 64-bit floating-point elements and stores them in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	IF k[j]
		CASE conv OF
		_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2850</id>
<name>_MM512_MASK_I64EXTGATHER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Up-converts 8 memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using conv to single-precision (32-bit) floating-point elements and stores them in the lower half of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). hint indicates to the processor whether the load is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_UPCONV_PS_NONE:
			dst[i+31:i] := addr[i+31:i]
		_MM_UPCONV_PS_FLOAT16:
			n := j*16
			dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_UINT8:
			n := j*8
			dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_SINT8:
			n := j*8
			dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
		_MM_UPCONV_PS_UINT16:
			n := j*16
			dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
		_MM_UPCONV_PS_SINT16:
			n := j*16
			dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2852</id>
<name>_MM512_MASK_I64EXTSCATTER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512I v1,_MM_DOWNCONV_EPI32_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts the low 8 packed 32-bit integer elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. Elements are written to memory using writemask k (elements are only written when the corresponding mask bit is set; otherwise, the memory location is left unchanged). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_EPI32_NONE:
			addr[i+31:i] := v1[i+31:i]
		_MM_DOWNCONV_EPI32_UINT8:
			n := j*8
			addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
		_MM_DOWNCONV_EPI32_SINT8:
			n := j*8
			addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
		_MM_DOWNCONV_EPI32_UINT16:
			n := j*16
			addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
		_MM_DOWNCONV_EPI32_SINT16:
			n := j*16
			addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2854</id>
<name>_MM512_MASK_I64EXTSCATTER_EPI64</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512I v1,_MM_DOWNCONV_EPI64_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts 8 packed 64-bit integer elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. Only those elements whose corresponding mask bit is set in writemask k are written to memory.</desc>
<oper>FOR j := 0 to 7
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		i := j*64
		CASE conv OF
		_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2856</id>
<name>_MM512_MASK_I64EXTSCATTER_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512D v1,_MM_DOWNCONV_PD_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts 8 packed double-precision (64-bit) floating-point elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. Elements are written to memory using writemask k (elements are not stored to memory when the corresponding mask bit is not set; the memory location is left unchagned). hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*64
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_EPI64_NONE:
			addr[i+63:i] := v1[i+63:i]
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2858</id>
<name>_MM512_MASK_I64EXTSCATTER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512 v1,_MM_DOWNCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>NONE</instr>
<desc>Down-converts 8 packed single-precision (32-bit) floating-point elements in v1 using conv and stores them in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale. Elements are only written when the corresponding mask bit is set in k; otherwise, elements are unchanged in memory. hint indicates to the processor whether the data is non-temporal.</desc>
<oper>FOR j := 0 to 7
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j]
		CASE conv OF
		_MM_DOWNCONV_PS_NONE:
			addr[i+31:i] := v[i+31:i]
		_MM_DOWNCONV_PS_FLOAT16:
			n := j*16
			addr[n+15:n] := Float32ToFloat16(v1[i+31:i])
		_MM_DOWNCONV_PS_UINT8:
			n := j*8
			addr[n+7:n] := Float32ToUInt8(v1[i+31:i])
		_MM_DOWNCONV_PS_SINT8:
			n := j*8
			addr[n+7:n] := Float32ToSInt8(v1[i+31:i])
		_MM_DOWNCONV_PS_UINT16:
			n := j*16
			addr[n+15:n] := Float32ToUInt16(v1[i+31:i])
		_MM_DOWNCONV_PS_SINT16:
			n := j*16
			addr[n+15:n] := Float32ToSInt16(v1[i+31:i])
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2866</id>
<name>_MM512_MASK_I64GATHER_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__M256I src,__MMASK8 k,__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERQD</instr>
<desc>Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2868</id>
<name>_MM512_MASK_I64GATHER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>NONE</instr>
<desc>Loads 8 32-bit integer memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale to dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		dst[i+31:i] := addr[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2876</id>
<name>_MM512_MASK_I64GATHER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VPGATHERQQ</instr>
<desc>Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2884</id>
<name>_MM512_MASK_I64GATHER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERQPD</instr>
<desc>
	Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2892</id>
<name>_MM512_MASK_I64GATHER_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__M256 src,__MMASK8 k,__M512I vindex,VOID_CONST_PTR base_addr,INT scale</sign>
<instr>VGATHERQPS</instr>
<desc>
	Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	IF k[j]
		dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
		k[j] := 0
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
k[MAX:8] := 0
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2894</id>
<name>_MM512_MASK_I64GATHER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK8 k,__M512I index,VOID_CONST_PTR mv,INT scale</sign>
<instr>NONE</instr>
<desc>Loads 8 single-precision (32-bit) floating-point memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale to dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		dst[i+31:i] := addr[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2900</id>
<name>_MM512_MASK_I64SCATTER_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I vindex,__M256I a,INT scale</sign>
<instr>VPSCATTERQD</instr>
<desc>Scatter 32-bit integers from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2902</id>
<name>_MM512_MASK_I64SCATTER_EPI32LO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512I v1,INT scale</sign>
<instr>NONE</instr>
<desc>Stores 8 packed 32-bit integer elements in v1 in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using writemask k (elements are only written to memory when the corresponding mask bit is set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		addr[i+31:i] := v1[i+31:i]
	FI	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2908</id>
<name>_MM512_MASK_I64SCATTER_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I vindex,__M512I a,INT scale</sign>
<instr>VPSCATTERQQ</instr>
<desc>Scatter 64-bit integers from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2914</id>
<name>_MM512_MASK_I64SCATTER_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I vindex,__M512D a,INT scale</sign>
<instr>VSCATTERQPD</instr>
<desc>Scatter double-precision (64-bit) floating-point elements from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2920</id>
<name>_MM512_MASK_I64SCATTER_PS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 k,__M512I vindex,__M256 a,INT scale</sign>
<instr>VSCATTERQPS</instr>
<desc>Scatter single-precision (32-bit) floating-point elements from a into memory using 64-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not stored when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>2922</id>
<name>_MM512_MASK_I64SCATTER_PSLO</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK8 k,__M512I index,__M512 v1,INT scale</sign>
<instr>NONE</instr>
<desc>Stores 8 packed single-precision (32-bit) floating-point elements in v1 in memory locations starting at location mv at packed 64-bit integer indices stored in index scaled by scale using writemask k (elements are only written to memory when the corresponding mask bit is set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		addr[i+31:i] := v1[i+31:i]
	FI	
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>2944</id>
<name>_MM512_MASK_INSERTF32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M128 b,INT imm8</sign>
<instr>VINSERTF32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2947</id>
<name>_MM512_MASK_INSERTF32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M256 b,INT imm8</sign>
<instr>VINSERTF32X8</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[7:0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2953</id>
<name>_MM512_MASK_INSERTF64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M128D b,INT imm8</sign>
<instr>VINSERTF64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2956</id>
<name>_MM512_MASK_INSERTF64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M256D b,INT imm8</sign>
<instr>VINSERTF64X4</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[0]) of
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2963</id>
<name>_MM512_MASK_INSERTI32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M128I b,INT imm8</sign>
<instr>VINSERTI32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed 32-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2966</id>
<name>_MM512_MASK_INSERTI32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M256I b,INT imm8</sign>
<instr>VINSERTI32X8</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 8 packed 32-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[7:0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2972</id>
<name>_MM512_MASK_INSERTI64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M128I b,INT imm8</sign>
<instr>VINSERTI64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed 64-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2975</id>
<name>_MM512_MASK_INSERTI64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M256I b,INT imm8</sign>
<instr>VINSERTI64X4</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 4 packed 64-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[0]) of
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2987</id>
<name>_MM512_MASK_INVSQRT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := InvSQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2991</id>
<name>_MM512_MASK_INVSQRT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the inverse square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := InvSQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3031</id>
<name>_MM512_MASK_LOAD_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3038</id>
<name>_MM512_MASK_LOAD_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load packed 64-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3047</id>
<name>_MM512_MASK_LOAD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3057</id>
<name>_MM512_MASK_LOAD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3086</id>
<name>_MM512_MASK_LOADU_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU16</instr>
<desc>Load packed 16-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3092</id>
<name>_MM512_MASK_LOADU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3098</id>
<name>_MM512_MASK_LOADU_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU64</instr>
<desc>Load packed 64-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3104</id>
<name>_MM512_MASK_LOADU_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU8</instr>
<desc>Load packed 8-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). 
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3113</id>
<name>_MM512_MASK_LOADU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memoy into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3122</id>
<name>_MM512_MASK_LOADU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3137</id>
<name>_MM512_MASK_LOADUNPACKHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHD</instr>
<desc>Loads the high-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt-64 and expands them into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*4 % 64) == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*32
			tmp := MEM[addr + loadOffset*4]
			dst[i+31:i] := tmp[i+31:i]
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3139</id>
<name>_MM512_MASK_LOADUNPACKHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHQ</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64 and expands them into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*8) == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*64
			tmp := MEM[addr + loadOffset*8]
			dst[i+63:i] := tmp[i+63:i]
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3141</id>
<name>_MM512_MASK_LOADUNPACKHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHPD</instr>
<desc>Loads the high-64-byte-aligned portion of the quadword stream starting at element-aligned address mt-64 and expands them into packed double-precision (64-bit) floating-point values in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*8) % 64 == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*64
			tmp := MEM[addr + loadOffset*8]
			dst[i+63:i] := tmp[i+63:i]
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3143</id>
<name>_MM512_MASK_LOADUNPACKHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKHPS</instr>
<desc>Loads the high-64-byte-aligned portion of the doubleword stream starting at element-aligned address mt-64 and expands them into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted quadwords that occur at or after the first 64-byte-aligned address following (mt-64) are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF (addr + (loadOffset + 1)*4 % 64) == 0
				foundNext64BytesBoundary := true
			FI
		ELSE
			i := j*32
			tmp := MEM[addr + loadOffset*4]
			dst[i+31:i] := tmp[i+31:i]
		FI
		loadOffset := loadOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3145</id>
<name>_MM512_MASK_LOADUNPACKLO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLD</instr>
<desc>Loads the low-64-byte-aligned portion of the byte/word/doubleword stream starting at element-aligned address mt and expands them into packed 32-bit integers in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 15
	i := j*32
	IF k[j]
		tmp := MEM[addr + loadOffset*4]
		dst[i+31:i] := tmp[i+31:i]
		loadOffset := loadOffset + 1
		IF (mt + loadOffset * 4) % 64 == 0
			break
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3147</id>
<name>_MM512_MASK_LOADUNPACKLO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLQ</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt and expands them into packed 64-bit integers in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp := MEM[addr + loadOffset*8]
		dst[i+63:i] := tmp[i+63:i]
		loadOffset := loadOffset + 1
		IF (addr + loadOffset*8 % 64) == 0
			break
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3149</id>
<name>_MM512_MASK_LOADUNPACKLO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLPD</instr>
<desc>Loads the low-64-byte-aligned portion of the quadword stream starting at element-aligned address mt and expands them into packed double-precision (64-bit) floating-point values in dst. The initial values of dst are copied from src. Only those converted quad that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those quadwords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp := MEM[addr + loadOffset*8]
		dst[i+63:i] := tmp[i+63:i]
		loadOffset := loadOffset + 1
		IF ((addr + 8*loadOffset) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3151</id>
<name>_MM512_MASK_LOADUNPACKLO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,VOID_CONST_PTR mt</sign>
<instr>VLOADUNPACKLPS</instr>
<desc>Loads the low-64-byte-aligned portion of the doubleword stream starting at element-aligned address mt and expanded into packed single-precision (32-bit) floating-point elements in dst. The initial values of dst are copied from src. Only those converted doublewords that occur before first 64-byte-aligned address following mt are loaded. Elements in the resulting vector that do not map to those doublewords are taken from src. Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>dst[511:0] := src[511:0]
loadOffset := 0
addr = mt
FOR j := 0 to 15
	i := j*32
	IF k[j]
		tmp := MEM[addr + loadOffset*4]
		dst[i+31:i] := tmp[i+31:i]
		loadOffset := loadOffset + 1
		IF (mt + loadOffset * 4) % 64 == 0
			break
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3155</id>
<name>_MM512_MASK_LOG_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ln(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3159</id>
<name>_MM512_MASK_LOG_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ln(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3163</id>
<name>_MM512_MASK_LOG10_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := log10(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3167</id>
<name>_MM512_MASK_LOG10_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the base-10 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := log10(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3171</id>
<name>_MM512_MASK_LOG1P_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ln(1.0 + a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3175</id>
<name>_MM512_MASK_LOG1P_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the natural logarithm of one plus packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ln(1.0 + a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3179</id>
<name>_MM512_MASK_LOG2_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the base-2 logarithm of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := log2(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3183</id>
<name>_MM512_MASK_LOG2_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VLOG2PS</instr>
<desc>Compute the base-2 logarithm of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := log2(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3185</id>
<name>_MM512_MASK_LOG2AE23_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VLOG2PS</instr>
<desc>Compute the base-2 logarithm of packed single-precision (32-bit) floating-point elements in a with absolute error of 2^(-23) and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Log2ae23(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3189</id>
<name>_MM512_MASK_LOGB_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3193</id>
<name>_MM512_MASK_LOGB_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision floating-point number representing the integer exponent, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3203</id>
<name>_MM512_MASK_LZCNT_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		tmp := 31
		dst[i+31:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+31:i] := dst[i+31:i] + 1
		OD
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3212</id>
<name>_MM512_MASK_LZCNT_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp := 63
		dst[i+63:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+63:i] := dst[i+63:i] + 1
		OD
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3223</id>
<name>_MM512_MASK_MADD_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3233</id>
<name>_MM512_MASK_MADD52HI_EPU64</name>
<cpuid>AVX512IFMA52</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK8 k,__M512I b,__M512I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3242</id>
<name>_MM512_MASK_MADD52LO_EPU64</name>
<cpuid>AVX512IFMA52</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK8 k,__M512I b,__M512I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3251</id>
<name>_MM512_MASK_MADDUBS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Multiply packed unsigned 8-bit integers in a by packed signed 8-bit integers in b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3282</id>
<name>_MM512_MASK_MAX_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3291</id>
<name>_MM512_MASK_MAX_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3300</id>
<name>_MM512_MASK_MAX_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3309</id>
<name>_MM512_MASK_MAX_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3318</id>
<name>_MM512_MASK_MAX_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3327</id>
<name>_MM512_MASK_MAX_EPU32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3336</id>
<name>_MM512_MASK_MAX_EPU64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3345</id>
<name>_MM512_MASK_MAX_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3354</id>
<name>_MM512_MASK_MAX_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3364</id>
<name>_MM512_MASK_MAX_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3368</id>
<name>_MM512_MASK_MAX_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT sae</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3371</id>
<name>_MM512_MASK_MAX_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT sae</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3386</id>
<name>_MM512_MASK_MAXABS_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VGMAXABSPS</instr>
<desc>Determines the maximum of the absolute elements of each pair of corresponding elements of packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3396</id>
<name>_MM512_MASK_MIN_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3405</id>
<name>_MM512_MASK_MIN_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
				dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3414</id>
<name>_MM512_MASK_MIN_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3423</id>
<name>_MM512_MASK_MIN_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3432</id>
<name>_MM512_MASK_MIN_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3441</id>
<name>_MM512_MASK_MIN_EPU32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3450</id>
<name>_MM512_MASK_MIN_EPU64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3459</id>
<name>_MM512_MASK_MIN_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3468</id>
<name>_MM512_MASK_MIN_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3478</id>
<name>_MM512_MASK_MIN_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3482</id>
<name>_MM512_MASK_MIN_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT sae</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3485</id>
<name>_MM512_MASK_MIN_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT sae</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3506</id>
<name>_MM512_MASK_MOV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a</sign>
<instr>VMOVDQU16</instr>
<desc>Move packed 16-bit integers from a into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3512</id>
<name>_MM512_MASK_MOV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a</sign>
<instr>VMOVDQA32</instr>
<desc>Move packed 32-bit integers from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3518</id>
<name>_MM512_MASK_MOV_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a</sign>
<instr>VMOVDQA64</instr>
<desc>Move packed 64-bit integers from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3524</id>
<name>_MM512_MASK_MOV_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a</sign>
<instr>VMOVDQU8</instr>
<desc>Move packed 8-bit integers from a into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3530</id>
<name>_MM512_MASK_MOV_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VMOVAPD</instr>
<desc>Move packed double-precision (64-bit) floating-point elements from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3536</id>
<name>_MM512_MASK_MOV_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VMOVAPS</instr>
<desc>Move packed single-precision (32-bit) floating-point elements from a to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3551</id>
<name>_MM512_MASK_MOVEDUP_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
tmp[319:256] := a[319:256] 
tmp[383:320] := a[319:256] 
tmp[447:384] := a[447:384]
tmp[511:448] := a[447:384]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3560</id>
<name>_MM512_MASK_MOVEHDUP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
tmp[287:256] := a[319:288] 
tmp[319:288] := a[319:288] 
tmp[351:320] := a[383:352] 
tmp[383:352] := a[383:352] 
tmp[415:384] := a[447:416] 
tmp[447:416] := a[447:416] 
tmp[479:448] := a[511:480]
tmp[511:480] := a[511:480]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3570</id>
<name>_MM512_MASK_MOVELDUP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
tmp[287:256] := a[287:256] 
tmp[319:288] := a[287:256] 
tmp[351:320] := a[351:320] 
tmp[383:352] := a[351:320] 
tmp[415:384] := a[415:384] 
tmp[447:416] := a[415:384] 
tmp[479:448] := a[479:448]
tmp[511:480] := a[479:448]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3615</id>
<name>_MM512_MASK_MUL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3624</id>
<name>_MM512_MASK_MUL_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3633</id>
<name>_MM512_MASK_MUL_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  RM.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3642</id>
<name>_MM512_MASK_MUL_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  RM.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3645</id>
<name>_MM512_MASK_MUL_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).  
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3648</id>
<name>_MM512_MASK_MUL_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	 Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3670</id>
<name>_MM512_MASK_MULHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3673</id>
<name>_MM512_MASK_MULHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMULHD</instr>
<desc>Performs element-by-element multiplication between packed 32-bit integer elements in a and b and stores the high 32 bits of each result into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) &amp;gt;&amp;gt; 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3681</id>
<name>_MM512_MASK_MULHI_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3684</id>
<name>_MM512_MASK_MULHI_EPU32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMULHUD</instr>
<desc>Performs element-by-element multiplication between packed unsigned 32-bit integer elements in a and b and stores the high 32 bits of each result into dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) &amp;gt;&amp;gt; 32
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3694</id>
<name>_MM512_MASK_MULHRS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
		dst[i+15:i] := tmp[16:1]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3704</id>
<name>_MM512_MASK_MULLO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3713</id>
<name>_MM512_MASK_MULLO_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		tmp[63:0] := a[i+31:i] * b[i+31:i]
		dst[i+31:i] := tmp[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3722</id>
<name>_MM512_MASK_MULLO_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp[127:0] := a[i+63:i] * b[i+63:i]
		dst[i+63:i] := tmp[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3726</id>
<name>_MM512_MASK_MULLOX_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Multiplies elements in packed 64-bit integer vectors a and b together, storing the lower 64 bits of the result in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3734</id>
<name>_MM512_MASK_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR i := 0 to 7
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		IF k[i*8+j]
			dst[q+j*8+7:q+j*8] := tmp8[7:0]
		ELSE
			dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
		FI
	ENDFOR
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3738</id>
<name>_MM512_MASK_NEARBYINT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Rounds each packed double-precision (64-bit) floating-point element in a to the nearest integer value and stores the results as packed double-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := NearbyInt(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3740</id>
<name>_MM512_MASK_NEARBYINT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Rounds each packed single-precision (32-bit) floating-point element in a to the nearest integer value and stores the results as packed double-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := NearbyInt(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3746</id>
<name>_MM512_MASK_OR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] OR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3753</id>
<name>_MM512_MASK_OR_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] OR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3762</id>
<name>_MM512_MASK_OR_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3771</id>
<name>_MM512_MASK_OR_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3784</id>
<name>_MM512_MASK_PACKS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
tmp_dst[263:256] := Saturate_Int16_To_Int8 (a[271:256])
tmp_dst[271:264] := Saturate_Int16_To_Int8 (a[287:272])
tmp_dst[279:272] := Saturate_Int16_To_Int8 (a[303:288])
tmp_dst[287:280] := Saturate_Int16_To_Int8 (a[319:304])
tmp_dst[295:288] := Saturate_Int16_To_Int8 (a[335:320])
tmp_dst[303:296] := Saturate_Int16_To_Int8 (a[351:336])
tmp_dst[311:304] := Saturate_Int16_To_Int8 (a[367:352])
tmp_dst[319:312] := Saturate_Int16_To_Int8 (a[383:368])
tmp_dst[327:320] := Saturate_Int16_To_Int8 (b[271:256])
tmp_dst[335:328] := Saturate_Int16_To_Int8 (b[287:272])
tmp_dst[343:336] := Saturate_Int16_To_Int8 (b[303:288])
tmp_dst[351:344] := Saturate_Int16_To_Int8 (b[319:304])
tmp_dst[359:352] := Saturate_Int16_To_Int8 (b[335:320])
tmp_dst[367:360] := Saturate_Int16_To_Int8 (b[351:336])
tmp_dst[375:368] := Saturate_Int16_To_Int8 (b[367:352])
tmp_dst[383:376] := Saturate_Int16_To_Int8 (b[383:368])
tmp_dst[391:384] := Saturate_Int16_To_Int8 (a[399:384])
tmp_dst[399:392] := Saturate_Int16_To_Int8 (a[415:400])
tmp_dst[407:400] := Saturate_Int16_To_Int8 (a[431:416])
tmp_dst[415:408] := Saturate_Int16_To_Int8 (a[447:432])
tmp_dst[423:416] := Saturate_Int16_To_Int8 (a[463:448])
tmp_dst[431:424] := Saturate_Int16_To_Int8 (a[479:464])
tmp_dst[439:432] := Saturate_Int16_To_Int8 (a[495:480])
tmp_dst[447:440] := Saturate_Int16_To_Int8 (a[511:496])
tmp_dst[455:448] := Saturate_Int16_To_Int8 (b[399:384])
tmp_dst[463:456] := Saturate_Int16_To_Int8 (b[415:400])
tmp_dst[471:464] := Saturate_Int16_To_Int8 (b[431:416])
tmp_dst[479:472] := Saturate_Int16_To_Int8 (b[447:432])
tmp_dst[487:480] := Saturate_Int16_To_Int8 (b[463:448])
tmp_dst[495:488] := Saturate_Int16_To_Int8 (b[479:464])
tmp_dst[503:496] := Saturate_Int16_To_Int8 (b[495:480])
tmp_dst[511:504] := Saturate_Int16_To_Int8 (b[511:496])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3793</id>
<name>_MM512_MASK_PACKS_EPI32</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
tmp_dst[271:256] := Saturate_Int32_To_Int16 (a[287:256])
tmp_dst[287:272] := Saturate_Int32_To_Int16 (a[319:288])
tmp_dst[303:288] := Saturate_Int32_To_Int16 (a[351:320])
tmp_dst[319:304] := Saturate_Int32_To_Int16 (a[383:352])
tmp_dst[335:320] := Saturate_Int32_To_Int16 (b[287:256])
tmp_dst[351:336] := Saturate_Int32_To_Int16 (b[319:288])
tmp_dst[367:352] := Saturate_Int32_To_Int16 (b[351:320])
tmp_dst[383:368] := Saturate_Int32_To_Int16 (b[383:352])
tmp_dst[399:384] := Saturate_Int32_To_Int16 (a[415:384])
tmp_dst[415:400] := Saturate_Int32_To_Int16 (a[447:416])
tmp_dst[431:416] := Saturate_Int32_To_Int16 (a[479:448])
tmp_dst[447:432] := Saturate_Int32_To_Int16 (a[511:480])
tmp_dst[463:448] := Saturate_Int32_To_Int16 (b[415:384])
tmp_dst[479:464] := Saturate_Int32_To_Int16 (b[447:416])
tmp_dst[495:480] := Saturate_Int32_To_Int16 (b[479:448])
tmp_dst[511:496] := Saturate_Int32_To_Int16 (b[511:480])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3801</id>
<name>_MM512_MASK_PACKSTOREHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512I v1</sign>
<instr>VPACKSTOREHD</instr>
<desc>Stores packed 32-bit integer elements of v1 into a doubleword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elements of the stream that map at or after the first 64-byte-aligned address following (m5-64)). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*4) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*32
			MEM[addr + storeOffset*4] := v1[i+31:i]
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3803</id>
<name>_MM512_MASK_PACKSTOREHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512I v1</sign>
<instr>VPACKSTOREHQ</instr>
<desc>Stores packed 64-bit integer elements of v1 into a quadword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*8) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*64
			MEM[addr + storeOffset*8] := v1[i+63:i]
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3805</id>
<name>_MM512_MASK_PACKSTOREHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512D v1</sign>
<instr>VPACKSTOREHPD</instr>
<desc>Stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*8) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*64
			MEM[addr + storeOffset*4] := v1[i+63:i]
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3807</id>
<name>_MM512_MASK_PACKSTOREHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512 v1</sign>
<instr>VPACKSTOREHPS</instr>
<desc>Stores packed single-precision (32-bit) floating-point elements of v1 into a doubleword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF k[j]
		IF foundNext64BytesBoundary == false
			IF ((addr + (storeOffset + 1)*4) % 64) == 0
				foundNext64BytesBoundary = true
			FI
		ELSE
			i := j*32
			MEM[addr + storeOffset*4] := v1[i+31:i]
		FI
		storeOffset := storeOffset + 1
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3809</id>
<name>_MM512_MASK_PACKSTORELO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512I v1</sign>
<instr>VPACKSTORELD</instr>
<desc>Stores packed 32-bit integer elements of v1 into a doubleword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 15
	IF k[j]
		i := j*32
		MEM[addr + storeOffset*4] := v1[i+31:i]
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset*4) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3811</id>
<name>_MM512_MASK_PACKSTORELO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512I v1</sign>
<instr>VPACKSTORELQ</instr>
<desc>Stores packed 64-bit integer elements of v1 into a quadword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 7
	IF k[j]
		i := j*64
		MEM[addr + storeOffset*8] := v1[i+63:i]
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset*8) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3813</id>
<name>_MM512_MASK_PACKSTORELO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK8 k,__M512D v1</sign>
<instr>VPACKSTORELPD</instr>
<desc>Stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 7
	IF k[j]
		i := j*64
		MEM[addr + storeOffset*8] := v1[i+63:i]
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset*8) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3815</id>
<name>_MM512_MASK_PACKSTORELO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__MMASK16 k,__M512 v1</sign>
<instr>VPACKSTORELPS</instr>
<desc>Stores packed single-precision (32-bit) floating-point elements of v1 into a doubleword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt). Elements are loaded from memory according to element selector k (elements are skipped when the corresponding mask bit is not set).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 15
	IF k[j]
		i := j*32
		MEM[addr + storeOffset*4] := v1[i+31:i]
		storeOffset := storeOffset + 1
		IF ((addr + storeOffset*4) % 64) == 0
			BREAK
		FI
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3823</id>
<name>_MM512_MASK_PACKUS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
tmp_dst[263:256] := Saturate_Int16_To_UnsignedInt8 (a[271:256])
tmp_dst[271:264] := Saturate_Int16_To_UnsignedInt8 (a[287:272])
tmp_dst[279:272] := Saturate_Int16_To_UnsignedInt8 (a[303:288])
tmp_dst[287:280] := Saturate_Int16_To_UnsignedInt8 (a[319:304])
tmp_dst[295:288] := Saturate_Int16_To_UnsignedInt8 (a[335:320])
tmp_dst[303:296] := Saturate_Int16_To_UnsignedInt8 (a[351:336])
tmp_dst[311:304] := Saturate_Int16_To_UnsignedInt8 (a[367:352])
tmp_dst[319:312] := Saturate_Int16_To_UnsignedInt8 (a[383:368])
tmp_dst[327:320] := Saturate_Int16_To_UnsignedInt8 (b[271:256])
tmp_dst[335:328] := Saturate_Int16_To_UnsignedInt8 (b[287:272])
tmp_dst[343:336] := Saturate_Int16_To_UnsignedInt8 (b[303:288])
tmp_dst[351:344] := Saturate_Int16_To_UnsignedInt8 (b[319:304])
tmp_dst[359:352] := Saturate_Int16_To_UnsignedInt8 (b[335:320])
tmp_dst[367:360] := Saturate_Int16_To_UnsignedInt8 (b[351:336])
tmp_dst[375:368] := Saturate_Int16_To_UnsignedInt8 (b[367:352])
tmp_dst[383:376] := Saturate_Int16_To_UnsignedInt8 (b[383:368])
tmp_dst[391:384] := Saturate_Int16_To_UnsignedInt8 (a[399:384])
tmp_dst[399:392] := Saturate_Int16_To_UnsignedInt8 (a[415:400])
tmp_dst[407:400] := Saturate_Int16_To_UnsignedInt8 (a[431:416])
tmp_dst[415:408] := Saturate_Int16_To_UnsignedInt8 (a[447:432])
tmp_dst[423:416] := Saturate_Int16_To_UnsignedInt8 (a[463:448])
tmp_dst[431:424] := Saturate_Int16_To_UnsignedInt8 (a[479:464])
tmp_dst[439:432] := Saturate_Int16_To_UnsignedInt8 (a[495:480])
tmp_dst[447:440] := Saturate_Int16_To_UnsignedInt8 (a[511:496])
tmp_dst[455:448] := Saturate_Int16_To_UnsignedInt8 (b[399:384])
tmp_dst[463:456] := Saturate_Int16_To_UnsignedInt8 (b[415:400])
tmp_dst[471:464] := Saturate_Int16_To_UnsignedInt8 (b[431:416])
tmp_dst[479:472] := Saturate_Int16_To_UnsignedInt8 (b[447:432])
tmp_dst[487:480] := Saturate_Int16_To_UnsignedInt8 (b[463:448])
tmp_dst[495:488] := Saturate_Int16_To_UnsignedInt8 (b[479:464])
tmp_dst[503:496] := Saturate_Int16_To_UnsignedInt8 (b[495:480])
tmp_dst[511:504] := Saturate_Int16_To_UnsignedInt8 (b[511:496])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3832</id>
<name>_MM512_MASK_PACKUS_EPI32</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
tmp_dst[271:256] := Saturate_Int32_To_UnsignedInt16 (a[287:256])
tmp_dst[287:272] := Saturate_Int32_To_UnsignedInt16 (a[319:288])
tmp_dst[303:288] := Saturate_Int32_To_UnsignedInt16 (a[351:320])
tmp_dst[319:304] := Saturate_Int32_To_UnsignedInt16 (a[383:352])
tmp_dst[335:320] := Saturate_Int32_To_UnsignedInt16 (b[287:256])
tmp_dst[351:336] := Saturate_Int32_To_UnsignedInt16 (b[319:288])
tmp_dst[367:352] := Saturate_Int32_To_UnsignedInt16 (b[351:320])
tmp_dst[383:368] := Saturate_Int32_To_UnsignedInt16 (b[383:352])
tmp_dst[399:384] := Saturate_Int32_To_UnsignedInt16 (a[415:384])
tmp_dst[415:400] := Saturate_Int32_To_UnsignedInt16 (a[447:416])
tmp_dst[431:416] := Saturate_Int32_To_UnsignedInt16 (a[479:448])
tmp_dst[447:432] := Saturate_Int32_To_UnsignedInt16 (a[511:480])
tmp_dst[463:448] := Saturate_Int32_To_UnsignedInt16 (b[415:384])
tmp_dst[479:464] := Saturate_Int32_To_UnsignedInt16 (b[447:416])
tmp_dst[495:480] := Saturate_Int32_To_UnsignedInt16 (b[479:448])
tmp_dst[511:496] := Saturate_Int32_To_UnsignedInt16 (b[511:480])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3862</id>
<name>_MM512_MASK_PERMUTE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3871</id>
<name>_MM512_MASK_PERMUTE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3878</id>
<name>_MM512_MASK_PERMUTE4F128_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,_MM_PERM_ENUM imm8</sign>
<instr>VPERMF32X4</instr>
<desc>Permutes 128-bit blocks of the packed 32-bit integer vector a using constant imm8. The results are stored in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control) {
	CASE control[1:0] OF
	0: tmp[127:0] := src[127:0]
	1: tmp[127:0] := src[255:128]
	2: tmp[127:0] := src[383:256]
	3: tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp[511:0] := 0
FOR j := 0 to 4
	i := j*128
	n := j*2
	tmp[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
ENDFOR
FOR j := 0 to 15
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3880</id>
<name>_MM512_MASK_PERMUTE4F128_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,_MM_PERM_ENUM imm8</sign>
<instr>VPERMF32X4</instr>
<desc>Permutes 128-bit blocks of the packed single-precision (32-bit) floating-point elements in a using constant imm8. The results are stored in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control) {
	CASE control[1:0] OF
	0: tmp[127:0] := src[127:0]
	1: tmp[127:0] := src[255:128]
	2: tmp[127:0] := src[383:256]
	3: tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp[511:0] := 0
FOR j := 0 to 4
	i := j*128
	n := j*2
	tmp[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
ENDFOR
FOR j := 0 to 15
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3884</id>
<name>_MM512_MASK_PERMUTEVAR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I idx,__M512I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). Note that this intrinsic shuffles across 128-bit lanes, unlike past intrinsics that use the permutevar name. This intrinsic is identical to _mm512_mask_permutexvar_epi32, and it is recommended that you use that intrinsic name.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3892</id>
<name>_MM512_MASK_PERMUTEVAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3901</id>
<name>_MM512_MASK_PERMUTEVAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3909</id>
<name>_MM512_MASK_PERMUTEX_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a within 256-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3915</id>
<name>_MM512_MASK_PERMUTEX_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,CONST_INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 256-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3926</id>
<name>_MM512_MASK_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK32 k,__M512I idx,__M512I b</sign>
<instr>VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		off := 16*idx[i+4:i]
		dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3938</id>
<name>_MM512_MASK_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK16 k,__M512I idx,__M512I b</sign>
<instr>VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3950</id>
<name>_MM512_MASK_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK8 k,__M512I idx,__M512I b</sign>
<instr>VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3962</id>
<name>_MM512_MASK_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__MMASK64 k,__M512I idx,__M512I b</sign>
<instr>VPERMT2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		off := 8*idx[i+5:i]
		dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3974</id>
<name>_MM512_MASK_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__MMASK8 k,__M512I idx,__M512D b</sign>
<instr>VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := a[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3986</id>
<name>_MM512_MASK_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__MMASK16 k,__M512I idx,__M512 b</sign>
<instr>VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := a[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3996</id>
<name>_MM512_MASK_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I idx,__M512I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	id := idx[i+4:i]*16
	IF k[j]
		dst[i+15:i] := a[id+15:id]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4002</id>
<name>_MM512_MASK_PERMUTEXVAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I idx,__M512I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4008</id>
<name>_MM512_MASK_PERMUTEXVAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I idx,__M512I a</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	id := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4017</id>
<name>_MM512_MASK_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I idx,__M512I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	id := idx[i+5:i]*8
	IF k[j]
		dst[i+7:i] := a[id+7:id]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4023</id>
<name>_MM512_MASK_PERMUTEXVAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512I idx,__M512D a</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	id := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4029</id>
<name>_MM512_MASK_PERMUTEXVAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512I idx,__M512 a</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4052</id>
<name>_MM512_MASK_POW_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed double-precision (64-bit) floating-point elements in a raised by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4056</id>
<name>_MM512_MASK_POW_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed single-precision (32-bit) floating-point elements in a raised by packed elements in b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4061</id>
<name>_MM512_MASK_PREFETCH_I32EXTGATHER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>__M512I index,__MMASK16 k,VOID_CONST_PTR mv,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VGATHERPF0DPS, VGATHERPF1DPS</instr>
<desc>Prefetches a set of 16 single-precision (32-bit) memory locations pointed by base address mv and 32-bit integer index vector index with scale scale to L1 or L2 level of cache depending on the value of hint. Gathered elements are merged in cache using writemask k (elements are brought into cache only when their corresponding mask bits are set). The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.
The conv parameter specifies the granularity used by compilers to better encode the instruction. It should be the same as the conv parameter specified for the subsequent gather intrinsic.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	IF k[j] THEN
		CASE hint OF
		_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
		_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
		ESAC
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4063</id>
<name>_MM512_MASK_PREFETCH_I32EXTSCATTER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK16 k,__M512I index,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VSCATTERPF0DPS, VSCATTERPF1DPS</instr>
<desc>Prefetches a set of 16 single-precision (32-bit) memory locations pointed by base address mv and 32-bit integer index vector index with scale scale to L1 or L2 level of cache depending on the value of hint. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.
The conv parameter specifies the granularity used by compilers to better encode the instruction. It should be the same as the conv parameter specified for the subsequent gather intrinsic. Only those elements whose corresponding mask bit in k is set are loaded into cache.</desc>
<oper>cachev := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		CASE hint OF
		_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
		_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
		ESAC
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4065</id>
<name>_MM512_MASK_PREFETCH_I32GATHER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>__M256I vindex,__MMASK8 mask,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0DPD, VGATHERPF1DPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged in cache using writemask k (elements are brought into cache only when their corresponding mask bits are set). scale should be 1, 2, 4 or 8. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j := 0 to 7
	i := j*32;
	IF mask[j] THEN
		Prefetch([base_addr + SignExtend(vindex[i*31:i]) * scale], hint, RFO=0);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4067</id>
<name>_MM512_MASK_PREFETCH_I32GATHER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>__M512I vindex,__MMASK16 mask,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0DPS, VGATHERPF1DPS</instr>
<desc>Prefetch single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged in cache using writemask k (elements are brought into cache only when their corresponding mask bits are set). scale should be 1, 2, 4 or 8. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j := 0 to 15
	i := j*16;
	IF mask[j] THEN
		Prefetch([base_addr + SignExtend(vindex[i*31:i]) * scale], hint, RFO=0);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4069</id>
<name>_MM512_MASK_PREFETCH_I32SCATTER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 mask,__M256I vinde,INT scale,INT hint</sign>
<instr>VSCATTERPF0DPD, VSCATTERPF1DPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements with intent to write using 32-bit indices. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 64-bit elements are brought into cache from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not brought into cache when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 TO 7
	i := j*32;
	IF mask[j] THEN
		Prefetch(base_addr + SignExtend(vindex[i+31:i]) * scale], Level=hint, RFO=1);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4071</id>
<name>_MM512_MASK_PREFETCH_I32SCATTER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__MMASK16 k,__M512I index,INT scale,INT hint</sign>
<instr>VSCATTERPF0DPS, VSCATTERPF1DPS</instr>
<desc>Prefetches 16 single-precision (32-bit) floating-point elements in memory starting at location mv at packed 32-bit integer indices stored in index scaled by scale. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. Only those elements whose corresponding mask bit in k is set are loaded into the desired cache.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		addr := MEM[mv + index[j] * scale]
		CASE hint OF
		_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
		_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
		_MM_HINT_T2: PrefetchL2WithT1HintNonTemporal(addr[i+31:i])
		_MM_HINT_NTA: PrefetchL1WithT0HintNonTemporal(addr[i+31:i])
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4073</id>
<name>_MM512_MASK_PREFETCH_I64GATHER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>__M512I vindex,__MMASK8 mask,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0QPD, VGATHERPF1QPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements from memory into cache level specified by hint using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Prefetched elements are merged in cache using writemask k (elements are copied from memory when the corresponding mask bit is set). scale should be 1, 2, 4 or 8. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF mask[j] THEN
		Prefetch([base_addr + SignExtend(vindex[i*63:i] * scale]), Level=hint, RFO=0);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4075</id>
<name>_MM512_MASK_PREFETCH_I64GATHER_PS</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>__M512I vindex,__MMASK8 mask,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0QPS, VGATHERPF1QPS</instr>
<desc>Prefetch single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged in cache using writemask k (elements are only brought into cache when their corresponding mask bit is set). scale should be 1, 2, 4 or 8.. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j:= 0 to 7
	i := j*64;
	IF mask[j] THEN
		Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], hint, RFO=0);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4077</id>
<name>_MM512_MASK_PREFETCH_I64SCATTER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 mask,__M512I vindex,INT scale,INT hint</sign>
<instr>VSCATTERPF0QPD, VSCATTERPF1QPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements with intent to write into memory using 64-bit indices. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 64-bit elements are brought into cache from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not brought into cache when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF mask[j] THEN
		Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4079</id>
<name>_MM512_MASK_PREFETCH_I64SCATTER_PS</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__MMASK8 mask,__M512I vindex,INT scale,INT hint</sign>
<instr>VSCATTERPF0QPS, VSCATTERPF1QPS</instr>
<desc>Prefetch single-precision (32-bit) floating-point elements with intent to write into memory using 64-bit indices. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale) subject to mask k (elements are not brought into cache when the corresponding mask bit is not set). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF mask[j] THEN
		Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4119</id>
<name>_MM512_MASK_RANGE_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4128</id>
<name>_MM512_MASK_RANGE_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4131</id>
<name>_MM512_MASK_RANGE_ROUND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT imm8,INT rounding</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4134</id>
<name>_MM512_MASK_RANGE_ROUND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT imm8,INT rounding</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4156</id>
<name>_MM512_MASK_RCP14_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4165</id>
<name>_MM512_MASK_RCP14_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4174</id>
<name>_MM512_MASK_RCP23_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VRCP23PS</instr>
<desc>Approximates the reciprocals of packed single-precision (32-bit) floating-point elements in a to 23 bits of precision, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4176</id>
<name>_MM512_MASK_RCP28_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VRCP28PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
	ELSE
		dst[i+63:i] := src[i+63:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4179</id>
<name>_MM512_MASK_RCP28_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VRCP28PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
	ELSE
		dst[i+31:i] := src[i+31:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4182</id>
<name>_MM512_MASK_RCP28_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VRCP28PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
	ELSE
		dst[i+63:i] := src[i+63:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4185</id>
<name>_MM512_MASK_RCP28_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VRCP28PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
	ELSE
		dst[i+31:i] := src[i+31:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4213</id>
<name>_MM512_MASK_RECIP_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Computes the reciprocal of packed double-precision (64-bit) floating-point elements in a, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (1 / a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4215</id>
<name>_MM512_MASK_RECIP_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Computes the reciprocal of packed single-precision (32-bit) floating-point elements in a, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (1 / a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4217</id>
<name>_MM512_MASK_REDUCE_ADD_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by addition using mask k. Returns the sum of all active elements in a.</desc>
<oper>sum[31:0] := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		sum[31:0] := sum[31:0] + a[i+31:i]
	FI
ENDFOR
RETURN sum[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4219</id>
<name>_MM512_MASK_REDUCE_ADD_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by addition using mask k. Returns the sum of all active elements in a.</desc>
<oper>sum[63:0] := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		sum[63:0] := sum[63:0] + a[i+63:i]
	FI
ENDFOR
RETURN sum[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4221</id>
<name>_MM512_MASK_REDUCE_ADD_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by addition using mask k. Returns the sum of all active elements in a.</desc>
<oper>sum[63:0] := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		sum[63:0] := sum[63:0] + a[i+63:i]
	FI
ENDFOR
RETURN sum[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4223</id>
<name>_MM512_MASK_REDUCE_ADD_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by addition using mask k. Returns the sum of all active elements in a.</desc>
<oper>sum[31:0] := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		sum[31:0] := sum[31:0] + a[i+31:i]
	FI
ENDFOR
RETURN sum[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4225</id>
<name>_MM512_MASK_REDUCE_AND_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by bitwise AND using mask k. Returns the bitwise AND of all active elements in a.</desc>
<oper>reduced[31:0] := 0xFFFFFFFF
FOR j := 0 to 15
	i := j*32
	IF k[j]
		reduced[31:0] := reduced[31:0] AND a[i+31:i]
	FI
ENDFOR
RETURN reduced[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4227</id>
<name>_MM512_MASK_REDUCE_AND_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by bitwise AND using mask k. Returns the bitwise AND of all active elements in a.</desc>
<oper>reduced[63:0] := 0xFFFFFFFFFFFFFFFF
FOR j := 0 to 7
	i := j*64
	IF k[j]
		reduced[63:0] := reduced[63:0] AND a[i+63:i]
	FI
ENDFOR
RETURN reduced[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4229</id>
<name>_MM512_MASK_REDUCE_GMAX_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Determines the maximum element of the packed double-precision (64-bit) floating-point elements stored in a and stores the result in dst. Bitmask k is used to exclude certain elements (elements are ignored when the corresponding mask bit is not set).</desc>
<oper>max = a[63:0]
FOR j := 1 to 7
	i := j*64
	IF k[j]
		CONTINUE
	ELSE
		dst = FpMax(max, a[i+63:i])
	FI
ENDFOR
dst := max</oper>
</intrinsic>
<intrinsic>
<id>4231</id>
<name>_MM512_MASK_REDUCE_GMAX_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Determines the maximum element of the packed single-precision (32-bit) floating-point elements stored in a and stores the result in dst. Bitmask k is used to exclude certain elements (elements are ignored when the corresponding mask bit is not set).</desc>
<oper>max = a[31:0]
FOR j := 1 to 15
	i := j*32
	IF k[j]
		CONTINUE
	ELSE
		dst = FpMax(max, a[i+31:i])
	FI
ENDFOR
dst := max</oper>
</intrinsic>
<intrinsic>
<id>4233</id>
<name>_MM512_MASK_REDUCE_GMIN_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Determines the minimum element of the packed double-precision (64-bit) floating-point elements stored in a and stores the result in dst. Bitmask k is used to exclude certain elements (elements are ignored when the corresponding mask bit is not set).</desc>
<oper>min = a[63:0]
FOR j := 1 to 7
	i := j*64
	IF k[j]
		CONTINUE
	ELSE
		dst = FpMin(min, a[i+63:i])
	FI
ENDFOR
dst := min</oper>
</intrinsic>
<intrinsic>
<id>4235</id>
<name>_MM512_MASK_REDUCE_GMIN_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Determines the minimum element of the packed single-precision (32-bit) floating-point elements stored in a and stores the result in dst using writemask k (elements are ignored when the corresponding mask bit is not set).</desc>
<oper>min = a[31:0]
FOR j := 1 to 15
	i := j*32
	IF k[j]
		CONTINUE
	ELSE
		dst = FpMin(min, a[i+31:i])
	FI
ENDFOR
dst := min</oper>
</intrinsic>
<intrinsic>
<id>4237</id>
<name>_MM512_MASK_REDUCE_MAX_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by maximum using mask k. Returns the maximum of all active elements in a.</desc>
<oper>max[31:0] := MIN_INT
FOR j := 0 to 15
	i := j*32
	IF k[j]
		max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
	FI
ENDFOR
RETURN max[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4239</id>
<name>_MM512_MASK_REDUCE_MAX_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by maximum using mask k. Returns the maximum of all active elements in a.</desc>
<oper>max[63:0] := MIN_INT
FOR j := 0 to 7
	i := j*64
	IF k[j]
		max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
	FI
ENDFOR
RETURN max[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4241</id>
<name>_MM512_MASK_REDUCE_MAX_EPU32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 32-bit integers in a by maximum using mask k. Returns the maximum of all active elements in a.</desc>
<oper>max[31:0] := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
	FI
ENDFOR
RETURN max[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4243</id>
<name>_MM512_MASK_REDUCE_MAX_EPU64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned __int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 64-bit integers in a by maximum using mask k. Returns the maximum of all active elements in a.</desc>
<oper>max[63:0] := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
	FI
ENDFOR
RETURN max[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4245</id>
<name>_MM512_MASK_REDUCE_MAX_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by maximum using mask k. Returns the maximum of all active elements in a.</desc>
<oper>max[63:0] := MIN_DOUBLE
FOR j := 0 to 7
	i := j*64
	IF k[j]
		max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
	FI
ENDFOR
RETURN max[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4247</id>
<name>_MM512_MASK_REDUCE_MAX_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by maximum using mask k. Returns the maximum of all active elements in a.</desc>
<oper>max[31:0] := MIN_FLOAT
FOR j := 0 to 15
	i := j*32
	IF k[j]
		max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
	FI
ENDFOR
RETURN max[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4249</id>
<name>_MM512_MASK_REDUCE_MIN_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by maximum using mask k. Returns the minimum of all active elements in a.</desc>
<oper>min[31:0] := MAX_INT
FOR j := 0 to 15
	i := j*32
	IF k[j]
		min[31:0] := MINIMUM(min[31:0], a[i+31:i])
	FI
ENDFOR
RETURN min[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4251</id>
<name>_MM512_MASK_REDUCE_MIN_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by maximum using mask k. Returns the minimum of all active elements in a.</desc>
<oper>min[63:0] := MAX_INT
FOR j := 0 to 7
	i := j*64
	IF k[j]
		min[63:0] := MINIMUM(min[63:0], a[i+63:i])
	FI
ENDFOR
RETURN min[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4253</id>
<name>_MM512_MASK_REDUCE_MIN_EPU32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 32-bit integers in a by maximum using mask k. Returns the minimum of all active elements in a.</desc>
<oper>min[31:0] := MAX_UINT
FOR j := 0 to 15
	i := j*32
	IF k[j]
		min[31:0] := MINIMUM(min[31:0], a[i+31:i])
	FI
ENDFOR
RETURN min[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4255</id>
<name>_MM512_MASK_REDUCE_MIN_EPU64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned __int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 64-bit integers in a by minimum using mask k. Returns the minimum of all active elements in a.</desc>
<oper>min[63:0] := MAX_UINT
FOR j := 0 to 7
	i := j*64
	IF k[j]
		min[63:0] := MINIMUM(min[63:0], a[i+63:i])
	FI
ENDFOR
RETURN min[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4257</id>
<name>_MM512_MASK_REDUCE_MIN_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by maximum using mask k. Returns the minimum of all active elements in a.</desc>
<oper>min[63:0] := MAX_DOUBLE
FOR j := 0 to 7
	i := j*64
	IF k[j]
		min[63:0] := MINIMUM(min[63:0], a[i+63:i])
	FI
ENDFOR
RETURN min[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4259</id>
<name>_MM512_MASK_REDUCE_MIN_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by maximum using mask k. Returns the minimum of all active elements in a.</desc>
<oper>min[31:0] := MAX_FLOAT
FOR j := 0 to 15
	i := j*32
	IF k[j]
		min[31:0] := MINIMUM(min[31:0], a[i+31:i])
	FI
ENDFOR
RETURN min[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4261</id>
<name>_MM512_MASK_REDUCE_MUL_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by multiplication using mask k. Returns the product of all active elements in a.</desc>
<oper>prod[31:0] := 1
FOR j := 0 to 15
	i := j*32
	IF k[j]
		prod[31:0] := prod[31:0] * a[i+31:i]
	FI
ENDFOR
RETURN prod[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4263</id>
<name>_MM512_MASK_REDUCE_MUL_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by multiplication using mask k. Returns the product of all active elements in a.</desc>
<oper>prod[63:0] := 1
FOR j := 0 to 7
	i := j*64
	IF k[j]
		prod[63:0] := prod[63:0] * a[i+63:i]
	FI
ENDFOR
RETURN prod[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4265</id>
<name>_MM512_MASK_REDUCE_MUL_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by multiplication using mask k. Returns the product of all active elements in a.</desc>
<oper>prod[63:0] := 1
FOR j := 0 to 7
	i := j*64
	IF k[j]
		prod[63:0] := prod[63:0] * a[i+63:i]
	FI
ENDFOR
RETURN prod[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4267</id>
<name>_MM512_MASK_REDUCE_MUL_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by multiplication using mask k. Returns the product of all active elements in a.</desc>
<oper>prod[31:0] := 1
FOR j := 0 to 15
	i := j*32
	IF k[j]
		prod[31:0] := prod[31:0] * a[i+31:i]
	FI
ENDFOR
RETURN prod[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4269</id>
<name>_MM512_MASK_REDUCE_OR_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by bitwise OR using mask k. Returns the bitwise OR of all active elements in a.</desc>
<oper>reduced[31:0] := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		reduced[31:0] := reduced[31:0] OR a[i+31:i]
	FI
ENDFOR
RETURN reduced[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4271</id>
<name>_MM512_MASK_REDUCE_OR_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by bitwise OR using mask k. Returns the bitwise OR of all active elements in a.</desc>
<oper>reduced[63:0] := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		reduced[63:0] := reduced[63:0] OR a[i+63:i]
	FI
ENDFOR
RETURN reduced[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4279</id>
<name>_MM512_MASK_REDUCE_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4288</id>
<name>_MM512_MASK_REDUCE_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4291</id>
<name>_MM512_MASK_REDUCE_ROUND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT imm8,INT rounding</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4294</id>
<name>_MM512_MASK_REDUCE_ROUND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT imm8,INT rounding</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4314</id>
<name>_MM512_MASK_REM_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4327</id>
<name>_MM512_MASK_REM_EPU32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4335</id>
<name>_MM512_MASK_RINT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Rounds the packed double-precision (64-bit) floating-point elements in a to the nearest even integer value and stores the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundToNearestEven(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4337</id>
<name>_MM512_MASK_RINT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Rounds the packed single-precision (32-bit) floating-point elements in a to the nearest even integer value and stores the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundToNearestEven(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4345</id>
<name>_MM512_MASK_ROL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4354</id>
<name>_MM512_MASK_ROL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4363</id>
<name>_MM512_MASK_ROLV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4372</id>
<name>_MM512_MASK_ROLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4381</id>
<name>_MM512_MASK_ROR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4390</id>
<name>_MM512_MASK_ROR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4399</id>
<name>_MM512_MASK_RORV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4408</id>
<name>_MM512_MASK_RORV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4419</id>
<name>_MM512_MASK_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a to the nearest integer value using expadj and in the direction of rounding, and store the results as packed single-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ROUND(a[i+31:i])
		CASE expadj OF
		_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
		_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
		_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
		_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
		_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
		_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
		_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
		_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4423</id>
<name>_MM512_MASK_ROUNDFXPNT_ADJUST_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VRNDFXPNTPD</instr>
<desc>Performs element-by-element rounding of packed double-precision (64-bit) floating-point elements in a using expadj and in the direction of rounding and stores results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ROUND(a[i+63:i])
		CASE expadj OF
		_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
		_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
		_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
		_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
		_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
		_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
		_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
		_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
		ESAC
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4425</id>
<name>_MM512_MASK_ROUNDFXPNT_ADJUST_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VRNDFXPNTPS</instr>
<desc>Performs element-by-element rounding of packed single-precision (32-bit) floating-point elements in a using expadj and in the direction of rounding and stores results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ROUND(a[i+31:i])
		CASE expadj OF
		_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
		_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
		_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
		_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
		_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
		_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
		_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
		_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
		ESAC
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4433</id>
<name>_MM512_MASK_ROUNDSCALE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4442</id>
<name>_MM512_MASK_ROUNDSCALE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4445</id>
<name>_MM512_MASK_ROUNDSCALE_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT imm8,INT rounding</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4448</id>
<name>_MM512_MASK_ROUNDSCALE_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT imm8,INT rounding</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4470</id>
<name>_MM512_MASK_RSQRT14_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4477</id>
<name>_MM512_MASK_RSQRT14_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4486</id>
<name>_MM512_MASK_RSQRT23_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VRSQRT23PS</instr>
<desc>Calculates the reciprocal square root of packed single-precision (32-bit) floating-point elements in a to 23 bits of accuracy and stores the result in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := Sqrt(1.0 / a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4488</id>
<name>_MM512_MASK_RSQRT28_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VRSQRT28PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
	ELSE
		dst[i+63:i] := src[i+63:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4491</id>
<name>_MM512_MASK_RSQRT28_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VRSQRT28PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
	ELSE
		dst[i+31:i] := src[i+31:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4494</id>
<name>_MM512_MASK_RSQRT28_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VRSQRT28PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
	ELSE
		dst[i+63:i] := src[i+63:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4497</id>
<name>_MM512_MASK_RSQRT28_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VRSQRT28PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
	ELSE
		dst[i+31:i] := src[i+31:i];
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4516</id>
<name>_MM512_MASK_SBB_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k1,__MMASK16 k2,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSBBD</instr>
<desc>Performs element-by-element three-input subtraction of packed 32-bit integer elements of v3 as well as the corresponding bit from k2 from v2. The borrowed value from the subtraction difference for the nth element is written to the nth bit of borrow (borrow flag). Results are stored in dst using writemask k1 (elements are copied from v2 when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		dst[i+31:i] := v2[i+31:i] - v3[i+31:i] - k2[j]
		borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k2[j])
	ELSE
		dst[i+31:i] := v2[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4518</id>
<name>_MM512_MASK_SBBR_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k1,__MMASK16 k2,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSBBRD</instr>
<desc>Performs element-by-element three-input subtraction of packed 32-bit integer elements of v2 as well as the corresponding bit from k2 from v3. The borrowed value from the subtraction difference for the nth element is written to the nth bit of borrow (borrow flag). Results are stored in dst using writemask k1 (elements are copied from v2 when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		dst[i+31:i] := v3[i+31:i] - v2[i+31:i] - k2[j]
		borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k[j])
	ELSE
		dst[i+31:i] := v2[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4520</id>
<name>_MM512_MASK_SCALE_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512I b</sign>
<instr>VSCALEPS</instr>
<desc>Scales each single-precision (32-bit) floating-point element in a by multiplying it by 2**exponent, where the exponenet is the corresponding 32-bit integer element in b, storing results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4522</id>
<name>_MM512_MASK_SCALE_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512I b,INT rounding</sign>
<instr>VSCALEPS</instr>
<desc>Scales each single-precision (32-bit) floating-point element in a by multiplying it by 2**exp, where the exp is the corresponding 32-bit integer element in b, storing results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). Results are rounded using constant rounding.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4530</id>
<name>_MM512_MASK_SCALEF_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4539</id>
<name>_MM512_MASK_SCALEF_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4542</id>
<name>_MM512_MASK_SCALEF_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4545</id>
<name>_MM512_MASK_SCALEF_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4597</id>
<name>_MM512_MASK_SET1_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,SHORT a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast 16-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4606</id>
<name>_MM512_MASK_SET1_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4614</id>
<name>_MM512_MASK_SET1_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4625</id>
<name>_MM512_MASK_SET1_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,CHAR a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast 8-bit integer a to all elements of dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4694</id>
<name>_MM512_MASK_SHUFFLE_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4703</id>
<name>_MM512_MASK_SHUFFLE_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle 8-bit integers in a within 128-bit lanes using the control in the corresponding 8-bit element of b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF b[i+7] == 1
			dst[i+7:i] := 0
		ELSE
			index[3:0] := b[i+3:i]
			dst[i+7:i] := a[index*8+7:index*8]
		FI
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4709</id>
<name>_MM512_MASK_SHUFFLE_F32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VSHUFF32X4</instr>
<desc>Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4715</id>
<name>_MM512_MASK_SHUFFLE_F64X2</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VSHUFF64X2</instr>
<desc>Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4721</id>
<name>_MM512_MASK_SHUFFLE_I32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VSHUFI32X4</instr>
<desc>Shuffle 128-bits (composed of 4 32-bit integers) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4727</id>
<name>_MM512_MASK_SHUFFLE_I64X2</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VSHUFI64X2</instr>
<desc>Shuffle 128-bits (composed of 2 64-bit integers) selected by imm8 from a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4736</id>
<name>_MM512_MASK_SHUFFLE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4747</id>
<name>_MM512_MASK_SHUFFLE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4756</id>
<name>_MM512_MASK_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the high 64 bits of 128-bit lanes of dst, with the low 64 bits of 128-bit lanes being copied from from a to dst, using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[207:192]
tmp_dst[319:256] := a[319:256]
tmp_dst[335:320] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[335:320]
tmp_dst[351:336] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[335:320]
tmp_dst[367:352] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[335:320]
tmp_dst[383:368] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[335:320]
tmp_dst[447:384] := a[447:384]
tmp_dst[463:448] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[463:448]
tmp_dst[479:464] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[463:448]
tmp_dst[495:480] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[463:448]
tmp_dst[511:496] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[463:448]

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4765</id>
<name>_MM512_MASK_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the low 64 bits of 128-bit lanes of dst, with the high 64 bits of 128-bit lanes being copied from from a to dst, using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]
tmp_dst[271:256] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[271:256]
tmp_dst[287:272] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[271:256]
tmp_dst[303:288] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[271:256]
tmp_dst[319:304] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[271:256]
tmp_dst[383:320] := a[383:320]
tmp_dst[399:384] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[399:384]
tmp_dst[415:400] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[399:384]
tmp_dst[431:416] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[399:384]
tmp_dst[447:432] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[399:384]
tmp_dst[511:448] := a[511:448]

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4779</id>
<name>_MM512_MASK_SIN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SIN(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4783</id>
<name>_MM512_MASK_SIN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SIN(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4787</id>
<name>_MM512_MASK_SINCOS_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D_PTR cos_res,__M512D sin_src,__M512D cos_src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Computes the sine and cosine of the packed double-precision (64-bit) floating-point elements in a and stores the results of the sine computation in dst and the results of the cosine computation in cos_res. Elements are written to their respective locations using writemask k (elements are copied from sin_src or cos_src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SIN(a[i+63:i])
		cos_res[i+63:i] := COS(a[i+63:i])
	ELSE
		dst[i+63:i] := sin_src[i+63:i]
		cos_res[i+63:i] := cos_src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0
cos_res[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4791</id>
<name>_MM512_MASK_SINCOS_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512_PTR cos_res,__M512 sin_src,__M512 cos_src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Computes the sine and cosine of the packed single-precision (32-bit) floating-point elements in a and stores the results of the sine computation in dst and the results of the cosine computation in cos_res. Elements are written to their respective locations using writemask k (elements are copied from sin_src or cos_src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SIN(a[i+31:i])
		cos_res[i+31:i] := COS(a[i+31:i])
	ELSE
		dst[i+31:i] := sin_src[i+31:i]
		cos_res[i+31:i] := cos_src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0
cos_res[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4795</id>
<name>_MM512_MASK_SIND_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SIND(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4799</id>
<name>_MM512_MASK_SIND_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SIND(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4803</id>
<name>_MM512_MASK_SINH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SINH(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4807</id>
<name>_MM512_MASK_SINH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SINH(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4815</id>
<name>_MM512_MASK_SLL_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4824</id>
<name>_MM512_MASK_SLL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4833</id>
<name>_MM512_MASK_SLL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4845</id>
<name>_MM512_MASK_SLLI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4854</id>
<name>_MM512_MASK_SLLI_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4863</id>
<name>_MM512_MASK_SLLI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4877</id>
<name>_MM512_MASK_SLLV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4886</id>
<name>_MM512_MASK_SLLV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4895</id>
<name>_MM512_MASK_SLLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4906</id>
<name>_MM512_MASK_SQRT_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4915</id>
<name>_MM512_MASK_SQRT_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4918</id>
<name>_MM512_MASK_SQRT_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4921</id>
<name>_MM512_MASK_SQRT_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4942</id>
<name>_MM512_MASK_SRA_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4951</id>
<name>_MM512_MASK_SRA_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4960</id>
<name>_MM512_MASK_SRA_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4971</id>
<name>_MM512_MASK_SRAI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4980</id>
<name>_MM512_MASK_SRAI_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4989</id>
<name>_MM512_MASK_SRAI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5000</id>
<name>_MM512_MASK_SRAV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5009</id>
<name>_MM512_MASK_SRAV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5018</id>
<name>_MM512_MASK_SRAV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5027</id>
<name>_MM512_MASK_SRL_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5036</id>
<name>_MM512_MASK_SRL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5045</id>
<name>_MM512_MASK_SRL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5057</id>
<name>_MM512_MASK_SRLI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5066</id>
<name>_MM512_MASK_SRLI_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5075</id>
<name>_MM512_MASK_SRLI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5089</id>
<name>_MM512_MASK_SRLV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5098</id>
<name>_MM512_MASK_SRLV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5107</id>
<name>_MM512_MASK_SRLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5112</id>
<name>_MM512_MASK_STORE_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK16 k,__M512I a</sign>
<instr>VMOVDQA32</instr>
<desc>Store packed 32-bit integers from a into memory using writemask k.
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5116</id>
<name>_MM512_MASK_STORE_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M512I a</sign>
<instr>VMOVDQA64</instr>
<desc>Store packed 64-bit integers from a into memory using writemask k.
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5122</id>
<name>_MM512_MASK_STORE_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M512D a</sign>
<instr>VMOVAPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using writemask k.
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5129</id>
<name>_MM512_MASK_STORE_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK16 k,__M512 a</sign>
<instr>VMOVAPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using writemask k.
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5157</id>
<name>_MM512_MASK_STOREU_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK32 k,__M512I a</sign>
<instr>VMOVDQU16</instr>
<desc>Store packed 16-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5160</id>
<name>_MM512_MASK_STOREU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK16 k,__M512I a</sign>
<instr>VMOVDQU32</instr>
<desc>Store packed 32-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5163</id>
<name>_MM512_MASK_STOREU_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M512I a</sign>
<instr>VMOVDQU64</instr>
<desc>Store packed 64-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5166</id>
<name>_MM512_MASK_STOREU_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK64 k,__M512I a</sign>
<instr>VMOVDQU8</instr>
<desc>Store packed 8-bit integers from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5171</id>
<name>_MM512_MASK_STOREU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK8 k,__M512D a</sign>
<instr>VMOVUPD</instr>
<desc>Store packed double-precision (64-bit) floating-point elements from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5177</id>
<name>_MM512_MASK_STOREU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__MMASK16 k,__M512 a</sign>
<instr>VMOVUPS</instr>
<desc>Store packed single-precision (32-bit) floating-point elements from a into memory using writemask k.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5212</id>
<name>_MM512_MASK_SUB_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] - b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5221</id>
<name>_MM512_MASK_SUB_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5230</id>
<name>_MM512_MASK_SUB_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5239</id>
<name>_MM512_MASK_SUB_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] - b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5248</id>
<name>_MM512_MASK_SUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5260</id>
<name>_MM512_MASK_SUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5263</id>
<name>_MM512_MASK_SUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5266</id>
<name>_MM512_MASK_SUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5284</id>
<name>_MM512_MASK_SUBR_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I v2,__M512I v3</sign>
<instr>VPSUBRD</instr>
<desc>Performs element-by-element subtraction of packed 32-bit integer elements in v2 from v3 storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set)</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5286</id>
<name>_MM512_MASK_SUBR_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D v2,__M512D v3</sign>
<instr>VSUBRPD</instr>
<desc>Performs element-by-element subtraction of packed double-precision (64-bit) floating-point elements in v2 from v3 storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5288</id>
<name>_MM512_MASK_SUBR_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2,__M512 v3</sign>
<instr>VSUBRPS</instr>
<desc>Performs element-by-element subtraction of packed single-precision (32-bit) floating-point elements in v2 from v3 storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5290</id>
<name>_MM512_MASK_SUBR_ROUND_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D v2,__M512D v3,INT rounding</sign>
<instr>VSUBRPD</instr>
<desc>Performs element-by-element subtraction of packed double-precision (64-bit) floating-point elements in v2 from v3 storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5292</id>
<name>_MM512_MASK_SUBR_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v2,__M512 v3,INT rounding</sign>
<instr>VSUBRPS</instr>
<desc>Performs element-by-element subtraction of packed single-precision (32-bit) floating-point elements in v2 from v3 storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5294</id>
<name>_MM512_MASK_SUBRSETB_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k,__MMASK16 k_old,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSUBRSETBD</instr>
<desc>Performs element-by-element subtraction of packed 32-bit integer elements in v2 from v3, storing the results in dst and v2. The borrowed value from the subtraction difference for the nth element is written to the nth bit of borrow (borrow flag). Results are written using writemask k (elements are copied from k to k_old when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		diff := v3[i+31:i] - v2[i+31:i]
		borrow[j] := Borrow(v3[i+31:i] - v2[i+31:i])
		dst[i+31:i] := diff
		v2[i+31:i] := diff
	ELSE
		borrow[j] := k_old[j]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5302</id>
<name>_MM512_MASK_SUBS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5311</id>
<name>_MM512_MASK_SUBS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5320</id>
<name>_MM512_MASK_SUBS_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5329</id>
<name>_MM512_MASK_SUBS_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5336</id>
<name>_MM512_MASK_SUBSETB_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k,__MMASK16 k_old,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSUBSETBD</instr>
<desc>Performs element-by-element subtraction of packed 32-bit integer elements in v3 from v2, storing the results in dst and the nth borrow bit in the nth position of borrow (borrow flag). Results are stored using writemask k (elements are copied from v2 and k_old when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := v2[i+31:i] - v3[i+31:i]
		borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
		borrow[j] := k_old[j]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5348</id>
<name>_MM512_MASK_SVML_ROUND_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a to the nearest integer value, and store the results as packed double-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ROUND(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i] 
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5356</id>
<name>_MM512_MASK_SWIZZLE_EPI32</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the four groups of packed 4x32-bit integer elements in v using swizzle parameter s, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 7
		i := j*64
		IF k[j*2]
			dst[i+31:i]	:= v[i+63:i+32]
		ELSE
			dst[i+31:i]	:= src[i+31:i]
		FI
		IF k[j*2+1]
			dst[i+63:i+32] := v[i+31:i]
		ELSE
			dst[i+63:i+32] := src[i+63:i+32]
		FI
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+95:i+64]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+127:i+96]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+31:i]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+63:i+32]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+31:i]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+31:i]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+31:i]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+31:i]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+63:i+32]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+63:i+32]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+63:i+32]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+63:i+32]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+95:i+64]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+95:i+64]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+95:i+64]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+95:i+64]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+127:i+96]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+127:i+96]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+127:i+96]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+127:i+96]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+63:i+32]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+95:i+64]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+31:i]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+127:i+96]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5358</id>
<name>_MM512_MASK_SWIZZLE_EPI64</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the four groups of packed 4x64-bit integer elements in v using swizzle parameter s, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 3
		i := j*64
		IF k[j*2]
			dst[i+63:i]	 := v[i+127:i+64]
		ELSE
			dst[i+63:i]	 := src[i+63:i]
		FI
		IF k[j*2+1]
			dst[i+127:i+64] := v[i+63:i]
		ELSE
			dst[i+127:i+64] := src[i+127:i+64]
		FI
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+191:i+128]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+255:i+192]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+63:i]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+127:i+64]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+63:i]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+63:i]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+63:i]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+63:i]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+127:i+63]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+127:i+63]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+127:i+63]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+127:i+63]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+191:i+128]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+191:i+128]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+191:i+128]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+191:i+128]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+255:i+192]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+255:i+192]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+255:i+192]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+255:i+192]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+127:i+64]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+191:i+128]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+63:i]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+255:i+192]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5360</id>
<name>_MM512_MASK_SWIZZLE_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the two groups of packed 4x double-precision (64-bit) floating-point elements in v using swizzle parameter s, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 3
		i := j*64
		IF k[j*2]
			dst[i+63:i]	 := v[i+127:i+64]
		ELSE
			dst[i+63:i]	 := src[i+63:i]
		FI
		IF k[j*2+1]
			dst[i+127:i+64] := v[i+63:i]
		ELSE
			dst[i+127:i+64] := src[i+127:i+64]
		FI
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+191:i+128]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+255:i+192]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+63:i]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+127:i+64]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+63:i]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+63:i]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+63:i]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+63:i]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+127:i+63]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+127:i+63]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+127:i+63]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+127:i+63]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+191:i+128]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+191:i+128]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+191:i+128]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+191:i+128]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+255:i+192]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+255:i+192]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+255:i+192]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+255:i+192]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 1
		i := j*256
		IF k[j*4]
			dst[i+63:i]	  := v[i+127:i+64]
		ELSE
			dst[i+63:i]	  := src[i+63:i]
		FI
		IF k[j*4+1]
			dst[i+127:i+64]  := v[i+191:i+128]
		ELSE
			dst[i+127:i+64]  := src[i+127:i+64]
		FI
		IF k[j*4+2]
			dst[i+191:i+128] := v[i+63:i]
		ELSE
			dst[i+191:i+128] := src[i+191:i+128]
		FI
		IF k[j*4+3]
			dst[i+255:i+192] := v[i+255:i+192]
		ELSE
			dst[i+255:i+192] := src[i+255:i+192]
		FI
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5362</id>
<name>_MM512_MASK_SWIZZLE_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the four groups of packed 4x single-precision (32-bit) floating-point elements in v using swizzle parameter s, storing the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 7
		i := j*64
		IF k[j*2]
			dst[i+31:i]	:= v[i+63:i+32]
		ELSE
			dst[i+31:i]	:= src[i+31:i]
		FI
		IF k[j*2+1]
			dst[i+63:i+32] := v[i+31:i]
		ELSE
			dst[i+63:i+32] := src[i+63:i+32]
		FI
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+95:i+64]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+127:i+96]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+31:i]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+63:i+32]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+31:i]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+31:i]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+31:i]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+31:i]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+63:i+32]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+63:i+32]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+63:i+32]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+63:i+32]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+95:i+64]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+95:i+64]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+95:i+64]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+95:i+64]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+127:i+96]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+127:i+96]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+127:i+96]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+127:i+96]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 3
		i := j*128
		IF k[j*4]
			dst[i+31:i]	 := v[i+63:i+32]
		ELSE
			dst[i+31:i]	 := src[i+31:i]
		FI
		IF k[j*4+1]
			dst[i+63:i+32]  := v[i+95:i+64]
		ELSE
			dst[i+63:i+32]  := src[i+63:i+32]
		FI
		IF k[j*4+2]
			dst[i+95:i+64]  := v[i+31:i]
		ELSE
			dst[i+95:i+64]  := src[i+95:i+64]
		FI
		IF k[j*4+3]
			dst[i+127:i+96] := v[i+127:i+96]
		ELSE
			dst[i+127:i+96] := src[i+127:i+96]
		FI
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5366</id>
<name>_MM512_MASK_TAN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := TAN(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5370</id>
<name>_MM512_MASK_TAN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := TAN(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5374</id>
<name>_MM512_MASK_TAND_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := TAND(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5378</id>
<name>_MM512_MASK_TAND_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := TAND(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5382</id>
<name>_MM512_MASK_TANH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := TANH(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5386</id>
<name>_MM512_MASK_TANH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := TANH(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5394</id>
<name>_MM512_MASK_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from src, a, and b are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using writemask k at 32-bit granularity (32-bit elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		FOR h := 0 to 31
			index[2:0] := (src[i+h] &amp;lt;&amp;lt; 2) OR (a[i+h] &amp;lt;&amp;lt; 1) OR b[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5403</id>
<name>_MM512_MASK_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from src, a, and b are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using writemask k at 64-bit granularity (64-bit elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		FOR h := 0 to 63
			index[2:0] := (src[i+h] &amp;lt;&amp;lt; 2) OR (a[i+h] &amp;lt;&amp;lt; 1) OR b[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5412</id>
<name>_MM512_MASK_TEST_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPTESTMW</instr>
<desc>Compute the bitwise AND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5418</id>
<name>_MM512_MASK_TEST_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPTESTMD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5424</id>
<name>_MM512_MASK_TEST_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPTESTMQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5430</id>
<name>_MM512_MASK_TEST_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPTESTMB</instr>
<desc>Compute the bitwise AND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>5443</id>
<name>_MM512_MASK_TESTN_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__MMASK32 k1,__M512I a,__M512I b</sign>
<instr>VPTESTNMW</instr>
<desc>Compute the bitwise NAND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k1[j]
		k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5449</id>
<name>_MM512_MASK_TESTN_EPI32_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__MMASK16 k1,__M512I a,__M512I b</sign>
<instr>VPTESTNMD</instr>
<desc>Compute the bitwise NAND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k1[j]
		k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5455</id>
<name>_MM512_MASK_TESTN_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__MMASK8 k1,__M512I a,__M512I b</sign>
<instr>VPTESTNMQ</instr>
<desc>Compute the bitwise NAND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k1[j]
		k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5461</id>
<name>_MM512_MASK_TESTN_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__MMASK64 k1,__M512I a,__M512I b</sign>
<instr>VPTESTNMB</instr>
<desc>Compute the bitwise NAND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k (subject to writemask k) if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k1[j]
		k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>5480</id>
<name>_MM512_MASK_TRUNC_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a</sign>
<instr>NONE</instr>
<desc>Truncate the packed double-precision (64-bit) floating-point elements in a, and store the results as packed double-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := TRUNCATE(a[i+63:i])
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5484</id>
<name>_MM512_MASK_TRUNC_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a</sign>
<instr>NONE</instr>
<desc>Truncate the packed single-precision (32-bit) floating-point elements in a, and store the results as packed single-precision floating-point elements in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := TRUNCATE(a[i+31:i])
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5524</id>
<name>_MM512_MASK_UNPACKHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5533</id>
<name>_MM512_MASK_UNPACKHI_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5542</id>
<name>_MM512_MASK_UNPACKHI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5551</id>
<name>_MM512_MASK_UNPACKHI_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5560</id>
<name>_MM512_MASK_UNPACKHI_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5572</id>
<name>_MM512_MASK_UNPACKHI_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5581</id>
<name>_MM512_MASK_UNPACKLO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5590</id>
<name>_MM512_MASK_UNPACKLO_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5599</id>
<name>_MM512_MASK_UNPACKLO_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5608</id>
<name>_MM512_MASK_UNPACKLO_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5617</id>
<name>_MM512_MASK_UNPACKLO_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5629</id>
<name>_MM512_MASK_UNPACKLO_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5646</id>
<name>_MM512_MASK_XOR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5653</id>
<name>_MM512_MASK_XOR_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I src,__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5662</id>
<name>_MM512_MASK_XOR_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D src,__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := src[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5671</id>
<name>_MM512_MASK_XOR_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 src,__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := src[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3927</id>
<name>_MM512_MASK2_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__MMASK32 k,__M512I b</sign>
<instr>VPERMI2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		off := 16*idx[i+4:i]
		dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := idx[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3939</id>
<name>_MM512_MASK2_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__MMASK16 k,__M512I b</sign>
<instr>VPERMI2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := idx[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3951</id>
<name>_MM512_MASK2_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__MMASK8 k,__M512I b</sign>
<instr>VPERMI2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := idx[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3963</id>
<name>_MM512_MASK2_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__MMASK64 k,__M512I b</sign>
<instr>VPERMI2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from a when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		off := 8*idx[i+5:i]
		dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := a[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3975</id>
<name>_MM512_MASK2_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512I idx,__MMASK8 k,__M512D b</sign>
<instr>VPERMI2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set)</desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := idx[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3987</id>
<name>_MM512_MASK2_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512I idx,__MMASK16 k,__M512 b</sign>
<instr>VPERMI2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using writemask k (elements are copied from idx when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := idx[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3255</id>
<name>_MM512_MASK2INT</name>
<cpuid>KNCNI</cpuid>
<ret>int</ret>
<sign>__MMASK16 k1</sign>
<instr>KMOV</instr>
<desc>Converts bit mask k1 into an integer value, storing the results in dst.</desc>
<oper>dst := SignExtend(k1)</oper>
</intrinsic>
<intrinsic>
<id>3256</id>
<name>_MM512_MASK2INT</name>
<cpuid>AVX512F</cpuid>
<ret>int</ret>
<sign>__MMASK16 k1</sign>
<instr>KMOVW</instr>
<desc>Converts bit mask k1 into an integer value, storing the results in dst.</desc>
<oper>dst := SignExtend(k1)</oper>
</intrinsic>
<intrinsic>
<id>2364</id>
<name>_MM512_MASK3_FMADD_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,__M512I c,__MMASK16 k</sign>
<instr>VPMADD231D</instr>
<desc>Multiply packed 32-bit integer elements in a and b, add the intermediate result to packed elements in c and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2375</id>
<name>_MM512_MASK3_FMADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2387</id>
<name>_MM512_MASK3_FMADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2391</id>
<name>_MM512_MASK3_FMADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k,INT rounding</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE 
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2395</id>
<name>_MM512_MASK3_FMADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k,INT rounding</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2427</id>
<name>_MM512_MASK3_FMADDSUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2439</id>
<name>_MM512_MASK3_FMADDSUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2443</id>
<name>_MM512_MASK3_FMADDSUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k,CONST_INT rounding</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE 
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2447</id>
<name>_MM512_MASK3_FMADDSUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k,CONST_INT rounding</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2459</id>
<name>_MM512_MASK3_FMSUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2471</id>
<name>_MM512_MASK3_FMSUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2475</id>
<name>_MM512_MASK3_FMSUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k,INT rounding</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2479</id>
<name>_MM512_MASK3_FMSUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k,INT rounding</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2505</id>
<name>_MM512_MASK3_FMSUBADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2517</id>
<name>_MM512_MASK3_FMSUBADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2521</id>
<name>_MM512_MASK3_FMSUBADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k,CONST_INT rounding</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2525</id>
<name>_MM512_MASK3_FMSUBADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k,CONST_INT rounding</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2537</id>
<name>_MM512_MASK3_FNMADD_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2549</id>
<name>_MM512_MASK3_FNMADD_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2553</id>
<name>_MM512_MASK3_FNMADD_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k,INT rounding</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2557</id>
<name>_MM512_MASK3_FNMADD_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k,INT rounding</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2583</id>
<name>_MM512_MASK3_FNMSUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2595</id>
<name>_MM512_MASK3_FNMSUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2599</id>
<name>_MM512_MASK3_FNMSUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,__M512D c,__MMASK8 k,INT rounding</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := c[i+63:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2603</id>
<name>_MM512_MASK3_FNMSUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,__M512 c,__MMASK16 k,INT rounding</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using writemask k (elements are copied from c when the corresponding mask bit is not set).  Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := c[i+31:i]
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>8</id>
<name>_MM512_MASKZ_ABS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a</sign>
<instr>VPABSW</instr>
<desc>Compute the absolute value of packed 16-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ABS(a[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>17</id>
<name>_MM512_MASKZ_ABS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPABSD</instr>
<desc>Compute the absolute value of packed 32-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ABS(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>26</id>
<name>_MM512_MASKZ_ABS_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPABSQ</instr>
<desc>Compute the absolute value of packed 64-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ABS(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>35</id>
<name>_MM512_MASKZ_ABS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a</sign>
<instr>VPABSB</instr>
<desc>Compute the absolute value of packed 8-bit integers in a, and store the unsigned results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := ABS(a[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>69</id>
<name>_MM512_MASKZ_ADD_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPADDW</instr>
<desc>Add packed 16-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] + b[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>78</id>
<name>_MM512_MASKZ_ADD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPADDD</instr>
<desc>Add packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>87</id>
<name>_MM512_MASKZ_ADD_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPADDQ</instr>
<desc>Add packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>96</id>
<name>_MM512_MASKZ_ADD_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPADDB</instr>
<desc>Add packed 8-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] + b[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>105</id>
<name>_MM512_MASKZ_ADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>117</id>
<name>_MM512_MASKZ_ADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>120</id>
<name>_MM512_MASKZ_ADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VADDPD</instr>
<desc>Add packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] + b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>123</id>
<name>_MM512_MASKZ_ADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VADDPS</instr>
<desc>Add packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] + b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>157</id>
<name>_MM512_MASKZ_ADDS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPADDSW</instr>
<desc>Add packed 16-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>166</id>
<name>_MM512_MASKZ_ADDS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPADDSB</instr>
<desc>Add packed 8-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>175</id>
<name>_MM512_MASKZ_ADDS_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPADDUSW</instr>
<desc>Add packed unsigned 16-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>184</id>
<name>_MM512_MASKZ_ADDS_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPADDUSB</instr>
<desc>Add packed unsigned 8-bit integers in a and b using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>215</id>
<name>_MM512_MASKZ_ALIGNR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b,CONST_INT count</sign>
<instr>VALIGND</instr>
<desc>Concatenate a and b into a 128-byte immediate result, shift the result right by count 32-bit elements, and stores the low 64 bytes (16 elements) in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &amp;gt;&amp;gt; (32*count)
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := temp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>224</id>
<name>_MM512_MASKZ_ALIGNR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b,CONST_INT count</sign>
<instr>VALIGNQ</instr>
<desc>Concatenate a and b into a 128-byte immediate result, shift the result right by count 64-bit elements, and stores the low 64 bytes (8 elements) in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &amp;gt;&amp;gt; (64*count)
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := temp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>233</id>
<name>_MM512_MASKZ_ALIGNR_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b,CONST_INT count</sign>
<instr>VPALIGNR</instr>
<desc>Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by count bytes, and store the low 16 bytes in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 3
	i := j*128
	tmp[255:0] := ((a[i+127:i] &amp;lt;&amp;lt; 128) OR b[i+127:i]) &amp;gt;&amp;gt; (count[7:0]*8)
	tmp_dst[i+127:i] := tmp[127:0]
ENDFOR

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>242</id>
<name>_MM512_MASKZ_AND_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPANDD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] AND b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>249</id>
<name>_MM512_MASKZ_AND_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPANDQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] AND b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>258</id>
<name>_MM512_MASKZ_AND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VANDPD</instr>
<desc>Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>267</id>
<name>_MM512_MASKZ_AND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VANDPS</instr>
<desc>Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>278</id>
<name>_MM512_MASKZ_ANDNOT_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPANDND</instr>
<desc>Compute the bitwise NOT of packed 32-bit integers in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>285</id>
<name>_MM512_MASKZ_ANDNOT_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPANDNQ</instr>
<desc>Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>294</id>
<name>_MM512_MASKZ_ANDNOT_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VANDNPD</instr>
<desc>Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>303</id>
<name>_MM512_MASKZ_ANDNOT_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VANDNPS</instr>
<desc>Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in a and then AND with b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>356</id>
<name>_MM512_MASKZ_AVG_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPAVGW</instr>
<desc>Average packed unsigned 16-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>365</id>
<name>_MM512_MASKZ_AVG_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPAVGB</instr>
<desc>Average packed unsigned 8-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &amp;gt;&amp;gt; 1
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>437</id>
<name>_MM512_MASKZ_BROADCAST_F32X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M128 a</sign>
<instr>VBROADCASTF32X2</instr>
<desc>Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>443</id>
<name>_MM512_MASKZ_BROADCAST_F32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M128 a</sign>
<instr>VBROADCASTF32X4</instr>
<desc>Broadcast the 4 packed single-precision (32-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>446</id>
<name>_MM512_MASKZ_BROADCAST_F32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M256 a</sign>
<instr>VBROADCASTF32X8</instr>
<desc>Broadcast the 8 packed single-precision (32-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 8)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>452</id>
<name>_MM512_MASKZ_BROADCAST_F64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTF64X2</instr>
<desc>Broadcast the 2 packed double-precision (64-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>455</id>
<name>_MM512_MASKZ_BROADCAST_F64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M256D a</sign>
<instr>VBROADCASTF64X4</instr>
<desc>Broadcast the 4 packed double-precision (64-bit) floating-point elements from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 4)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>464</id>
<name>_MM512_MASKZ_BROADCAST_I32X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VBROADCASTI32X2</instr>
<desc>Broadcast the lower 2 packed 32-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 2)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>470</id>
<name>_MM512_MASKZ_BROADCAST_I32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VBROADCASTI32X4</instr>
<desc>Broadcast the 4 packed 32-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 4)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>473</id>
<name>_MM512_MASKZ_BROADCAST_I32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VBROADCASTI32X8</instr>
<desc>Broadcast the 8 packed 32-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	n := (j mod 8)*32
	IF k[j]
		dst[i+31:i] := a[n+31:n]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>479</id>
<name>_MM512_MASKZ_BROADCAST_I64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VBROADCASTI64X2</instr>
<desc>Broadcast the 2 packed 64-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 2)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>482</id>
<name>_MM512_MASKZ_BROADCAST_I64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VBROADCASTI64X4</instr>
<desc>Broadcast the 4 packed 64-bit integers from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	n := (j mod 4)*64
	IF k[j]
		dst[i+63:i] := a[n+63:n]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>496</id>
<name>_MM512_MASKZ_BROADCASTB_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M128I a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast the low packed 8-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>505</id>
<name>_MM512_MASKZ_BROADCASTD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast the low packed 32-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>520</id>
<name>_MM512_MASKZ_BROADCASTQ_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast the low packed 64-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>527</id>
<name>_MM512_MASKZ_BROADCASTSD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M128D a</sign>
<instr>VBROADCASTSD</instr>
<desc>Broadcast the low double-precision (64-bit) floating-point element from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>537</id>
<name>_MM512_MASKZ_BROADCASTSS_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M128 a</sign>
<instr>VBROADCASTSS</instr>
<desc>Broadcast the low single-precision (32-bit) floating-point element from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>546</id>
<name>_MM512_MASKZ_BROADCASTW_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M128I a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1145</id>
<name>_MM512_MASKZ_COMPRESS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPCOMPRESSD</instr>
<desc>Contiguously store the active 32-bit integers in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1151</id>
<name>_MM512_MASKZ_COMPRESS_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPCOMPRESSQ</instr>
<desc>Contiguously store the active 64-bit integers in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1157</id>
<name>_MM512_MASKZ_COMPRESS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCOMPRESSPD</instr>
<desc>Contiguously store the active double-precision (64-bit) floating-point elements in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 64
m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[m+size-1:m] := a[i+63:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1163</id>
<name>_MM512_MASKZ_COMPRESS_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VCOMPRESSPS</instr>
<desc>Contiguously store the active single-precision (32-bit) floating-point elements in a (those with their respective bit set in zeromask k) to dst, and set the remaining elements to zero.</desc>
<oper>size := 32
m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[m+size-1:m] := a[i+31:i]
		m := m + size
	FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1184</id>
<name>_MM512_MASKZ_CONFLICT_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPCONFLICTD</instr>
<desc>Test each 32-bit element of a for equality with all other elements in a closer to the least significant bit using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[i]
		FOR l := 0 to j-1
			m := l*32
			dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
		ENDFOR
		dst[i+31:i+j] := 0
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1193</id>
<name>_MM512_MASKZ_CONFLICT_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPCONFLICTQ</instr>
<desc>Test each 64-bit element of a for equality with all other elements in a closer to the least significant bit using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Each element's comparison forms a zero extended bit vector in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		FOR l := 0 to j-1
			m := l*64
			dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
		ENDFOR
		dst[i+63:i+j] := 0
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1230</id>
<name>_MM512_MASKZ_CVT_ROUNDEPI32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512I a,INT rounding</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1233</id>
<name>_MM512_MASKZ_CVT_ROUNDEPI64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1236</id>
<name>_MM512_MASKZ_CVT_ROUNDEPI64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1239</id>
<name>_MM512_MASKZ_CVT_ROUNDEPU32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512I a,INT rounding</sign>
<instr>VCVTUDQ2PS</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1242</id>
<name>_MM512_MASKZ_CVT_ROUNDEPU64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1245</id>
<name>_MM512_MASKZ_CVT_ROUNDEPU64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512I a,INT rounding</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1251</id>
<name>_MM512_MASKZ_CVT_ROUNDPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1254</id>
<name>_MM512_MASKZ_CVT_ROUNDPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1257</id>
<name>_MM512_MASKZ_CVT_ROUNDPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1260</id>
<name>_MM512_MASKZ_CVT_ROUNDPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1263</id>
<name>_MM512_MASKZ_CVT_ROUNDPD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1268</id>
<name>_MM512_MASKZ_CVT_ROUNDPH_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M256I a,INT sae</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1271</id>
<name>_MM512_MASKZ_CVT_ROUNDPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1274</id>
<name>_MM512_MASKZ_CVT_ROUNDPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1277</id>
<name>_MM512_MASKZ_CVT_ROUNDPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1280</id>
<name>_MM512_MASKZ_CVT_ROUNDPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a,INT rounding</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1283</id>
<name>_MM512_MASKZ_CVT_ROUNDPS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M256 a,INT sae</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1290</id>
<name>_MM512_MASKZ_CVT_ROUNDPS_PH</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1325</id>
<name>_MM512_MASKZ_CVTEPI16_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VPMOVSXWD</instr>
<desc>Sign extend packed 16-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1334</id>
<name>_MM512_MASKZ_CVTEPI16_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXWQ</instr>
<desc>Sign extend packed 16-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1343</id>
<name>_MM512_MASKZ_CVTEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M512I a</sign>
<instr>VPMOVWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1355</id>
<name>_MM512_MASKZ_CVTEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPMOVDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1364</id>
<name>_MM512_MASKZ_CVTEPI32_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVSXDQ</instr>
<desc>Sign extend packed 32-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1373</id>
<name>_MM512_MASKZ_CVTEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPMOVDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1382</id>
<name>_MM512_MASKZ_CVTEPI32_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTDQ2PD</instr>
<desc>Convert packed 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	m := j*64
	IF k[j]
		dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
	ELSE
		dst[m+63:m] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1391</id>
<name>_MM512_MASKZ_CVTEPI32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VCVTDQ2PS</instr>
<desc>Convert packed 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1408</id>
<name>_MM512_MASKZ_CVTEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1417</id>
<name>_MM512_MASKZ_CVTEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1426</id>
<name>_MM512_MASKZ_CVTEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1435</id>
<name>_MM512_MASKZ_CVTEPI64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VCVTQQ2PD</instr>
<desc>Convert packed 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1444</id>
<name>_MM512_MASKZ_CVTEPI64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VCVTQQ2PS</instr>
<desc>Convert packed 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1462</id>
<name>_MM512_MASKZ_CVTEPI8_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M256I a</sign>
<instr>VPMOVSXBW</instr>
<desc>Sign extend packed 8-bit integers in a to packed 16-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := SignExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1471</id>
<name>_MM512_MASKZ_CVTEPI8_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPMOVSXBD</instr>
<desc>Sign extend packed 8-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1480</id>
<name>_MM512_MASKZ_CVTEPI8_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVSXBQ</instr>
<desc>Sign extend packed 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := SignExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1489</id>
<name>_MM512_MASKZ_CVTEPU16_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VPMOVZXWD</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1498</id>
<name>_MM512_MASKZ_CVTEPU16_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXWQ</instr>
<desc>Zero extend packed unsigned 16-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+15:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1507</id>
<name>_MM512_MASKZ_CVTEPU32_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VPMOVZXDQ</instr>
<desc>Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+31:l])
	ELSE 
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1516</id>
<name>_MM512_MASKZ_CVTEPU32_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M256I a</sign>
<instr>VCVTUDQ2PD</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1519</id>
<name>_MM512_MASKZ_CVTEPU32_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VCVTUDQ2PS</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1530</id>
<name>_MM512_MASKZ_CVTEPU64_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VCVTUQQ2PD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1539</id>
<name>_MM512_MASKZ_CVTEPU64_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VCVTUQQ2PS</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1548</id>
<name>_MM512_MASKZ_CVTEPU8_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M256I a</sign>
<instr>VPMOVZXBW</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*8
	l := j*16
	IF k[j]
		dst[l+15:l] := ZeroExtend(a[i+7:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1557</id>
<name>_MM512_MASKZ_CVTEPU8_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M128I a</sign>
<instr>VPMOVZXBD</instr>
<desc>Zero extend packed unsigned 8-bit integers in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1566</id>
<name>_MM512_MASKZ_CVTEPU8_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M128I a</sign>
<instr>VPMOVZXBQ</instr>
<desc>Zero extend packed unsigned 8-bit integers in the low 8 bytes of a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[l+7:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1589</id>
<name>_MM512_MASKZ_CVTPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1598</id>
<name>_MM512_MASKZ_CVTPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1607</id>
<name>_MM512_MASKZ_CVTPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1616</id>
<name>_MM512_MASKZ_CVTPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1626</id>
<name>_MM512_MASKZ_CVTPD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTPD2PS</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*32
	l := j*64
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1637</id>
<name>_MM512_MASKZ_CVTPH_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M256I a</sign>
<instr>VCVTPH2PS</instr>
<desc>Convert packed half-precision (16-bit) floating-point elements in a to packed single-precision (32-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	m := j*16
	IF k[j]
		dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1651</id>
<name>_MM512_MASKZ_CVTPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VCVTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1660</id>
<name>_MM512_MASKZ_CVTPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1669</id>
<name>_MM512_MASKZ_CVTPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VCVTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1678</id>
<name>_MM512_MASKZ_CVTPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1683</id>
<name>_MM512_MASKZ_CVTPS_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTPS2PD</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed double-precision (64-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1692</id>
<name>_MM512_MASKZ_CVTPS_PH</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VCVTPS2PH</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed half-precision (16-bit) floating-point elements, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 16*j
	l := 32*j
	IF k[j]
		dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1719</id>
<name>_MM512_MASKZ_CVTSEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M512I a</sign>
<instr>VPMOVSWB</instr>
<desc>Convert packed 16-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1731</id>
<name>_MM512_MASKZ_CVTSEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPMOVSDW</instr>
<desc>Convert packed 32-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1740</id>
<name>_MM512_MASKZ_CVTSEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPMOVSDB</instr>
<desc>Convert packed 32-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1755</id>
<name>_MM512_MASKZ_CVTSEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQW</instr>
<desc>Convert packed 64-bit integers in a to packed 16-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1764</id>
<name>_MM512_MASKZ_CVTSEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQD</instr>
<desc>Convert packed 64-bit integers in a to packed 32-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1773</id>
<name>_MM512_MASKZ_CVTSEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVSQB</instr>
<desc>Convert packed 64-bit integers in a to packed 8-bit integers with signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>1812</id>
<name>_MM512_MASKZ_CVTT_ROUNDPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1815</id>
<name>_MM512_MASKZ_CVTT_ROUNDPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1818</id>
<name>_MM512_MASKZ_CVTT_ROUNDPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := 32*i
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1821</id>
<name>_MM512_MASKZ_CVTT_ROUNDPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a,INT sae</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1824</id>
<name>_MM512_MASKZ_CVTT_ROUNDPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a,INT sae</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := 32*i
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1827</id>
<name>_MM512_MASKZ_CVTT_ROUNDPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a,INT sae</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1830</id>
<name>_MM512_MASKZ_CVTT_ROUNDPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a,INT sae</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := 32*i
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1833</id>
<name>_MM512_MASKZ_CVTT_ROUNDPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a,INT sae</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1855</id>
<name>_MM512_MASKZ_CVTTPD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2DQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1864</id>
<name>_MM512_MASKZ_CVTTPD_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2QQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1873</id>
<name>_MM512_MASKZ_CVTTPD_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2UDQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 32*j
	l := 64*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1882</id>
<name>_MM512_MASKZ_CVTTPD_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VCVTTPD2UQQ</instr>
<desc>Convert packed double-precision (64-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1892</id>
<name>_MM512_MASKZ_CVTTPS_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VCVTTPS2DQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1901</id>
<name>_MM512_MASKZ_CVTTPS_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2QQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1910</id>
<name>_MM512_MASKZ_CVTTPS_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VCVTTPS2UDQ</instr>
<desc>Convert packed double-precision (32-bit) floating-point elements in a to packed unsigned 32-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1919</id>
<name>_MM512_MASKZ_CVTTPS_EPU64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M256 a</sign>
<instr>VCVTTPS2UQQ</instr>
<desc>Convert packed single-precision (32-bit) floating-point elements in a to packed unsigned 64-bit integers with truncation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	l := j*32
	IF k[j]
		dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>1946</id>
<name>_MM512_MASKZ_CVTUSEPI16_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m256i</ret>
<sign>__MMASK32 k,__M512I a</sign>
<instr>VPMOVUSWB</instr>
<desc>Convert packed unsigned 16-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := 16*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1958</id>
<name>_MM512_MASKZ_CVTUSEPI32_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPMOVUSDW</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>1967</id>
<name>_MM512_MASKZ_CVTUSEPI32_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPMOVUSDB</instr>
<desc>Convert packed unsigned 32-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := 32*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1982</id>
<name>_MM512_MASKZ_CVTUSEPI64_EPI16</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQW</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 16-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 16*j
	IF k[j]
		dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
	ELSE
		dst[l+15:l] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>1991</id>
<name>_MM512_MASKZ_CVTUSEPI64_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQD</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 32-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 32*j
	IF k[j]
		dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
	ELSE
		dst[l+31:l] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2000</id>
<name>_MM512_MASKZ_CVTUSEPI64_EPI8</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPMOVUSQB</instr>
<desc>Convert packed unsigned 64-bit integers in a to packed unsigned 8-bit integers with unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := 64*j
	l := 8*j
	IF k[j]
		dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
	ELSE
		dst[l+7:l] := 0
	FI
ENDFOR
dst[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>2018</id>
<name>_MM512_MASKZ_DBSAD_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b,INT imm8</sign>
<instr>VDBPSADBW</instr>
<desc>Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b, and store the 16-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from a, and the last two SADs use the uppper 8-bit quadruplet of the lane from a. Quadruplets from b are selected from within 128-bit lanes according to the control in imm8, and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.
	</desc>
<oper>FOR j := 0 to 3
	i := j*128
	tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
	tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
	tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
	tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
ENDFOR

FOR j := 0 to 7
	i := j*64
	tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
				 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
	
	tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
				 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
	
	tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
				 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
	
	tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
				 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2055</id>
<name>_MM512_MASKZ_DIV_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2064</id>
<name>_MM512_MASKZ_DIV_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2067</id>
<name>_MM512_MASKZ_DIV_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VDIVPD</instr>
<desc>Divide packed double-precision (64-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	IF k[j]
		dst[i+63:i] := a[i+63:i] / b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2070</id>
<name>_MM512_MASKZ_DIV_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VDIVPS</instr>
<desc>Divide packed single-precision (32-bit) floating-point elements in a by packed elements in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	IF k[j]
		dst[i+31:i] := a[i+31:i] / b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2148</id>
<name>_MM512_MASKZ_EXP2A23_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VEXP2PD</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
	ELSE
		dst[i+63:i] := 0;
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2151</id>
<name>_MM512_MASKZ_EXP2A23_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VEXP2PS</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
	ELSE
		dst[i*31:i] := 0;
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2154</id>
<name>_MM512_MASKZ_EXP2A23_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VEXP2PD</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
	ELSE
		dst[i+63:i] := 0;
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2157</id>
<name>_MM512_MASKZ_EXP2A23_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VEXP2PS</instr>
<desc>Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
	ELSE
		dst[i*31:i] := 0;
	FI
ENDFOR;
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2163</id>
<name>_MM512_MASKZ_EXPAND_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2169</id>
<name>_MM512_MASKZ_EXPAND_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2175</id>
<name>_MM512_MASKZ_EXPAND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[m+63:m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2181</id>
<name>_MM512_MASKZ_EXPAND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from a (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[m+31:m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2187</id>
<name>_MM512_MASKZ_EXPANDLOADU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDD</instr>
<desc>Load contiguous active 32-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2193</id>
<name>_MM512_MASKZ_EXPANDLOADU_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VPEXPANDQ</instr>
<desc>Load contiguous active 64-bit integers from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2199</id>
<name>_MM512_MASKZ_EXPANDLOADU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPD</instr>
<desc>Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
		m := m + 64
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2205</id>
<name>_MM512_MASKZ_EXPANDLOADU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VEXPANDPS</instr>
<desc>Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at mem_addr (those with their respective bit set in mask k), and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>m := 0
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
		m := m + 32
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2272</id>
<name>_MM512_MASKZ_EXTRACTF32X4_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m128</ret>
<sign>__MMASK8 k,__M512 a,INT imm8</sign>
<instr>VEXTRACTF32X4</instr>
<desc>Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2275</id>
<name>_MM512_MASKZ_EXTRACTF32X8_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256</ret>
<sign>__MMASK8 k,__M512 a,INT imm8</sign>
<instr>VEXTRACTF32X8</instr>
<desc>Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2281</id>
<name>_MM512_MASKZ_EXTRACTF64X2_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VEXTRACTF64X2</instr>
<desc>Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2284</id>
<name>_MM512_MASKZ_EXTRACTF64X4_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m256d</ret>
<sign>__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VEXTRACTF64X4</instr>
<desc>Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2291</id>
<name>_MM512_MASKZ_EXTRACTI32X4_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI32X4</instr>
<desc>Extract 128 bits (composed of 4 packed 32-bit integers) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2294</id>
<name>_MM512_MASKZ_EXTRACTI32X8_EPI32</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI32X8</instr>
<desc>Extract 256 bits (composed of 8 packed 32-bit integers) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC

FOR j := 0 to 7
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2300</id>
<name>_MM512_MASKZ_EXTRACTI64X2_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m128d</ret>
<sign>__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI64X2</instr>
<desc>Extract 128 bits (composed of 2 packed 64-bit integers) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC

FOR j := 0 to 1
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:128] := 0</oper>
</intrinsic>
<intrinsic>
<id>2303</id>
<name>_MM512_MASKZ_EXTRACTI64X4_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m256i</ret>
<sign>__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VEXTRACTI64X4</instr>
<desc>Extract 256 bits (composed of 4 packed 64-bit integers) from a, selected with imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>CASE imm8[7:0] of
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:256] := 0</oper>
</intrinsic>
<intrinsic>
<id>2320</id>
<name>_MM512_MASKZ_FIXUPIMM_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512I c,INT imm8</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2329</id>
<name>_MM512_MASKZ_FIXUPIMM_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512I c,INT imm8</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2332</id>
<name>_MM512_MASKZ_FIXUPIMM_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMPD</instr>
<desc>Fix up packed double-precision (64-bit) floating-point elements in a and b using packed 64-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN := 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
	tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
	CASE(tsrc[63:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[63:0] := src1[63:0]
	1 : dest[63:0] := tsrc[63:0]
	2 : dest[63:0] := QNaN(tsrc[63:0])
	3 : dest[63:0] := QNAN_Indefinite
	4 : dest[63:0] := -INF
	5 : dest[63:0] := +INF
	6 : dest[63:0] := tsrc.sign? â€“INF : +INF
	7 : dest[63:0] := -0
	8 : dest[63:0] := +0
	9 : dest[63:0] := -1
	10: dest[63:0] := +1
	11: dest[63:0] := 1â„2
	12: dest[63:0] := 90.0
	13: dest[63:0] := PI/2
	14: dest[63:0] := MAX_FLOAT
	15: dest[63:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2335</id>
<name>_MM512_MASKZ_FIXUPIMM_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512I c,INT imm8,INT rounding</sign>
<instr>VFIXUPIMMPS</instr>
<desc>Fix up packed single-precision (32-bit) floating-point elements in a and b using packed 32-bit integers in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). imm8 is used to set the required flags reporting.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>enum TOKEN_TYPE {
	QNAN_TOKEN := 0, 
	SNAN_TOKEN L= 1, 
	ZERO_VALUE_TOKEN := 2, 
	ONE_VALUE_TOKEN := 3, 
	NEG_INF_TOKEN := 4, 
	POS_INF_TOKEN := 5, 
	NEG_VALUE_TOKEN := 6, 
	POS_VALUE_TOKEN := 7
}
FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
	tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
	CASE(tsrc[31:0] of TOKEN_TYPE)
	QNAN_TOKEN:j := 0
	SNAN_TOKEN:j := 1
	ZERO_VALUE_TOKEN: j := 2
	ONE_VALUE_TOKEN: j := 3
	NEG_INF_TOKEN: j := 4
	POS_INF_TOKEN: j := 5
	NEG_VALUE_TOKEN: j := 6
	POS_VALUE_TOKEN: j := 7
	ESAC
	
	token_response[3:0] := src3[3+4*j:4*j]
	
	CASE(token_response[3:0]) of
	0 : dest[31:0] := src1[31:0]
	1 : dest[31:0] := tsrc[31:0]
	2 : dest[31:0] := QNaN(tsrc[31:0])
	3 : dest[31:0] := QNAN_Indefinite
	4 : dest[31:0] := -INF
	5 : dest[31:0] := +INF
	6 : dest[31:0] := tsrc.sign? â€“INF : +INF
	7 : dest[31:0] := -0
	8 : dest[31:0] := +0
	9 : dest[31:0] := -1
	10: dest[31:0] := +1
	11: dest[31:0] := 1â„2
	12: dest[31:0] := 90.0
	13: dest[31:0] := PI/2
	14: dest[31:0] := MAX_FLOAT
	15: dest[31:0] := -MAX_FLOAT
	ESAC
	
	CASE(tsrc[31:0] of TOKEN_TYPE)
	ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
	ZERO_VALUE_TOKEN: if imm8[1] then set #IE
	ONE_VALUE_TOKEN: if imm8[2] then set #ZE
	ONE_VALUE_TOKEN: if imm8[3] then set #IE
	SNAN_TOKEN: if imm8[4] then set #IE
	NEG_INF_TOKEN: if imm8[5] then set #IE
	NEG_VALUE_TOKEN: if imm8[6] then set #IE
	POS_INF_TOKEN: if imm8[7] then set #IE
	ESAC
	RETURN dest[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2376</id>
<name>_MM512_MASKZ_FMADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2388</id>
<name>_MM512_MASKZ_FMADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2392</id>
<name>_MM512_MASKZ_FMADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMADD132PD, VFMADD213PD, VFMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2396</id>
<name>_MM512_MASKZ_FMADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMADD132PS, VFMADD213PS, VFMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the intermediate result to packed elements in c, and store the results in a using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		a[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2428</id>
<name>_MM512_MASKZ_FMADDSUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2440</id>
<name>_MM512_MASKZ_FMADDSUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2444</id>
<name>_MM512_MASKZ_FMADDSUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2448</id>
<name>_MM512_MASKZ_FMADDSUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively add and subtract packed elements in c to/from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2460</id>
<name>_MM512_MASKZ_FMSUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2472</id>
<name>_MM512_MASKZ_FMSUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2476</id>
<name>_MM512_MASKZ_FMSUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMSUB132PD, VFMSUB213PD, VFMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2480</id>
<name>_MM512_MASKZ_FMSUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMSUB132PS, VFMSUB213PS, VFMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2506</id>
<name>_MM512_MASKZ_FMSUBADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2518</id>
<name>_MM512_MASKZ_FMSUBADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2522</id>
<name>_MM512_MASKZ_FMSUBADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF (j is even) 
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
		ELSE
			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2526</id>
<name>_MM512_MASKZ_FMSUBADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, alternatively subtract and add packed elements in c from/to the intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF (j is even) 
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
		ELSE
			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2538</id>
<name>_MM512_MASKZ_FNMADD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2550</id>
<name>_MM512_MASKZ_FNMADD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2554</id>
<name>_MM512_MASKZ_FNMADD_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFNMADD132PD, VFNMADD213PD, VFNMADD231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2558</id>
<name>_MM512_MASKZ_FNMADD_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFNMADD132PS, VFNMADD213PS, VFNMADD231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, add the negated intermediate result to packed elements in c, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2584</id>
<name>_MM512_MASKZ_FNMSUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2596</id>
<name>_MM512_MASKZ_FNMSUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2600</id>
<name>_MM512_MASKZ_FNMSUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,__M512D c,CONST_INT rounding</sign>
<instr>VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2604</id>
<name>_MM512_MASKZ_FNMSUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,__M512 c,CONST_INT rounding</sign>
<instr>VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, subtract packed elements in c from the negated intermediate result, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR	
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2655</id>
<name>_MM512_MASKZ_GETEXP_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2664</id>
<name>_MM512_MASKZ_GETEXP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2667</id>
<name>_MM512_MASKZ_GETEXP_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VGETEXPPD</instr>
<desc>Convert the exponent of each packed double-precision (64-bit) floating-point element in a to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ConvertExpFP64(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2670</id>
<name>_MM512_MASKZ_GETEXP_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VGETEXPPS</instr>
<desc>Convert the exponent of each packed single-precision (32-bit) floating-point element in a to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates floor(log2(x)) for each element.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ConvertExpFP32(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2691</id>
<name>_MM512_MASKZ_GETMANT_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2700</id>
<name>_MM512_MASKZ_GETMANT_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2703</id>
<name>_MM512_MASKZ_GETMANT_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTPD</instr>
<desc>Normalize the mantissas of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2706</id>
<name>_MM512_MASKZ_GETMANT_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,_MM_MANTISSA_NORM_ENUM interv,_MM_MANTISSA_SIGN_ENUM sc,INT rounding</sign>
<instr>VGETMANTPS</instr>
<desc>Normalize the mantissas of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates Â±(2^k)*|x.significand|, where k depends on the interval range defined by interv and the sign depends on sc and the source sign.
	The mantissa is normalized to the interval specified by interv, which can take the following values:    _MM_MANT_NORM_1_2     // interval [1, 2)
    _MM_MANT_NORM_p5_2    // interval [0.5, 2)
    _MM_MANT_NORM_p5_1    // interval [0.5, 1)
    _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by sc which can take the following values:    _MM_MANT_SIGN_src     // sign = sign(src)
    _MM_MANT_SIGN_zero    // sign = 0
    _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2945</id>
<name>_MM512_MASKZ_INSERTF32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M128 b,INT imm8</sign>
<instr>VINSERTF32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2948</id>
<name>_MM512_MASKZ_INSERTF32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M256 b,INT imm8</sign>
<instr>VINSERTF32X8</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[7:0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2954</id>
<name>_MM512_MASKZ_INSERTF64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M128D b,INT imm8</sign>
<instr>VINSERTF64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2957</id>
<name>_MM512_MASKZ_INSERTF64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M256D b,INT imm8</sign>
<instr>VINSERTF64X4</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[0]) of
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2964</id>
<name>_MM512_MASKZ_INSERTI32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M128I b,INT imm8</sign>
<instr>VINSERTI32X4</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 4 packed 32-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2967</id>
<name>_MM512_MASKZ_INSERTI32X8</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M256I b,INT imm8</sign>
<instr>VINSERTI32X8</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 8 packed 32-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[7:0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2973</id>
<name>_MM512_MASKZ_INSERTI64X2</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M128I b,INT imm8</sign>
<instr>VINSERTI64X2</instr>
<desc>Copy a to tmp, then insert 128 bits (composed of 2 packed 64-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[1:0]) of
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>2976</id>
<name>_MM512_MASKZ_INSERTI64X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M256I b,INT imm8</sign>
<instr>VINSERTI64X4</instr>
<desc>Copy a to tmp, then insert 256 bits (composed of 4 packed 64-bit integers) from b into tmp at the location specified by imm8.  Store tmp to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	</desc>
<oper>tmp[511:0] := a[511:0]
CASE (imm8[0]) of
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3032</id>
<name>_MM512_MASKZ_LOAD_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA32</instr>
<desc>Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3039</id>
<name>_MM512_MASKZ_LOAD_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQA64</instr>
<desc>Load packed 64-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3048</id>
<name>_MM512_MASKZ_LOAD_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3058</id>
<name>_MM512_MASKZ_LOAD_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVAPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3087</id>
<name>_MM512_MASKZ_LOADU_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU16</instr>
<desc>Load packed 16-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3093</id>
<name>_MM512_MASKZ_LOADU_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU32</instr>
<desc>Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3099</id>
<name>_MM512_MASKZ_LOADU_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU64</instr>
<desc>Load packed 64-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3105</id>
<name>_MM512_MASKZ_LOADU_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVDQU8</instr>
<desc>Load packed 8-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3114</id>
<name>_MM512_MASKZ_LOADU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPD</instr>
<desc>Load packed double-precision (64-bit) floating-point elements from memoy into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3123</id>
<name>_MM512_MASKZ_LOADU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,VOID_CONST_PTR mem_addr</sign>
<instr>VMOVUPS</instr>
<desc>Load packed single-precision (32-bit) floating-point elements from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3204</id>
<name>_MM512_MASKZ_LZCNT_EPI32</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VPLZCNTD</instr>
<desc>Counts the number of leading zero bits in each packed 32-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		tmp := 31
		dst[i+31:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+31:i] := dst[i+31:i] + 1
		OD
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3213</id>
<name>_MM512_MASKZ_LZCNT_EPI64</name>
<cpuid>AVX512CD</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VPLZCNTQ</instr>
<desc>Counts the number of leading zero bits in each packed 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp := 63
		dst[i+63:i] := 0
		DO WHILE (tmp &amp;gt;= 0 AND a[i+tmp] == 0)
			tmp := tmp - 1
			dst[i+63:i] := dst[i+63:i] + 1
		OD
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3224</id>
<name>_MM512_MASKZ_MADD_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMADDWD</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3234</id>
<name>_MM512_MASKZ_MADD52HI_EPU64</name>
<cpuid>AVX512IFMA52</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b,__M512I c</sign>
<instr>VPMADD52HUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3243</id>
<name>_MM512_MASKZ_MADD52LO_EPU64</name>
<cpuid>AVX512IFMA52</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b,__M512I c</sign>
<instr>VPMADD52LUQ</instr>
<desc>Multiply packed unsigned 52-bit integers in each 64-bit element of b and c to form a 104-bit intermediate result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit integer in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
		dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3252</id>
<name>_MM512_MASKZ_MADDUBS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMADDUBSW</instr>
<desc>Multiply packed unsigned 8-bit integers in a by packed signed 8-bit integers in b, producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3283</id>
<name>_MM512_MASKZ_MAX_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3292</id>
<name>_MM512_MASKZ_MAX_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0 
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3301</id>
<name>_MM512_MASKZ_MAX_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3310</id>
<name>_MM512_MASKZ_MAX_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3319</id>
<name>_MM512_MASKZ_MAX_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;gt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3328</id>
<name>_MM512_MASKZ_MAX_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;gt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3337</id>
<name>_MM512_MASKZ_MAX_EPU64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;gt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3346</id>
<name>_MM512_MASKZ_MAX_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;gt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3355</id>
<name>_MM512_MASKZ_MAX_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3365</id>
<name>_MM512_MASKZ_MAX_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3369</id>
<name>_MM512_MASKZ_MAX_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT sae</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3372</id>
<name>_MM512_MASKZ_MAX_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT sae</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3397</id>
<name>_MM512_MASKZ_MIN_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3406</id>
<name>_MM512_MASKZ_MIN_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3415</id>
<name>_MM512_MASKZ_MIN_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3424</id>
<name>_MM512_MASKZ_MIN_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3433</id>
<name>_MM512_MASKZ_MIN_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF a[i+15:i] &amp;lt; b[i+15:i]
			dst[i+15:i] := a[i+15:i]
		ELSE
			dst[i+15:i] := b[i+15:i]
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3442</id>
<name>_MM512_MASKZ_MIN_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF a[i+31:i] &amp;lt; b[i+31:i]
			dst[i+31:i] := a[i+31:i]
		ELSE
			dst[i+31:i] := b[i+31:i]
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3451</id>
<name>_MM512_MASKZ_MIN_EPU64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF a[i+63:i] &amp;lt; b[i+63:i]
			dst[i+63:i] := a[i+63:i]
		ELSE
			dst[i+63:i] := b[i+63:i]
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3460</id>
<name>_MM512_MASKZ_MIN_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF a[i+7:i] &amp;lt; b[i+7:i]
			dst[i+7:i] := a[i+7:i]
		ELSE
			dst[i+7:i] := b[i+7:i]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3469</id>
<name>_MM512_MASKZ_MIN_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3479</id>
<name>_MM512_MASKZ_MIN_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3483</id>
<name>_MM512_MASKZ_MIN_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT sae</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3486</id>
<name>_MM512_MASKZ_MIN_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT sae</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3507</id>
<name>_MM512_MASKZ_MOV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a</sign>
<instr>VMOVDQU16</instr>
<desc>Move packed 16-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3513</id>
<name>_MM512_MASKZ_MOV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a</sign>
<instr>VMOVDQA32</instr>
<desc>Move packed 32-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3519</id>
<name>_MM512_MASKZ_MOV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a</sign>
<instr>VMOVDQA64</instr>
<desc>Move packed 64-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3525</id>
<name>_MM512_MASKZ_MOV_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a</sign>
<instr>VMOVDQU8</instr>
<desc>Move packed 8-bit integers from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3531</id>
<name>_MM512_MASKZ_MOV_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VMOVAPD</instr>
<desc>Move packed double-precision (64-bit) floating-point elements from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3537</id>
<name>_MM512_MASKZ_MOV_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VMOVAPS</instr>
<desc>Move packed single-precision (32-bit) floating-point elements from a into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3552</id>
<name>_MM512_MASKZ_MOVEDUP_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
tmp[319:256] := a[319:256] 
tmp[383:320] := a[319:256] 
tmp[447:384] := a[447:384]
tmp[511:448] := a[447:384]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3561</id>
<name>_MM512_MASKZ_MOVEHDUP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
tmp[287:256] := a[319:288] 
tmp[319:288] := a[319:288] 
tmp[351:320] := a[383:352] 
tmp[383:352] := a[383:352] 
tmp[415:384] := a[447:416] 
tmp[447:416] := a[447:416] 
tmp[479:448] := a[511:480]
tmp[511:480] := a[511:480]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3571</id>
<name>_MM512_MASKZ_MOVELDUP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
tmp[287:256] := a[287:256] 
tmp[319:288] := a[287:256] 
tmp[351:320] := a[351:320] 
tmp[383:352] := a[351:320] 
tmp[415:384] := a[415:384] 
tmp[447:416] := a[415:384] 
tmp[479:448] := a[479:448]
tmp[511:480] := a[479:448]
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3616</id>
<name>_MM512_MASKZ_MUL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3625</id>
<name>_MM512_MASKZ_MUL_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3634</id>
<name>_MM512_MASKZ_MUL_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3643</id>
<name>_MM512_MASKZ_MUL_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3646</id>
<name>_MM512_MASKZ_MUL_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] * b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3649</id>
<name>_MM512_MASKZ_MUL_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] * b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3671</id>
<name>_MM512_MASKZ_MULHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3682</id>
<name>_MM512_MASKZ_MULHI_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[31:16]
	ELSE
		dst[i+15:i] := o
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3695</id>
<name>_MM512_MASKZ_MULHRS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
		dst[i+15:i] := tmp[16:1]
	ELSE
		dst[i+15:i] := 9
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3705</id>
<name>_MM512_MASKZ_MULLO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		tmp[31:0] := a[i+15:i] * b[i+15:i]
		dst[i+15:i] := tmp[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3714</id>
<name>_MM512_MASKZ_MULLO_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		tmp[63:0] := a[i+31:i] * b[i+31:i]
		dst[i+31:i] := tmp[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3723</id>
<name>_MM512_MASKZ_MULLO_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		tmp[127:0] := a[i+63:i] * b[i+63:i]
		dst[i+63:i] := tmp[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3735</id>
<name>_MM512_MASKZ_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR i := 0 to 7
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		IF k[i*8+j]
			dst[q+j*8+7:q+j*8] := tmp8[7:0]
		ELSE
			dst[q+j*8+7:q+j*8] := 0
		FI
	ENDFOR
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3747</id>
<name>_MM512_MASKZ_OR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] OR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3754</id>
<name>_MM512_MASKZ_OR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] OR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3763</id>
<name>_MM512_MASKZ_OR_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3772</id>
<name>_MM512_MASKZ_OR_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3785</id>
<name>_MM512_MASKZ_PACKS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
tmp_dst[263:256] := Saturate_Int16_To_Int8 (a[271:256])
tmp_dst[271:264] := Saturate_Int16_To_Int8 (a[287:272])
tmp_dst[279:272] := Saturate_Int16_To_Int8 (a[303:288])
tmp_dst[287:280] := Saturate_Int16_To_Int8 (a[319:304])
tmp_dst[295:288] := Saturate_Int16_To_Int8 (a[335:320])
tmp_dst[303:296] := Saturate_Int16_To_Int8 (a[351:336])
tmp_dst[311:304] := Saturate_Int16_To_Int8 (a[367:352])
tmp_dst[319:312] := Saturate_Int16_To_Int8 (a[383:368])
tmp_dst[327:320] := Saturate_Int16_To_Int8 (b[271:256])
tmp_dst[335:328] := Saturate_Int16_To_Int8 (b[287:272])
tmp_dst[343:336] := Saturate_Int16_To_Int8 (b[303:288])
tmp_dst[351:344] := Saturate_Int16_To_Int8 (b[319:304])
tmp_dst[359:352] := Saturate_Int16_To_Int8 (b[335:320])
tmp_dst[367:360] := Saturate_Int16_To_Int8 (b[351:336])
tmp_dst[375:368] := Saturate_Int16_To_Int8 (b[367:352])
tmp_dst[383:376] := Saturate_Int16_To_Int8 (b[383:368])
tmp_dst[391:384] := Saturate_Int16_To_Int8 (a[399:384])
tmp_dst[399:392] := Saturate_Int16_To_Int8 (a[415:400])
tmp_dst[407:400] := Saturate_Int16_To_Int8 (a[431:416])
tmp_dst[415:408] := Saturate_Int16_To_Int8 (a[447:432])
tmp_dst[423:416] := Saturate_Int16_To_Int8 (a[463:448])
tmp_dst[431:424] := Saturate_Int16_To_Int8 (a[479:464])
tmp_dst[439:432] := Saturate_Int16_To_Int8 (a[495:480])
tmp_dst[447:440] := Saturate_Int16_To_Int8 (a[511:496])
tmp_dst[455:448] := Saturate_Int16_To_Int8 (b[399:384])
tmp_dst[463:456] := Saturate_Int16_To_Int8 (b[415:400])
tmp_dst[471:464] := Saturate_Int16_To_Int8 (b[431:416])
tmp_dst[479:472] := Saturate_Int16_To_Int8 (b[447:432])
tmp_dst[487:480] := Saturate_Int16_To_Int8 (b[463:448])
tmp_dst[495:488] := Saturate_Int16_To_Int8 (b[479:464])
tmp_dst[503:496] := Saturate_Int16_To_Int8 (b[495:480])
tmp_dst[511:504] := Saturate_Int16_To_Int8 (b[511:496])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3794</id>
<name>_MM512_MASKZ_PACKS_EPI32</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
tmp_dst[271:256] := Saturate_Int32_To_Int16 (a[287:256])
tmp_dst[287:272] := Saturate_Int32_To_Int16 (a[319:288])
tmp_dst[303:288] := Saturate_Int32_To_Int16 (a[351:320])
tmp_dst[319:304] := Saturate_Int32_To_Int16 (a[383:352])
tmp_dst[335:320] := Saturate_Int32_To_Int16 (b[287:256])
tmp_dst[351:336] := Saturate_Int32_To_Int16 (b[319:288])
tmp_dst[367:352] := Saturate_Int32_To_Int16 (b[351:320])
tmp_dst[383:368] := Saturate_Int32_To_Int16 (b[383:352])
tmp_dst[399:384] := Saturate_Int32_To_Int16 (a[415:384])
tmp_dst[415:400] := Saturate_Int32_To_Int16 (a[447:416])
tmp_dst[431:416] := Saturate_Int32_To_Int16 (a[479:448])
tmp_dst[447:432] := Saturate_Int32_To_Int16 (a[511:480])
tmp_dst[463:448] := Saturate_Int32_To_Int16 (b[415:384])
tmp_dst[479:464] := Saturate_Int32_To_Int16 (b[447:416])
tmp_dst[495:480] := Saturate_Int32_To_Int16 (b[479:448])
tmp_dst[511:496] := Saturate_Int32_To_Int16 (b[511:480])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3824</id>
<name>_MM512_MASKZ_PACKUS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
tmp_dst[263:256] := Saturate_Int16_To_UnsignedInt8 (a[271:256])
tmp_dst[271:264] := Saturate_Int16_To_UnsignedInt8 (a[287:272])
tmp_dst[279:272] := Saturate_Int16_To_UnsignedInt8 (a[303:288])
tmp_dst[287:280] := Saturate_Int16_To_UnsignedInt8 (a[319:304])
tmp_dst[295:288] := Saturate_Int16_To_UnsignedInt8 (a[335:320])
tmp_dst[303:296] := Saturate_Int16_To_UnsignedInt8 (a[351:336])
tmp_dst[311:304] := Saturate_Int16_To_UnsignedInt8 (a[367:352])
tmp_dst[319:312] := Saturate_Int16_To_UnsignedInt8 (a[383:368])
tmp_dst[327:320] := Saturate_Int16_To_UnsignedInt8 (b[271:256])
tmp_dst[335:328] := Saturate_Int16_To_UnsignedInt8 (b[287:272])
tmp_dst[343:336] := Saturate_Int16_To_UnsignedInt8 (b[303:288])
tmp_dst[351:344] := Saturate_Int16_To_UnsignedInt8 (b[319:304])
tmp_dst[359:352] := Saturate_Int16_To_UnsignedInt8 (b[335:320])
tmp_dst[367:360] := Saturate_Int16_To_UnsignedInt8 (b[351:336])
tmp_dst[375:368] := Saturate_Int16_To_UnsignedInt8 (b[367:352])
tmp_dst[383:376] := Saturate_Int16_To_UnsignedInt8 (b[383:368])
tmp_dst[391:384] := Saturate_Int16_To_UnsignedInt8 (a[399:384])
tmp_dst[399:392] := Saturate_Int16_To_UnsignedInt8 (a[415:400])
tmp_dst[407:400] := Saturate_Int16_To_UnsignedInt8 (a[431:416])
tmp_dst[415:408] := Saturate_Int16_To_UnsignedInt8 (a[447:432])
tmp_dst[423:416] := Saturate_Int16_To_UnsignedInt8 (a[463:448])
tmp_dst[431:424] := Saturate_Int16_To_UnsignedInt8 (a[479:464])
tmp_dst[439:432] := Saturate_Int16_To_UnsignedInt8 (a[495:480])
tmp_dst[447:440] := Saturate_Int16_To_UnsignedInt8 (a[511:496])
tmp_dst[455:448] := Saturate_Int16_To_UnsignedInt8 (b[399:384])
tmp_dst[463:456] := Saturate_Int16_To_UnsignedInt8 (b[415:400])
tmp_dst[471:464] := Saturate_Int16_To_UnsignedInt8 (b[431:416])
tmp_dst[479:472] := Saturate_Int16_To_UnsignedInt8 (b[447:432])
tmp_dst[487:480] := Saturate_Int16_To_UnsignedInt8 (b[463:448])
tmp_dst[495:488] := Saturate_Int16_To_UnsignedInt8 (b[479:464])
tmp_dst[503:496] := Saturate_Int16_To_UnsignedInt8 (b[495:480])
tmp_dst[511:504] := Saturate_Int16_To_UnsignedInt8 (b[511:496])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3833</id>
<name>_MM512_MASKZ_PACKUS_EPI32</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
tmp_dst[271:256] := Saturate_Int32_To_UnsignedInt16 (a[287:256])
tmp_dst[287:272] := Saturate_Int32_To_UnsignedInt16 (a[319:288])
tmp_dst[303:288] := Saturate_Int32_To_UnsignedInt16 (a[351:320])
tmp_dst[319:304] := Saturate_Int32_To_UnsignedInt16 (a[383:352])
tmp_dst[335:320] := Saturate_Int32_To_UnsignedInt16 (b[287:256])
tmp_dst[351:336] := Saturate_Int32_To_UnsignedInt16 (b[319:288])
tmp_dst[367:352] := Saturate_Int32_To_UnsignedInt16 (b[351:320])
tmp_dst[383:368] := Saturate_Int32_To_UnsignedInt16 (b[383:352])
tmp_dst[399:384] := Saturate_Int32_To_UnsignedInt16 (a[415:384])
tmp_dst[415:400] := Saturate_Int32_To_UnsignedInt16 (a[447:416])
tmp_dst[431:416] := Saturate_Int32_To_UnsignedInt16 (a[479:448])
tmp_dst[447:432] := Saturate_Int32_To_UnsignedInt16 (a[511:480])
tmp_dst[463:448] := Saturate_Int32_To_UnsignedInt16 (b[415:384])
tmp_dst[479:464] := Saturate_Int32_To_UnsignedInt16 (b[447:416])
tmp_dst[495:480] := Saturate_Int32_To_UnsignedInt16 (b[479:448])
tmp_dst[511:496] := Saturate_Int32_To_UnsignedInt16 (b[511:480])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3863</id>
<name>_MM512_MASKZ_PERMUTE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3872</id>
<name>_MM512_MASKZ_PERMUTE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3893</id>
<name>_MM512_MASKZ_PERMUTEVAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3902</id>
<name>_MM512_MASKZ_PERMUTEVAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3910</id>
<name>_MM512_MASKZ_PERMUTEX_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a within 256-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3916</id>
<name>_MM512_MASKZ_PERMUTEX_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,CONST_INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 256-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3928</id>
<name>_MM512_MASKZ_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2W, VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		off := 16*idx[i+4:i]
		dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3940</id>
<name>_MM512_MASKZ_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2D, VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3952</id>
<name>_MM512_MASKZ_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2Q, VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3964</id>
<name>_MM512_MASKZ_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2B, VPERMT2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		off := 8*idx[i+5:i]
		dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3976</id>
<name>_MM512_MASKZ_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512I idx,__M512D b</sign>
<instr>VPERMI2PD, VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3988</id>
<name>_MM512_MASKZ_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512I idx,__M512 b</sign>
<instr>VPERMI2PS, VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3997</id>
<name>_MM512_MASKZ_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I idx,__M512I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	id := idx[i+4:i]*16
	IF k[j]
		dst[i+15:i] := a[id+15:id]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4003</id>
<name>_MM512_MASKZ_PERMUTEXVAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I idx,__M512I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4009</id>
<name>_MM512_MASKZ_PERMUTEXVAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I idx,__M512I a</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	id := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4018</id>
<name>_MM512_MASKZ_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I idx,__M512I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	id := idx[i+5:i]*8
	IF k[j]
		dst[i+7:i] := a[id+7:id]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4024</id>
<name>_MM512_MASKZ_PERMUTEXVAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512I idx,__M512D a</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	id := idx[i+2:i]*64
	IF k[j]
		dst[i+63:i] := a[id+63:id]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4030</id>
<name>_MM512_MASKZ_PERMUTEXVAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512I idx,__M512 a</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	IF k[j]
		dst[i+31:i] := a[id+31:id]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4120</id>
<name>_MM512_MASKZ_RANGE_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4129</id>
<name>_MM512_MASKZ_RANGE_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4132</id>
<name>_MM512_MASKZ_RANGE_ROUND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT imm8,INT rounding</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4135</id>
<name>_MM512_MASKZ_RANGE_ROUND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT imm8,INT rounding</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4157</id>
<name>_MM512_MASKZ_RCP14_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4166</id>
<name>_MM512_MASKZ_RCP14_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4177</id>
<name>_MM512_MASKZ_RCP28_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VRCP28PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
	ELSE
		dst[i+63:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4180</id>
<name>_MM512_MASKZ_RCP28_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VRCP28PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
	ELSE
		dst[i+31:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4183</id>
<name>_MM512_MASKZ_RCP28_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VRCP28PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
	ELSE
		dst[i+63:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4186</id>
<name>_MM512_MASKZ_RCP28_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VRCP28PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
	ELSE
		dst[i+31:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4280</id>
<name>_MM512_MASKZ_REDUCE_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4289</id>
<name>_MM512_MASKZ_REDUCE_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4292</id>
<name>_MM512_MASKZ_REDUCE_ROUND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT imm8,INT rounding</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4295</id>
<name>_MM512_MASKZ_REDUCE_ROUND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT imm8,INT rounding</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4346</id>
<name>_MM512_MASKZ_ROL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4355</id>
<name>_MM512_MASKZ_ROL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4364</id>
<name>_MM512_MASKZ_ROLV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4373</id>
<name>_MM512_MASKZ_ROLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4382</id>
<name>_MM512_MASKZ_ROR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4391</id>
<name>_MM512_MASKZ_ROR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4400</id>
<name>_MM512_MASKZ_RORV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4409</id>
<name>_MM512_MASKZ_RORV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4434</id>
<name>_MM512_MASKZ_ROUNDSCALE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4443</id>
<name>_MM512_MASKZ_ROUNDSCALE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4446</id>
<name>_MM512_MASKZ_ROUNDSCALE_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT imm8,INT rounding</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4449</id>
<name>_MM512_MASKZ_ROUNDSCALE_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT imm8,INT rounding</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4471</id>
<name>_MM512_MASKZ_RSQRT14_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4478</id>
<name>_MM512_MASKZ_RSQRT14_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4489</id>
<name>_MM512_MASKZ_RSQRT28_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VRSQRT28PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
	ELSE
		dst[i+63:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4492</id>
<name>_MM512_MASKZ_RSQRT28_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VRSQRT28PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
	ELSE
		dst[i+31:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4495</id>
<name>_MM512_MASKZ_RSQRT28_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VRSQRT28PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	IF k[j] THEN
		dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
	ELSE
		dst[i+63:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4498</id>
<name>_MM512_MASKZ_RSQRT28_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VRSQRT28PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	IF k[j] THEN
		dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
	ELSE
		dst[i+31:i] := 0;
	FI
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4531</id>
<name>_MM512_MASKZ_SCALEF_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4540</id>
<name>_MM512_MASKZ_SCALEF_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4543</id>
<name>_MM512_MASKZ_SCALEF_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4546</id>
<name>_MM512_MASKZ_SCALEF_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4598</id>
<name>_MM512_MASKZ_SET1_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,SHORT a</sign>
<instr>VPBROADCASTW</instr>
<desc>Broadcast the low packed 16-bit integer from a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[15:0]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4607</id>
<name>_MM512_MASKZ_SET1_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[31:0]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4615</id>
<name>_MM512_MASKZ_SET1_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[63:0]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4626</id>
<name>_MM512_MASKZ_SET1_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,CHAR a</sign>
<instr>VPBROADCASTB</instr>
<desc>Broadcast 8-bit integer a to all elements of dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[7:0]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4695</id>
<name>_MM512_MASKZ_SHUFFLE_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4704</id>
<name>_MM512_MASKZ_SHUFFLE_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		IF b[i+7] == 1
			dst[i+7:i] := 0
		ELSE
			index[3:0] := b[i+3:i]
			dst[i+7:i] := a[index*8+7:index*8]
		FI
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4710</id>
<name>_MM512_MASKZ_SHUFFLE_F32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VSHUFF32X4</instr>
<desc>Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4716</id>
<name>_MM512_MASKZ_SHUFFLE_F64X2</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VSHUFF64X2</instr>
<desc>Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4722</id>
<name>_MM512_MASKZ_SHUFFLE_I32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VSHUFI32X4</instr>
<desc>Shuffle 128-bits (composed of 4 32-bit integers) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4728</id>
<name>_MM512_MASKZ_SHUFFLE_I64X2</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VSHUFI64X2</instr>
<desc>Shuffle 128-bits (composed of 2 64-bit integers) selected by imm8 from a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4737</id>
<name>_MM512_MASKZ_SHUFFLE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4748</id>
<name>_MM512_MASKZ_SHUFFLE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4757</id>
<name>_MM512_MASKZ_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the high 64 bits of 128-bit lanes of dst, with the low 64 bits of 128-bit lanes being copied from from a to dst, using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[207:192]
tmp_dst[319:256] := a[319:256]
tmp_dst[335:320] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[335:320]
tmp_dst[351:336] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[335:320]
tmp_dst[367:352] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[335:320]
tmp_dst[383:368] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[335:320]
tmp_dst[447:384] := a[447:384]
tmp_dst[463:448] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[463:448]
tmp_dst[479:464] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[463:448]
tmp_dst[495:480] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[463:448]
tmp_dst[511:496] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[463:448]

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4766</id>
<name>_MM512_MASKZ_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the low 64 bits of 128-bit lanes of dst, with the high 64 bits of 128-bit lanes being copied from from a to dst, using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>tmp_dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]
tmp_dst[271:256] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[271:256]
tmp_dst[287:272] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[271:256]
tmp_dst[303:288] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[271:256]
tmp_dst[319:304] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[271:256]
tmp_dst[383:320] := a[383:320]
tmp_dst[399:384] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[399:384]
tmp_dst[415:400] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[399:384]
tmp_dst[431:416] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[399:384]
tmp_dst[447:432] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[399:384]
tmp_dst[511:448] := a[511:448]

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4816</id>
<name>_MM512_MASKZ_SLL_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4825</id>
<name>_MM512_MASKZ_SLL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4834</id>
<name>_MM512_MASKZ_SLL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4846</id>
<name>_MM512_MASKZ_SLLI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4855</id>
<name>_MM512_MASKZ_SLLI_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4864</id>
<name>_MM512_MASKZ_SLLI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4878</id>
<name>_MM512_MASKZ_SLLV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4887</id>
<name>_MM512_MASKZ_SLLV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4896</id>
<name>_MM512_MASKZ_SLLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4907</id>
<name>_MM512_MASKZ_SQRT_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4916</id>
<name>_MM512_MASKZ_SQRT_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4919</id>
<name>_MM512_MASKZ_SQRT_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,INT rounding</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SQRT(a[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4922</id>
<name>_MM512_MASKZ_SQRT_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,INT rounding</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SQRT(a[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4943</id>
<name>_MM512_MASKZ_SRA_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4952</id>
<name>_MM512_MASKZ_SRA_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4961</id>
<name>_MM512_MASKZ_SRA_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4972</id>
<name>_MM512_MASKZ_SRAI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := SignBit
		ELSE
			dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4981</id>
<name>_MM512_MASKZ_SRAI_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := SignBit
		ELSE
			dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4990</id>
<name>_MM512_MASKZ_SRAI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := SignBit
		ELSE
			dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5001</id>
<name>_MM512_MASKZ_SRAV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5010</id>
<name>_MM512_MASKZ_SRAV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5019</id>
<name>_MM512_MASKZ_SRAV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5028</id>
<name>_MM512_MASKZ_SRL_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF count[63:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5037</id>
<name>_MM512_MASKZ_SRL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF count[63:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5046</id>
<name>_MM512_MASKZ_SRL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF count[63:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5058</id>
<name>_MM512_MASKZ_SRLI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		IF imm8[7:0] &amp;gt; 15
			dst[i+15:i] := 0
		ELSE
			dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5067</id>
<name>_MM512_MASKZ_SRLI_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		IF imm8[7:0] &amp;gt; 31
			dst[i+31:i] := 0
		ELSE
			dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5076</id>
<name>_MM512_MASKZ_SRLI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		IF imm8[7:0] &amp;gt; 63
			dst[i+63:i] := 0
		ELSE
			dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
		FI
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5090</id>
<name>_MM512_MASKZ_SRLV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5099</id>
<name>_MM512_MASKZ_SRLV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5108</id>
<name>_MM512_MASKZ_SRLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5213</id>
<name>_MM512_MASKZ_SUB_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := a[i+15:i] - b[i+15:i]
	ELSE
		dst[i+15:i] := src[i+15:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5222</id>
<name>_MM512_MASKZ_SUB_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5231</id>
<name>_MM512_MASKZ_SUB_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5240</id>
<name>_MM512_MASKZ_SUB_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := a[i+7:i] - b[i+7:i]
	ELSE
		dst[i+7:i] := src[i+7:i]
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5249</id>
<name>_MM512_MASKZ_SUB_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5261</id>
<name>_MM512_MASKZ_SUB_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5264</id>
<name>_MM512_MASKZ_SUB_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b,INT rounding</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] - b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5267</id>
<name>_MM512_MASKZ_SUB_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b,INT rounding</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] - b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5303</id>
<name>_MM512_MASKZ_SUBS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5312</id>
<name>_MM512_MASKZ_SUBS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5321</id>
<name>_MM512_MASKZ_SUBS_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5330</id>
<name>_MM512_MASKZ_SUBS_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
	ELSE
		dst[i+7:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5395</id>
<name>_MM512_MASKZ_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b,__M512I c,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using zeromask k at 32-bit granularity (32-bit elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		FOR h := 0 to 31
			index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5404</id>
<name>_MM512_MASKZ_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b,__M512I c,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst using zeromask k at 64-bit granularity (64-bit elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		FOR h := 0 to 63
			index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
			dst[i+h] := imm8[index[2:0]]
		ENDFOR
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5525</id>
<name>_MM512_MASKZ_UNPACKHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5534</id>
<name>_MM512_MASKZ_UNPACKHI_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5543</id>
<name>_MM512_MASKZ_UNPACKHI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5552</id>
<name>_MM512_MASKZ_UNPACKHI_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5561</id>
<name>_MM512_MASKZ_UNPACKHI_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5573</id>
<name>_MM512_MASKZ_UNPACKHI_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5582</id>
<name>_MM512_MASKZ_UNPACKLO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])

FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := tmp_dst[i+15:i]
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5591</id>
<name>_MM512_MASKZ_UNPACKLO_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5600</id>
<name>_MM512_MASKZ_UNPACKLO_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5609</id>
<name>_MM512_MASKZ_UNPACKLO_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k,__M512I a,__M512I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])

FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := tmp_dst[i+7:i]
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5618</id>
<name>_MM512_MASKZ_UNPACKLO_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])

FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := tmp_dst[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5630</id>
<name>_MM512_MASKZ_UNPACKLO_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])

FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := tmp_dst[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5647</id>
<name>_MM512_MASKZ_XOR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k,__M512I a,__M512I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5654</id>
<name>_MM512_MASKZ_XOR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k,__M512I a,__M512I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5663</id>
<name>_MM512_MASKZ_XOR_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__MMASK8 k,__M512D a,__M512D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5672</id>
<name>_MM512_MASKZ_XOR_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__MMASK16 k,__M512 a,__M512 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3284</id>
<name>_MM512_MAX_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed maximum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3293</id>
<name>_MM512_MAX_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF a[i+31:i] &amp;gt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3302</id>
<name>_MM512_MAX_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF a[i+63:i] &amp;gt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3311</id>
<name>_MM512_MAX_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed maximum values in dst. </desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3320</id>
<name>_MM512_MAX_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed maximum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF a[i+15:i] &amp;gt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3329</id>
<name>_MM512_MAX_EPU32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF a[i+31:i] &amp;gt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3338</id>
<name>_MM512_MAX_EPU64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed maximum values in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF a[i+63:i] &amp;gt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3347</id>
<name>_MM512_MAX_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMAXUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed maximum values in dst.
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF a[i+7:i] &amp;gt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3356</id>
<name>_MM512_MAX_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3366</id>
<name>_MM512_MAX_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3370</id>
<name>_MM512_MAX_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT sae</sign>
<instr>VMAXPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed maximum values in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3373</id>
<name>_MM512_MAX_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT sae</sign>
<instr>VMAXPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed maximum values in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3387</id>
<name>_MM512_MAXABS_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VGMAXABSPS</instr>
<desc>Determines the maximum of the absolute elements of each pair of corresponding elements of packed single-precision (32-bit) floating-point elements in a and b, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3398</id>
<name>_MM512_MIN_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINSW</instr>
<desc>Compare packed 16-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3407</id>
<name>_MM512_MIN_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINSD</instr>
<desc>Compare packed 32-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF a[i+31:i] &amp;lt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3416</id>
<name>_MM512_MIN_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINSQ</instr>
<desc>Compare packed 64-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF a[i+63:i] &amp;lt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3425</id>
<name>_MM512_MIN_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINSB</instr>
<desc>Compare packed 8-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3434</id>
<name>_MM512_MIN_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINUW</instr>
<desc>Compare packed unsigned 16-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF a[i+15:i] &amp;lt; b[i+15:i]
		dst[i+15:i] := a[i+15:i]
	ELSE
		dst[i+15:i] := b[i+15:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3443</id>
<name>_MM512_MIN_EPU32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINUD</instr>
<desc>Compare packed unsigned 32-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF a[i+31:i] &amp;lt; b[i+31:i]
		dst[i+31:i] := a[i+31:i]
	ELSE
		dst[i+31:i] := b[i+31:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3452</id>
<name>_MM512_MIN_EPU64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINUQ</instr>
<desc>Compare packed unsigned 64-bit integers in a and b, and store packed minimum values in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF a[i+63:i] &amp;lt; b[i+63:i]
		dst[i+63:i] := a[i+63:i]
	ELSE
		dst[i+63:i] := b[i+63:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3461</id>
<name>_MM512_MIN_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMINUB</instr>
<desc>Compare packed unsigned 8-bit integers in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF a[i+7:i] &amp;lt; b[i+7:i]
		dst[i+7:i] := a[i+7:i]
	ELSE
		dst[i+7:i] := b[i+7:i]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3470</id>
<name>_MM512_MIN_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3480</id>
<name>_MM512_MIN_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3484</id>
<name>_MM512_MIN_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT sae</sign>
<instr>VMINPD</instr>
<desc>Compare packed double-precision (64-bit) floating-point elements in a and b, and store packed minimum values in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3487</id>
<name>_MM512_MIN_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT sae</sign>
<instr>VMINPS</instr>
<desc>Compare packed single-precision (32-bit) floating-point elements in a and b, and store packed minimum values in dst. 
	Pass __MM_FROUND_NO_EXC to sae to suppress all exceptions.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3553</id>
<name>_MM512_MOVEDUP_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VMOVDDUP</instr>
<desc>Duplicate even-indexed double-precision (64-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
tmp[319:256] := a[319:256] 
tmp[383:320] := a[319:256] 
tmp[447:384] := a[447:384]
tmp[511:448] := a[447:384]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3562</id>
<name>_MM512_MOVEHDUP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VMOVSHDUP</instr>
<desc>Duplicate odd-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[31:0] := a[63:32] 
dst[63:32] := a[63:32] 
dst[95:64] := a[127:96] 
dst[127:96] := a[127:96]
dst[159:128] := a[191:160] 
dst[191:160] := a[191:160] 
dst[223:192] := a[255:224] 
dst[255:224] := a[255:224]
dst[287:256] := a[319:288] 
dst[319:288] := a[319:288] 
dst[351:320] := a[383:352] 
dst[383:352] := a[383:352] 
dst[415:384] := a[447:416] 
dst[447:416] := a[447:416] 
dst[479:448] := a[511:480]
dst[511:480] := a[511:480]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3572</id>
<name>_MM512_MOVELDUP_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VMOVSLDUP</instr>
<desc>Duplicate even-indexed single-precision (32-bit) floating-point elements from a, and store the results in dst.
	</desc>
<oper>dst[31:0] := a[31:0] 
dst[63:32] := a[31:0] 
dst[95:64] := a[95:64] 
dst[127:96] := a[95:64]
dst[159:128] := a[159:128] 
dst[191:160] := a[159:128] 
dst[223:192] := a[223:192] 
dst[255:224] := a[223:192]
dst[287:256] := a[287:256] 
dst[319:288] := a[287:256] 
dst[351:320] := a[351:320] 
dst[383:352] := a[351:320] 
dst[415:384] := a[415:384] 
dst[447:416] := a[415:384] 
dst[479:448] := a[479:448]
dst[511:480] := a[479:448]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3583</id>
<name>_MM512_MOVEPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a</sign>
<instr>VPMOVW2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 16-bit integer in a.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF a[i+15]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>3586</id>
<name>_MM512_MOVEPI32_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a</sign>
<instr>VPMOVD2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 32-bit integer in a.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF a[i+31]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>3589</id>
<name>_MM512_MOVEPI64_MASK</name>
<cpuid>AVX512DQ</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a</sign>
<instr>VPMOVQ2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 64-bit integer in a.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF a[i+63]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>3593</id>
<name>_MM512_MOVEPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a</sign>
<instr>VPMOVB2M</instr>
<desc>Set each bit of mask register k based on the most significant bit of the corresponding packed 8-bit integer in a.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF a[i+7]
		k[j] := 1
	ELSE
		k[j] := 0
	FI
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>3596</id>
<name>_MM512_MOVM_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK32 k</sign>
<instr>VPMOVM2W</instr>
<desc>Set each packed 16-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := 0xFFFF
	ELSE
		dst[i+15:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3599</id>
<name>_MM512_MOVM_EPI32</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK16 k</sign>
<instr>VPMOVM2D</instr>
<desc>Set each packed 32-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF k[j]
		dst[i+31:i] := 0xFFFFFFFF
	ELSE
		dst[i+31:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3602</id>
<name>_MM512_MOVM_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__MMASK8 k</sign>
<instr>VPMOVM2Q</instr>
<desc>Set each packed 64-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF k[j]
		dst[i+63:i] := 0xFFFFFFFFffffffff
	ELSE
		dst[i+63:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3605</id>
<name>_MM512_MOVM_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__MMASK64 k</sign>
<instr>VPMOVM2B</instr>
<desc>Set each packed 8-bit integer in dst to all ones or all zeros based on the value of the corresponding bit in k.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF k[j]
		dst[i+7:i] := 0xFF
	ELSE
		dst[i+7:i] := 0
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3617</id>
<name>_MM512_MUL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULDQ</instr>
<desc>Multiply the low 32-bit integers from each packed 64-bit element in a and b, and store the signed 64-bit results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3626</id>
<name>_MM512_MUL_EPU32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULUDQ</instr>
<desc>Multiply the low unsigned 32-bit integers from each packed 64-bit element in a and b, and store the unsigned 64-bit results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3635</id>
<name>_MM512_MUL_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] * b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3644</id>
<name>_MM512_MUL_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3647</id>
<name>_MM512_MUL_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT rounding</sign>
<instr>VMULPD</instr>
<desc>Multiply packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	 </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] * b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3650</id>
<name>_MM512_MUL_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT rounding</sign>
<instr>VMULPS</instr>
<desc>Multiply packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst. 
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	 </desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3672</id>
<name>_MM512_MULHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULHW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3674</id>
<name>_MM512_MULHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULHD</instr>
<desc>Performs element-by-element multiplication between packed 32-bit integer elements in a and b and stores the high 32 bits of each result into dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) &amp;gt;&amp;gt; 32
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3683</id>
<name>_MM512_MULHI_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULHUW</instr>
<desc>Multiply the packed unsigned 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3685</id>
<name>_MM512_MULHI_EPU32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULHUD</instr>
<desc>Performs element-by-element multiplication between packed unsigned 32-bit integer elements in a and b and stores the high 32 bits of each result into dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i] * b[i+31:i]) &amp;gt;&amp;gt; 32
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3696</id>
<name>_MM512_MULHRS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULHRSW</instr>
<desc>Multiply packed 16-bit integers in a and b, producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	tmp[31:0] := ((a[i+15:i] * b[i+15:i]) &amp;gt;&amp;gt; 14) + 1
	dst[i+15:i] := tmp[16:1]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3706</id>
<name>_MM512_MULLO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULLW</instr>
<desc>Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	tmp[31:0] := a[i+15:i] * b[i+15:i]
	dst[i+15:i] := tmp[15:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3715</id>
<name>_MM512_MULLO_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULLD</instr>
<desc>Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	tmp[63:0] := a[i+31:i] * b[i+31:i]
	dst[i+31:i] := tmp[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3724</id>
<name>_MM512_MULLO_EPI64</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULLQ</instr>
<desc>Multiply the packed 64-bit integers in a and b, producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	tmp[127:0] := a[i+63:i] * b[i+63:i]
	dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3727</id>
<name>_MM512_MULLOX_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Multiplies elements in packed 64-bit integer vectors a and b together, storing the lower 64 bits of the result in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] * b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3736</id>
<name>_MM512_MULTISHIFT_EPI64_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPMULTISHIFTQB</instr>
<desc>For each 64-bit element in b, select 8 unaligned bytes using a byte-granular shift control within the corresponding 64-bit element of a, and store the 8 assembled bytes to the corresponding 64-bit element of dst.</desc>
<oper>FOR i := 0 to 7
	q := i * 64
	FOR j := 0 to 7
		tmp8 := 0
		ctrl := a[q+j*8+7:q+j*8] &amp; 63
		FOR l := 0 to 7
			tmp8[l] := b[q+((ctrl+l) &amp; 63)]
		ENDFOR
		dst[q+j*8+7:q+j*8] := tmp8[7:0]
	ENDFOR
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3739</id>
<name>_MM512_NEARBYINT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Rounds each packed double-precision (64-bit) floating-point element in a to the nearest integer value and stores the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := NearbyInt(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3741</id>
<name>_MM512_NEARBYINT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Rounds each packed single-precision (32-bit) floating-point element in a to the nearest integer value and stores the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := NearbyInt(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3748</id>
<name>_MM512_OR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] OR b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3755</id>
<name>_MM512_OR_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPORQ</instr>
<desc>Compute the bitwise OR of packed 64-bit integers in a and b, and store the resut in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] OR b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3764</id>
<name>_MM512_OR_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VORPD</instr>
<desc>Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3773</id>
<name>_MM512_OR_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VORPS</instr>
<desc>Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3776</id>
<name>_MM512_OR_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPORD</instr>
<desc>Compute the bitwise OR of 512 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[511:0] := (a[511:0] OR b[511:0])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3786</id>
<name>_MM512_PACKS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPACKSSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using signed saturation, and store the results in dst.
	</desc>
<oper>dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
dst[263:256] := Saturate_Int16_To_Int8 (a[271:256])
dst[271:264] := Saturate_Int16_To_Int8 (a[287:272])
dst[279:272] := Saturate_Int16_To_Int8 (a[303:288])
dst[287:280] := Saturate_Int16_To_Int8 (a[319:304])
dst[295:288] := Saturate_Int16_To_Int8 (a[335:320])
dst[303:296] := Saturate_Int16_To_Int8 (a[351:336])
dst[311:304] := Saturate_Int16_To_Int8 (a[367:352])
dst[319:312] := Saturate_Int16_To_Int8 (a[383:368])
dst[327:320] := Saturate_Int16_To_Int8 (b[271:256])
dst[335:328] := Saturate_Int16_To_Int8 (b[287:272])
dst[343:336] := Saturate_Int16_To_Int8 (b[303:288])
dst[351:344] := Saturate_Int16_To_Int8 (b[319:304])
dst[359:352] := Saturate_Int16_To_Int8 (b[335:320])
dst[367:360] := Saturate_Int16_To_Int8 (b[351:336])
dst[375:368] := Saturate_Int16_To_Int8 (b[367:352])
dst[383:376] := Saturate_Int16_To_Int8 (b[383:368])
dst[391:384] := Saturate_Int16_To_Int8 (a[399:384])
dst[399:392] := Saturate_Int16_To_Int8 (a[415:400])
dst[407:400] := Saturate_Int16_To_Int8 (a[431:416])
dst[415:408] := Saturate_Int16_To_Int8 (a[447:432])
dst[423:416] := Saturate_Int16_To_Int8 (a[463:448])
dst[431:424] := Saturate_Int16_To_Int8 (a[479:464])
dst[439:432] := Saturate_Int16_To_Int8 (a[495:480])
dst[447:440] := Saturate_Int16_To_Int8 (a[511:496])
dst[455:448] := Saturate_Int16_To_Int8 (b[399:384])
dst[463:456] := Saturate_Int16_To_Int8 (b[415:400])
dst[471:464] := Saturate_Int16_To_Int8 (b[431:416])
dst[479:472] := Saturate_Int16_To_Int8 (b[447:432])
dst[487:480] := Saturate_Int16_To_Int8 (b[463:448])
dst[495:488] := Saturate_Int16_To_Int8 (b[479:464])
dst[503:496] := Saturate_Int16_To_Int8 (b[495:480])
dst[511:504] := Saturate_Int16_To_Int8 (b[511:496])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3795</id>
<name>_MM512_PACKS_EPI32</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPACKSSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using signed saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
dst[271:256] := Saturate_Int32_To_Int16 (a[287:256])
dst[287:272] := Saturate_Int32_To_Int16 (a[319:288])
dst[303:288] := Saturate_Int32_To_Int16 (a[351:320])
dst[319:304] := Saturate_Int32_To_Int16 (a[383:352])
dst[335:320] := Saturate_Int32_To_Int16 (b[287:256])
dst[351:336] := Saturate_Int32_To_Int16 (b[319:288])
dst[367:352] := Saturate_Int32_To_Int16 (b[351:320])
dst[383:368] := Saturate_Int32_To_Int16 (b[383:352])
dst[399:384] := Saturate_Int32_To_Int16 (a[415:384])
dst[415:400] := Saturate_Int32_To_Int16 (a[447:416])
dst[431:416] := Saturate_Int32_To_Int16 (a[479:448])
dst[447:432] := Saturate_Int32_To_Int16 (a[511:480])
dst[463:448] := Saturate_Int32_To_Int16 (b[415:384])
dst[479:464] := Saturate_Int32_To_Int16 (b[447:416])
dst[495:480] := Saturate_Int32_To_Int16 (b[479:448])
dst[511:496] := Saturate_Int32_To_Int16 (b[511:480])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3802</id>
<name>_MM512_PACKSTOREHI_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1</sign>
<instr>VPACKSTOREHD</instr>
<desc>Stores packed 32-bit integer elements of v1 into a doubleword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elements of the stream that map at or after the first 64-byte-aligned address following (m5-64)).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*4) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*32
		MEM[addr + storeOffset*4] := v1[i+31:i]
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3804</id>
<name>_MM512_PACKSTOREHI_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1</sign>
<instr>VPACKSTOREHQ</instr>
<desc>Stores packed 64-bit integer elements of v1 into a quadword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*8) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*64
		MEM[addr + storeOffset*8] := v1[i+63:i]
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3806</id>
<name>_MM512_PACKSTOREHI_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v1</sign>
<instr>VPACKSTOREHPD</instr>
<desc>Stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 7
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*8) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*64
		MEM[addr + storeOffset*4] := v1[i+63:i]
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3808</id>
<name>_MM512_PACKSTOREHI_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v1</sign>
<instr>VPACKSTOREHPS</instr>
<desc>Stores packed single-precision (32-bit) floating-point elements of v1 into a doubleword stream at a logically mapped starting address (mt-64), storing the high-64-byte elements of that stream (those elemetns of the stream that map at or after the first 64-byte-aligned address following (m5-64)).</desc>
<oper>storeOffset := 0
foundNext64BytesBoundary := false
addr = mt-64
FOR j := 0 to 15
	IF foundNext64BytesBoundary == false
		IF ((addr + (storeOffset + 1)*4) % 64) == 0
			foundNext64BytesBoundary = true
		FI
	ELSE
		i := j*32
		MEM[addr + storeOffset*4] := v1[i+31:i]
	FI
	storeOffset := storeOffset + 1
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3810</id>
<name>_MM512_PACKSTORELO_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1</sign>
<instr>VPACKSTORELD</instr>
<desc>Stores packed 32-bit integer elements of v1 into a doubleword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 15
	i := j*32
	MEM[addr + storeOffset*4] := v1[i+31:i]
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset*4) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3812</id>
<name>_MM512_PACKSTORELO_EPI64</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512I v1</sign>
<instr>VPACKSTORELQ</instr>
<desc>Stores packed 64-bit integer elements of v1 into a quadword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 7
	i := j*64
	MEM[addr + storeOffset*8] := v1[i+63:i]
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset*8) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3814</id>
<name>_MM512_PACKSTORELO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v1</sign>
<instr>VPACKSTORELPD</instr>
<desc>Stores packed double-precision (64-bit) floating-point elements of v1 into a quadword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 7
	i := j*64
	MEM[addr + storeOffset*8] := v1[i+63:i]
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset*8) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3816</id>
<name>_MM512_PACKSTORELO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v1</sign>
<instr>VPACKSTORELPS</instr>
<desc>Stores packed single-precision (32-bit) floating-point elements of v1 into a doubleword stream at a logically mapped starting address mt, storing the low-64-byte elements of that stream (those elements of the stream that map before the first 64-byte-aligned address follwing mt).</desc>
<oper>storeOffset := 0
addr = mt
FOR j := 0 to 15
	i := j*32
	MEM[addr + storeOffset*4] := v1[i+31:i]
	storeOffset := storeOffset + 1
	IF ((addr + storeOffset*4) % 64) == 0
		BREAK
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3825</id>
<name>_MM512_PACKUS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPACKUSWB</instr>
<desc>Convert packed 16-bit integers from a and b to packed 8-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
dst[263:256] := Saturate_Int16_To_UnsignedInt8 (a[271:256])
dst[271:264] := Saturate_Int16_To_UnsignedInt8 (a[287:272])
dst[279:272] := Saturate_Int16_To_UnsignedInt8 (a[303:288])
dst[287:280] := Saturate_Int16_To_UnsignedInt8 (a[319:304])
dst[295:288] := Saturate_Int16_To_UnsignedInt8 (a[335:320])
dst[303:296] := Saturate_Int16_To_UnsignedInt8 (a[351:336])
dst[311:304] := Saturate_Int16_To_UnsignedInt8 (a[367:352])
dst[319:312] := Saturate_Int16_To_UnsignedInt8 (a[383:368])
dst[327:320] := Saturate_Int16_To_UnsignedInt8 (b[271:256])
dst[335:328] := Saturate_Int16_To_UnsignedInt8 (b[287:272])
dst[343:336] := Saturate_Int16_To_UnsignedInt8 (b[303:288])
dst[351:344] := Saturate_Int16_To_UnsignedInt8 (b[319:304])
dst[359:352] := Saturate_Int16_To_UnsignedInt8 (b[335:320])
dst[367:360] := Saturate_Int16_To_UnsignedInt8 (b[351:336])
dst[375:368] := Saturate_Int16_To_UnsignedInt8 (b[367:352])
dst[383:376] := Saturate_Int16_To_UnsignedInt8 (b[383:368])
dst[391:384] := Saturate_Int16_To_UnsignedInt8 (a[399:384])
dst[399:392] := Saturate_Int16_To_UnsignedInt8 (a[415:400])
dst[407:400] := Saturate_Int16_To_UnsignedInt8 (a[431:416])
dst[415:408] := Saturate_Int16_To_UnsignedInt8 (a[447:432])
dst[423:416] := Saturate_Int16_To_UnsignedInt8 (a[463:448])
dst[431:424] := Saturate_Int16_To_UnsignedInt8 (a[479:464])
dst[439:432] := Saturate_Int16_To_UnsignedInt8 (a[495:480])
dst[447:440] := Saturate_Int16_To_UnsignedInt8 (a[511:496])
dst[455:448] := Saturate_Int16_To_UnsignedInt8 (b[399:384])
dst[463:456] := Saturate_Int16_To_UnsignedInt8 (b[415:400])
dst[471:464] := Saturate_Int16_To_UnsignedInt8 (b[431:416])
dst[479:472] := Saturate_Int16_To_UnsignedInt8 (b[447:432])
dst[487:480] := Saturate_Int16_To_UnsignedInt8 (b[463:448])
dst[495:488] := Saturate_Int16_To_UnsignedInt8 (b[479:464])
dst[503:496] := Saturate_Int16_To_UnsignedInt8 (b[495:480])
dst[511:504] := Saturate_Int16_To_UnsignedInt8 (b[511:496])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3834</id>
<name>_MM512_PACKUS_EPI32</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPACKUSDW</instr>
<desc>Convert packed 32-bit integers from a and b to packed 16-bit integers using unsigned saturation, and store the results in dst.</desc>
<oper>dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
dst[271:256] := Saturate_Int32_To_UnsignedInt16 (a[287:256])
dst[287:272] := Saturate_Int32_To_UnsignedInt16 (a[319:288])
dst[303:288] := Saturate_Int32_To_UnsignedInt16 (a[351:320])
dst[319:304] := Saturate_Int32_To_UnsignedInt16 (a[383:352])
dst[335:320] := Saturate_Int32_To_UnsignedInt16 (b[287:256])
dst[351:336] := Saturate_Int32_To_UnsignedInt16 (b[319:288])
dst[367:352] := Saturate_Int32_To_UnsignedInt16 (b[351:320])
dst[383:368] := Saturate_Int32_To_UnsignedInt16 (b[383:352])
dst[399:384] := Saturate_Int32_To_UnsignedInt16 (a[415:384])
dst[415:400] := Saturate_Int32_To_UnsignedInt16 (a[447:416])
dst[431:416] := Saturate_Int32_To_UnsignedInt16 (a[479:448])
dst[447:432] := Saturate_Int32_To_UnsignedInt16 (a[511:480])
dst[463:448] := Saturate_Int32_To_UnsignedInt16 (b[415:384])
dst[479:464] := Saturate_Int32_To_UnsignedInt16 (b[447:416])
dst[495:480] := Saturate_Int32_To_UnsignedInt16 (b[479:448])
dst[511:496] := Saturate_Int32_To_UnsignedInt16 (b[511:480])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3864</id>
<name>_MM512_PERMUTE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,CONST_INT imm8</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>IF (imm8[0] == 0) dst[63:0] := a[63:0]
IF (imm8[0] == 1) dst[63:0] := a[127:64]
IF (imm8[1] == 0) dst[127:64] := a[63:0]
IF (imm8[1] == 1) dst[127:64] := a[127:64]
IF (imm8[2] == 0) dst[191:128] := a[191:128]
IF (imm8[2] == 1) dst[191:128] := a[255:192]
IF (imm8[3] == 0) dst[255:192] := a[191:128]
IF (imm8[3] == 1) dst[255:192] := a[255:192]
IF (imm8[4] == 0) dst[319:256] := a[319:256]
IF (imm8[4] == 1) dst[319:256] := a[383:320]
IF (imm8[5] == 0) dst[383:320] := a[319:256]
IF (imm8[5] == 1) dst[383:320] := a[383:320]
IF (imm8[6] == 0) dst[447:384] := a[447:384]
IF (imm8[6] == 1) dst[447:384] := a[511:448]
IF (imm8[7] == 0) dst[511:448] := a[447:384]
IF (imm8[7] == 1) dst[511:448] := a[511:448]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3873</id>
<name>_MM512_PERMUTE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,CONST_INT imm8</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(a[255:128], imm8[5:4])
dst[255:224] := SELECT4(a[255:128], imm8[7:6])
dst[287:256] := SELECT4(a[383:256], imm8[1:0])
dst[319:288] := SELECT4(a[383:256], imm8[3:2])
dst[351:320] := SELECT4(a[383:256], imm8[5:4])
dst[383:352] := SELECT4(a[383:256], imm8[7:6])
dst[415:384] := SELECT4(a[511:384], imm8[1:0])
dst[447:416] := SELECT4(a[511:384], imm8[3:2])
dst[479:448] := SELECT4(a[511:384], imm8[5:4])
dst[511:480] := SELECT4(a[511:384], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3879</id>
<name>_MM512_PERMUTE4F128_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,_MM_PERM_ENUM imm8</sign>
<instr>VPERMF32X4</instr>
<desc>Permutes 128-bit blocks of the packed 32-bit integer vector a using constant imm8. The results are stored in dst.</desc>
<oper>SELECT4(src, control) {
	CASE control[1:0] OF
	0: tmp[127:0] := src[127:0]
	1: tmp[127:0] := src[255:128]
	2: tmp[127:0] := src[383:256]
	3: tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

FOR j := 0 to 3
	i := j*128
	n := j*2
	dst[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3881</id>
<name>_MM512_PERMUTE4F128_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,_MM_PERM_ENUM imm8</sign>
<instr>VPERMF32X4</instr>
<desc>Permutes 128-bit blocks of the packed single-precision (32-bit) floating-point elements in a using constant imm8. The results are stored in dst.</desc>
<oper>SELECT4(src, control) {
	CASE control[1:0] OF
	0: tmp[127:0] := src[127:0]
	1: tmp[127:0] := src[255:128]
	2: tmp[127:0] := src[383:256]
	3: tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

FOR j := 0 to 3
	i := j*128
	n := j*2
	dst[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3885</id>
<name>_MM512_PERMUTEVAR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I idx,__M512I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst. Note that this intrinsic shuffles across 128-bit lanes, unlike past intrinsics that use the permutevar name. This intrinsic is identical to _mm512_permutexvar_epi32, and it is recommended that you use that intrinsic name.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3894</id>
<name>_MM512_PERMUTEVAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512I b</sign>
<instr>VPERMILPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst.</desc>
<oper>IF (b[1] == 0) dst[63:0] := a[63:0]
IF (b[1] == 1) dst[63:0] := a[127:64]
IF (b[65] == 0) dst[127:64] := a[63:0]
IF (b[65] == 1) dst[127:64] := a[127:64]
IF (b[129] == 0) dst[191:128] := a[191:128]
IF (b[129] == 1) dst[191:128] := a[255:192]
IF (b[193] == 0) dst[255:192] := a[191:128]
IF (b[193] == 1) dst[255:192] := a[255:192]
IF (b[257] == 0) dst[319:256] := a[319:256]
IF (b[257] == 1) dst[319:256] := a[383:320]
IF (b[321] == 0) dst[383:320] := a[319:256]
IF (b[321] == 1) dst[383:320] := a[383:320]
IF (b[385] == 0) dst[447:384] := a[447:384]
IF (b[385] == 1) dst[447:384] := a[511:448]
IF (b[449] == 0) dst[511:448] := a[447:384]
IF (b[449] == 1) dst[511:448] := a[511:448]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3903</id>
<name>_MM512_PERMUTEVAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512I b</sign>
<instr>VPERMILPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], b[1:0])
dst[63:32] := SELECT4(a[127:0], b[33:32])
dst[95:64] := SELECT4(a[127:0], b[65:64])
dst[127:96] := SELECT4(a[127:0], b[97:96])
dst[159:128] := SELECT4(a[255:128], b[129:128])
dst[191:160] := SELECT4(a[255:128], b[161:160])
dst[223:192] := SELECT4(a[255:128], b[193:192])
dst[255:224] := SELECT4(a[255:128], b[225:224])
dst[287:256] := SELECT4(a[383:256], b[257:256])
dst[319:288] := SELECT4(a[383:256], b[289:288])
dst[351:320] := SELECT4(a[383:256], b[321:320])
dst[383:352] := SELECT4(a[383:256], b[353:352])
dst[415:384] := SELECT4(a[511:384], b[385:384])
dst[447:416] := SELECT4(a[511:384], b[417:416])
dst[479:448] := SELECT4(a[511:384], b[449:448])
dst[511:480] := SELECT4(a[511:384], b[481:480])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3911</id>
<name>_MM512_PERMUTEX_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,CONST_INT imm8</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a within 256-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[319:256] := SELECT4(a[511:256], imm8[1:0])
dst[383:320] := SELECT4(a[511:256], imm8[3:2])
dst[447:384] := SELECT4(a[511:256], imm8[5:4])
dst[511:448] := SELECT4(a[511:256], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3917</id>
<name>_MM512_PERMUTEX_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,CONST_INT imm8</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a within 256-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[63:0] := src[63:0]
	1:	tmp[63:0] := src[127:64]
	2:	tmp[63:0] := src[191:128]
	3:	tmp[63:0] := src[255:192]
	ESAC
	RETURN tmp[63:0]
}

dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[319:256] := SELECT4(a[511:256], imm8[1:0])
dst[383:320] := SELECT4(a[511:256], imm8[3:2])
dst[447:384] := SELECT4(a[511:256], imm8[5:4])
dst[511:448] := SELECT4(a[511:256], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3929</id>
<name>_MM512_PERMUTEX2VAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2W, VPERMT2W</instr>
<desc>Shuffle 16-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	off := 16*idx[i+4:i]
	dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3941</id>
<name>_MM512_PERMUTEX2VAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2D, VPERMT2D</instr>
<desc>Shuffle 32-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3953</id>
<name>_MM512_PERMUTEX2VAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2Q, VPERMT2Q</instr>
<desc>Shuffle 64-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3965</id>
<name>_MM512_PERMUTEX2VAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I idx,__M512I b</sign>
<instr>VPERMI2B</instr>
<desc>Shuffle 8-bit integers in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	off := 8*idx[i+5:i]
	dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3977</id>
<name>_MM512_PERMUTEX2VAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512I idx,__M512D b</sign>
<instr>VPERMI2PD, VPERMT2PD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	off := idx[i+2:i]*64
	dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3989</id>
<name>_MM512_PERMUTEX2VAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512I idx,__M512 b</sign>
<instr>VPERMI2PS, VPERMT2PS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a and b across lanes using the corresponding selector and index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	off := idx[i+3:i]*32
	dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3998</id>
<name>_MM512_PERMUTEXVAR_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I idx,__M512I a</sign>
<instr>VPERMW</instr>
<desc>Shuffle 16-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	id := idx[i+4:i]*16
	dst[i+15:i] := a[id+15:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4004</id>
<name>_MM512_PERMUTEXVAR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I idx,__M512I a</sign>
<instr>VPERMD</instr>
<desc>Shuffle 32-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4010</id>
<name>_MM512_PERMUTEXVAR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I idx,__M512I a</sign>
<instr>VPERMQ</instr>
<desc>Shuffle 64-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	id := idx[i+2:i]*64
	dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4019</id>
<name>_MM512_PERMUTEXVAR_EPI8</name>
<cpuid>AVX512VBMI</cpuid>
<ret>__m512i</ret>
<sign>__M512I idx,__M512I a</sign>
<instr>VPERMB</instr>
<desc>Shuffle 8-bit integers in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	id := idx[i+5:i]*8
	dst[i+7:i] := a[id+7:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4025</id>
<name>_MM512_PERMUTEXVAR_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512I idx,__M512D a</sign>
<instr>VPERMPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements in a across lanes using the corresponding index in idx, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	id := idx[i+2:i]*64
	dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4031</id>
<name>_MM512_PERMUTEXVAR_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512I idx,__M512 a</sign>
<instr>VPERMPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a across lanes using the corresponding index in idx.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	id := idx[i+3:i]*32
	dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4053</id>
<name>_MM512_POW_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed double-precision (64-bit) floating-point elements in a raised by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4057</id>
<name>_MM512_POW_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>NONE</instr>
<desc>Compute the exponential value of packed single-precision (32-bit) floating-point elements in a raised by packed elements in b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4062</id>
<name>_MM512_PREFETCH_I32EXTGATHER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>__M512I index,VOID_CONST_PTR mv,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VGATHERPF0DPS, VGATHERPF1DPS</instr>
<desc>Prefetches a set of 16 single-precision (32-bit) memory locations pointed by base address mv and 32-bit integer index vector index with scale scale to L1 or L2 level of cache depending on the value of hint. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.
The conv parameter specifies the granularity used by compilers to better encode the instruction. It should be the same as the conv parameter specified for the subsequent gather intrinsic.</desc>
<oper>FOR j := 0 to 15
	addr := MEM[mv + index[j] * scale]
	i := j*32
	CASE hint OF
	_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
	_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4064</id>
<name>_MM512_PREFETCH_I32EXTSCATTER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,_MM_UPCONV_PS_ENUM conv,INT scale,INT hint</sign>
<instr>VSCATTERPF0DPS, VSCATTERPF1DPS</instr>
<desc>Prefetches a set of 16 single-precision (32-bit) memory locations pointed by base address mv and 32-bit integer index vector index with scale scale to L1 or L2 level of cache depending on the value of hint, with a request for exclusive ownership. The hint parameter may be one of the following: _MM_HINT_T0 = 1 for prefetching to L1 cache, _MM_HINT_T1 = 2 for prefetching to L2 cache, _MM_HINT_T2 = 3 for prefetching to L2 cache non-temporal, _MM_HINT_NTA = 0 for prefetching to L1 cache non-temporal. The conv parameter specifies the granularity used by compilers to better encode the instruction. It should be the same as the conv parameter specified for the subsequent scatter intrinsic.</desc>
<oper>cachev := 0
FOR j := 0 to 15
	i := j*32
	addr := MEM[mv + index[j] * scale]
	CASE hint OF
	_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
	_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
	_MM_HINT_T2: PrefetchL2WithT1HintNonTemporal(addr[i+31:i])
	_MM_HINT_NTA: PrefetchL1WithT0HintNonTemporal(addr[i+31:i])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4066</id>
<name>_MM512_PREFETCH_I32GATHER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>__M256I vindex,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0DPD, VGATHERPF1DPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged in cache. scale should be 1, 2, 4 or 8. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j := 0 to 7
	i := j*32;
	Prefetch([base_addr + SignExtend(vindex[i*31:i]) * scale], hint, RFO=0);
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4068</id>
<name>_MM512_PREFETCH_I32GATHER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>__M512I index,VOID_CONST_PTR mv,INT scale,INT hint</sign>
<instr>VGATHERPF0DPS, VGATHERPF1DPS</instr>
<desc>Prefetches 16 single-precision (32-bit) floating-point elements in memory starting at location mv at packed 32-bit integer indices stored in index scaled by scale. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>cachev := 0
FOR j := 0 to 15
	i := j*32
	addr := MEM[mv + index[j] * scale]
	cachev[i+31:i] := addr[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4070</id>
<name>_MM512_PREFETCH_I32SCATTER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M256I vindex,INT scale,INT hint</sign>
<instr>VSCATTERPF0DPD, VSCATTERPF1DPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements with intent to write using 32-bit indices. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 64-bit elements are brought into cache from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 TO 7
	i := j*32;
	Prefetch(base_addr + SignExtend(vindex[i+31:i]) * scale], Level=hint, RFO=1);
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4072</id>
<name>_MM512_PREFETCH_I32SCATTER_PS</name>
<cpuid>AVX512PF, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mv,__M512I index,INT scale,INT hint</sign>
<instr>VSCATTERPF0DPS, VSCATTERPF1DPS</instr>
<desc>Prefetches 16 single-precision (32-bit) floating-point elements in memory starting at location mv at packed 32-bit integer indices stored in index scaled by scale. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	addr := MEM[mv + index[j] * scale]
	CASE hint OF
	_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
	_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
	_MM_HINT_T2: PrefetchL2WithT1HintNonTemporal(addr[i+31:i])
	_MM_HINT_NTA: PrefetchL1WithT0HintNonTemporal(addr[i+31:i])
	ESAC
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4074</id>
<name>_MM512_PREFETCH_I64GATHER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0QPD, VGATHERPF1QPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements from memory into cache level specified by hint using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	Prefetch([base_addr + SignExtend(vindex[i*63:i] * scale]), Level=hint, RFO=0);
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4076</id>
<name>_MM512_PREFETCH_I64GATHER_PS</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>__M512I vindex,VOID_CONST_PTR base_addr,INT scale,INT hint</sign>
<instr>VGATHERPF0QPS, VGATHERPF1QPS</instr>
<desc>Prefetch single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). Gathered elements are merged in cache. scale should be 1, 2, 4 or 8. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.</desc>
<oper>FOR j:= 0 to 7
	i := j*64;
	Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], hint, RFO=0);
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4078</id>
<name>_MM512_PREFETCH_I64SCATTER_PD</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,INT scale,INT hint</sign>
<instr>VSCATTERPF0QPD, VSCATTERPF1QPD</instr>
<desc>Prefetch double-precision (64-bit) floating-point elements with intent to write into memory using 64-bit indices. The hint parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 64-bit elements are brought into cache from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4080</id>
<name>_MM512_PREFETCH_I64SCATTER_PS</name>
<cpuid>AVX512PF</cpuid>
<ret>void</ret>
<sign>VOID_PTR base_addr,__M512I vindex,INT scale,INT hint</sign>
<instr>VSCATTERPF0QPS, VSCATTERPF1QPS</instr>
<desc>Prefetch single-precision (32-bit) floating-point elements with intent to write into memory using 64-bit indices. Elements are prefetched into cache level hint, where hint is 0 or 1. 32-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale). scale should be 1, 2, 4 or 8.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4121</id>
<name>_MM512_RANGE_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT imm8</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4130</id>
<name>_MM512_RANGE_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT imm8</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4133</id>
<name>_MM512_RANGE_ROUND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT imm8,INT rounding</sign>
<instr>VRANGEPD</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
	1: tmp[63:0] := (src1[63:0] &amp;lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
	2: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
	3: tmp[63:0] := (ABS(src1[63:0]) &amp;lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[63:0] := (src1[63] &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	1: dst[63:0] := tmp[63:0]
	2: dst[63:0] := (0 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	3: dst[63:0] := (1 &amp;lt;&amp;lt; 63) OR (tmp[62:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4136</id>
<name>_MM512_RANGE_ROUND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT imm8,INT rounding</sign>
<instr>VRANGEPS</instr>
<desc>Calculate the max, min, absolute max, or absolute min (depending on control in imm8) for packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
{
	CASE opCtl[1:0]
	0: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
	1: tmp[31:0] := (src1[31:0] &amp;lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
	2: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
	3: tmp[31:0] := (ABS(src1[31:0]) &amp;lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
	ESAC
	
	CASE signSelCtl[1:0]
	0: dst[31:0] := (src1[31] &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	1: dst[31:0] := tmp[63:0]
	2: dst[31:0] := (0 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	3: dst[31:0] := (1 &amp;lt;&amp;lt; 31) OR (tmp[30:0])
	ESAC
	
	RETURN dst
}

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4158</id>
<name>_MM512_RCP14_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VRCP14PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4167</id>
<name>_MM512_RCP14_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VRCP14PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4175</id>
<name>_MM512_RCP23_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VRCP23PS</instr>
<desc>Approximates the reciprocals of packed single-precision (32-bit) floating-point elements in a to 23 bits of precision, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4178</id>
<name>_MM512_RCP28_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VRCP28PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4181</id>
<name>_MM512_RCP28_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VRCP28PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4184</id>
<name>_MM512_RCP28_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VRCP28PD</instr>
<desc>Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4187</id>
<name>_MM512_RCP28_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VRCP28PS</instr>
<desc>Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4214</id>
<name>_MM512_RECIP_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Computes the reciprocal of packed double-precision (64-bit) floating-point elements in a, storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := (1 / a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4216</id>
<name>_MM512_RECIP_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Computes the reciprocal of packed single-precision (32-bit) floating-point elements in a, storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := (1 / a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4218</id>
<name>_MM512_REDUCE_ADD_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by addition. Returns the sum of all elements in a.</desc>
<oper>sum[31:0] := 0
FOR j := 0 to 15
	i := j*32
	sum[31:0] := sum[31:0] + a[i+31:i]
ENDFOR
RETURN sum[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4220</id>
<name>_MM512_REDUCE_ADD_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by addition. Returns the sum of all elements in a.</desc>
<oper>sum[63:0] := 0
FOR j := 0 to 7
	i := j*64
	sum[63:0] := sum[63:0] + a[i+63:i]
ENDFOR
RETURN sum[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4222</id>
<name>_MM512_REDUCE_ADD_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by addition. Returns the sum of all elements in a.</desc>
<oper>sum[63:0] := 0
FOR j := 0 to 7
	i := j*64
	sum[63:0] := sum[63:0] + a[i+63:i]
ENDFOR
RETURN sum[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4224</id>
<name>_MM512_REDUCE_ADD_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by addition. Returns the sum of all elements in a.</desc>
<oper>sum[31:0] := 0
FOR j := 0 to 15
	i := j*32
	sum[31:0] := sum[31:0] + a[i+31:i]
ENDFOR
RETURN sum[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4226</id>
<name>_MM512_REDUCE_AND_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by bitwise AND. Returns the bitwise AND of all elements in a.</desc>
<oper>reduced[31:0] := 0xFFFFFFFF
FOR j := 0 to 15
	i := j*32
	reduced[31:0] := reduced[31:0] AND a[i+31:i]
ENDFOR
RETURN reduced[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4228</id>
<name>_MM512_REDUCE_AND_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by bitwise AND. Returns the bitwise AND of all elements in a.</desc>
<oper>reduced[63:0] := 0xFFFFFFFFFFFFFFFF
FOR j := 0 to 7
	i := j*64
	reduced[63:0] := reduced[63:0] AND a[i+63:i]
ENDFOR
RETURN reduced[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4230</id>
<name>_MM512_REDUCE_GMAX_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Determines the maximum element of the packed double-precision (64-bit) floating-point elements stored in a and stores the result in dst.</desc>
<oper>max = a[63:0]
FOR j := 1 to 7
	i := j*64
	dst = FpMax(max, a[i+63:i])
ENDFOR
dst := max</oper>
</intrinsic>
<intrinsic>
<id>4232</id>
<name>_MM512_REDUCE_GMAX_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Determines the maximum element of the packed single-precision (32-bit) floating-point elements stored in a and stores the result in dst.</desc>
<oper>max = a[31:0]
FOR j := 1 to 15
	i := j*32
	dst = FpMax(max, a[i+31:i])
ENDFOR
dst := max</oper>
</intrinsic>
<intrinsic>
<id>4234</id>
<name>_MM512_REDUCE_GMIN_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Determines the minimum element of the packed double-precision (64-bit) floating-point elements stored in a and stores the result in dst.</desc>
<oper>min = a[63:0]
FOR j := 1 to 7
	i := j*64
	dst = FpMin(min, a[i+63:i])
ENDFOR
dst := min</oper>
</intrinsic>
<intrinsic>
<id>4236</id>
<name>_MM512_REDUCE_GMIN_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Determines the minimum element of the packed single-precision (32-bit) floating-point elements stored in a and stores the result in dst.</desc>
<oper>min = a[31:0]
FOR j := 1 to 15
	i := j*32
	dst = FpMin(min, a[i+31:i])
ENDFOR
dst := min</oper>
</intrinsic>
<intrinsic>
<id>4238</id>
<name>_MM512_REDUCE_MAX_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by maximum. Returns the maximum of all elements in a.</desc>
<oper>max[31:0] := MIN_INT
FOR j := 0 to 15
	i := j*32
	max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
ENDFOR
RETURN max[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4240</id>
<name>_MM512_REDUCE_MAX_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by maximum. Returns the maximum of all elements in a.</desc>
<oper>max[63:0] := MIN_INT
FOR j := 0 to 7
	i := j*64
	max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
ENDFOR
RETURN max[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4242</id>
<name>_MM512_REDUCE_MAX_EPU32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 32-bit integers in a by maximum. Returns the maximum of all elements in a.</desc>
<oper>max[31:0] := 0
FOR j := 0 to 15
	i := j*32
	max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
ENDFOR
RETURN max[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4244</id>
<name>_MM512_REDUCE_MAX_EPU64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned __int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 64-bit integers in a by maximum. Returns the maximum of all elements in a.</desc>
<oper>max[63:0] := 0
FOR j := 0 to 7
	i := j*64
	max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
ENDFOR
RETURN max[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4246</id>
<name>_MM512_REDUCE_MAX_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by maximum. Returns the maximum of all elements in a.</desc>
<oper>max[63:0] := MIN_DOUBLE
FOR j := 0 to 7
	i := j*64
	max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
ENDFOR
RETURN max[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4248</id>
<name>_MM512_REDUCE_MAX_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by maximum. Returns the maximum of all elements in a.</desc>
<oper>max[31:0] := MIN_FLOAT
FOR j := 0 to 15
	i := j*32
	max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
ENDFOR
RETURN max[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4250</id>
<name>_MM512_REDUCE_MIN_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by minimum. Returns the minimum of all elements in a.</desc>
<oper>min[31:0] := MAX_INT
FOR j := 0 to 15
	i := j*32
	min[31:0] := MINIMUM(min[31:0], a[i+31:i])
ENDFOR
RETURN min[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4252</id>
<name>_MM512_REDUCE_MIN_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by minimum. Returns the minimum of all elements in a.</desc>
<oper>min[63:0] := MAX_INT
FOR j := 0 to 7
	i := j*64
	min[63:0] := MINIMUM(min[63:0], a[i+63:i])
ENDFOR
RETURN min[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4254</id>
<name>_MM512_REDUCE_MIN_EPU32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 32-bit integers in a by minimum. Returns the minimum of all elements in a.</desc>
<oper>min[31:0] := MAX_UINT
FOR j := 0 to 15
	i := j*32
	min[31:0] := MINIMUM(min[31:0], a[i+31:i])
ENDFOR
RETURN min[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4256</id>
<name>_MM512_REDUCE_MIN_EPU64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>unsigned __int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed unsigned 64-bit integers in a by minimum. Returns the minimum of all elements in a.</desc>
<oper>min[63:0] := MAX_UINT
FOR j := 0 to 7
	i := j*64
	min[63:0] := MINIMUM(min[63:0], a[i+63:i])
ENDFOR
RETURN min[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4258</id>
<name>_MM512_REDUCE_MIN_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by minimum. Returns the minimum of all elements in a.</desc>
<oper>min[63:0] := MAX_DOUBLE
FOR j := 0 to 7
	i := j*64
	min[63:0] := MINIMUM(min[63:0], a[i+63:i])
ENDFOR
RETURN min[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4260</id>
<name>_MM512_REDUCE_MIN_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by minimum. Returns the minimum of all elements in a.</desc>
<oper>min[31:0] := MAX_INT
FOR j := 0 to 15
	i := j*32
	min[31:0] := MINIMUM(min[31:0], a[i+31:i])
ENDFOR
RETURN min[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4262</id>
<name>_MM512_REDUCE_MUL_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by multiplication. Returns the product of all elements in a.</desc>
<oper>prod[31:0] := 1
FOR j := 0 to 15
	i := j*32
	prod[31:0] := prod[31:0] * a[i+31:i]
ENDFOR
RETURN prod[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4264</id>
<name>_MM512_REDUCE_MUL_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by multiplication. Returns the product of all elements in a.</desc>
<oper>prod[63:0] := 1
FOR j := 0 to 7
	i := j*64
	prod[63:0] := prod[63:0] * a[i+63:i]
ENDFOR
RETURN prod[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4266</id>
<name>_MM512_REDUCE_MUL_PD</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>double</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Reduce the packed double-precision (64-bit) floating-point elements in a by multiplication. Returns the product of all elements in a.</desc>
<oper>prod[63:0] := 1
FOR j := 0 to 7
	i := j*64
	prod[63:0] := prod[63:0] * a[i+63:i]
ENDFOR
RETURN prod[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4268</id>
<name>_MM512_REDUCE_MUL_PS</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>float</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Reduce the packed single-precision (32-bit) floating-point elements in a by multiplication. Returns the product of all elements in a.</desc>
<oper>prod[31:0] := 1
FOR j := 0 to 15
	i := j*32
	prod[31:0] := prod[31:0] * a[i+31:i]
ENDFOR
RETURN prod[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4270</id>
<name>_MM512_REDUCE_OR_EPI32</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>int</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 32-bit integers in a by bitwise OR. Returns the bitwise OR of all elements in a.</desc>
<oper>reduced[31:0] := 0
FOR j := 0 to 15
	i := j*32
	reduced[31:0] := reduced[31:0] OR a[i+31:i]
ENDFOR
RETURN reduced[31:0]</oper>
</intrinsic>
<intrinsic>
<id>4272</id>
<name>_MM512_REDUCE_OR_EPI64</name>
<cpuid>AVX512F, KNCNI, SVML</cpuid>
<ret>__int64</ret>
<sign>__M512I a</sign>
<instr>NONE</instr>
<desc>Reduce the packed 64-bit integers in a by bitwise OR. Returns the bitwise OR of all elements in a.</desc>
<oper>reduced[63:0] := 0
FOR j := 0 to 7
	i := j*64
	reduced[63:0] := reduced[63:0] OR a[i+63:i]
ENDFOR
RETURN reduced[63:0]</oper>
</intrinsic>
<intrinsic>
<id>4281</id>
<name>_MM512_REDUCE_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT imm8</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst. </desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4290</id>
<name>_MM512_REDUCE_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT imm8</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst.</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4293</id>
<name>_MM512_REDUCE_ROUND_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT imm8,INT rounding</sign>
<instr>VREDUCEPD</instr>
<desc>Extract the reduced argument of packed double-precision (64-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPD(src1[63:0], imm8[7:0])
{
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
	tmp[63:0] := src1[63:0] - tmp[63:0]
	RETURN tmp[63:0]
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4296</id>
<name>_MM512_REDUCE_ROUND_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT imm8,INT rounding</sign>
<instr>VREDUCEPS</instr>
<desc>Extract the reduced argument of packed single-precision (32-bit) floating-point elements in a by the number of bits specified by imm8, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>ReduceArgumentPS(src1[31:0], imm8[7:0])
{
	IF src1[31:0] == NAN
		RETURN (convert src1[31:0] to QNaN)
	FI
	
	m := imm8[7:4] // number of fraction bits after the binary point to be preserved
	rc := imm8[1:0] // round control
	rc_src := imm8[2] // round ccontrol source
	spe := 0
	tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
	tmp[31:0] := src1[31:0] - tmp[31:0]
	RETURN tmp[31:0]
}
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4311</id>
<name>_MM512_REM_EPI16</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 16-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4315</id>
<name>_MM512_REM_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 32-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4318</id>
<name>_MM512_REM_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 64-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4321</id>
<name>_MM512_REM_EPI8</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed 8-bit integers in a by packed elements in b, and store the remainders as packed 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 63
	i := 8*j
	dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4324</id>
<name>_MM512_REM_EPU16</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 16-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 31
	i := 16*j
	dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4328</id>
<name>_MM512_REM_EPU32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 32-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 15
	i := 32*j
	dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4331</id>
<name>_MM512_REM_EPU64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 64-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 7
	i := 64*j
	dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4334</id>
<name>_MM512_REM_EPU8</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>NONE</instr>
<desc>Divide packed unsigned 8-bit integers in a by packed elements in b, and store the remainders as packed unsigned 32-bit integers in dst.</desc>
<oper>FOR j := 0 to 63
	i := 8*j
	dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4336</id>
<name>_MM512_RINT_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Rounds the packed double-precision (64-bit) floating-point elements in a to the nearest even integer value and stores the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RoundToNearestEven(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4338</id>
<name>_MM512_RINT_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Rounds the packed single-precision (32-bit) floating-point elements in a to the nearest even integer value and stores the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RoundToNearestEven(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4347</id>
<name>_MM512_ROL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,CONST_INT imm8</sign>
<instr>VPROLD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4356</id>
<name>_MM512_ROL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,CONST_INT imm8</sign>
<instr>VPROLQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4365</id>
<name>_MM512_ROLV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPROLVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>LEFT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4374</id>
<name>_MM512_ROLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPROLVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the left by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>LEFT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;lt;&amp;lt; count) OR (src &amp;gt;&amp;gt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4383</id>
<name>_MM512_ROR_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VPRORD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4392</id>
<name>_MM512_ROR_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VPRORQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in imm8, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4401</id>
<name>_MM512_RORV_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPRORVD</instr>
<desc>Rotate the bits in each packed 32-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_DWORDS(src, count_src){
	count := count_src modulo 32
	RETURN (src &amp;gt;&amp;gt;count) OR (src &amp;lt;&amp;lt; (32 - count))
}
FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4410</id>
<name>_MM512_RORV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPRORVQ</instr>
<desc>Rotate the bits in each packed 64-bit integer in a to the right by the number of bits specified in the corresponding element of b, and store the results in dst. </desc>
<oper>RIGHT_ROTATE_QWORDS(src, count_src){
	count := count_src modulo 64
	RETURN (src &amp;gt;&amp;gt; count) OR (src &amp;lt;&amp;lt; (64 - count))
}
FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4420</id>
<name>_MM512_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VROUNDPS</instr>
<desc>Round the packed single-precision (32-bit) floating-point elements in a to the nearest integer value using expadj and in the direction of rounding, and store the results as packed single-precision floating-point elements in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ROUND(a[i+31:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
	_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
	_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
	_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4424</id>
<name>_MM512_ROUNDFXPNT_ADJUST_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VRNDFXPNTPD</instr>
<desc>Performs element-by-element rounding of packed double-precision (64-bit) floating-point elements in a using expadj and in the direction of rounding and stores results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ROUND(a[i+63:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
	_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
	_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
	_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4426</id>
<name>_MM512_ROUNDFXPNT_ADJUST_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding,_MM_EXP_ADJ_ENUM expadj</sign>
<instr>VRNDFXPNTPS</instr>
<desc>Performs element-by-element rounding of packed single-precision (32-bit) floating-point elements in a using expadj and in the direction of rounding and stores results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ROUND(a[i+31:i])
	CASE expadj OF
	_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
	_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
	_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
	_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
	_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
	_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
	_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
	_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
	ESAC
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4435</id>
<name>_MM512_ROUNDSCALE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT imm8</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4444</id>
<name>_MM512_ROUNDSCALE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT imm8</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4447</id>
<name>_MM512_ROUNDSCALE_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT imm8,INT rounding</sign>
<instr>VRNDSCALEPD</instr>
<desc>Round packed double-precision (64-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPD(src[63:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
	1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
	2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
	3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
	ESAC
	
	dst[63:0] := 2^-M * tmp[63:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[63:0] != dst[63:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[63:0]
}	

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4450</id>
<name>_MM512_ROUNDSCALE_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT imm8,INT rounding</sign>
<instr>VRNDSCALEPS</instr>
<desc>Round packed single-precision (32-bit) floating-point elements in a to the number of fraction bits specified by imm8, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>RoundTo_IntegerPS(src[31:0], imm8[7:0]){
	IF(imm8[2] == 1)
		rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
	ELSE
		rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
	FI
	
	M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
	
	CASE(rounding_direction)
	0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
	1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
	2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
	3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
	ESAC
	
	dst[31:0] := 2^-M * tmp[31:0] // scale back down
	
	IF imm8[3] == 0 //check SPE
		IF src[31:0] != dst[31:0] //check if precision has been lost
			set_precision() //set #PE
		FI
	FI
	RETURN dst[31:0]
}	

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4472</id>
<name>_MM512_RSQRT14_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VRSQRT14PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4479</id>
<name>_MM512_RSQRT14_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VRSQRT14PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst. The maximum relative error for this approximation is less than 2^-14.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4487</id>
<name>_MM512_RSQRT23_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VRSQRT23PS</instr>
<desc>Calculates the reciprocal square root of packed single-precision (32-bit) floating-point elements in a to 23 bits of accuracy and stores the result in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := Sqrt(1.0 / a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4490</id>
<name>_MM512_RSQRT28_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VRSQRT28PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, store the results in dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4493</id>
<name>_MM512_RSQRT28_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VRSQRT28PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, store the results in dst. The maximum relative error for this approximation is less than 2^-28.</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4496</id>
<name>_MM512_RSQRT28_ROUND_PD</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VRSQRT28PD</instr>
<desc>Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in a, store the results in dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64;
	dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4499</id>
<name>_MM512_RSQRT28_ROUND_PS</name>
<cpuid>AVX512ER</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VRSQRT28PS</instr>
<desc>Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in a, store the results in dst. The maximum relative error for this approximation is less than 2^-28. Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32;
	dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
ENDFOR;</oper>
</intrinsic>
<intrinsic>
<id>4514</id>
<name>_MM512_SAD_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSADBW</instr>
<desc>Compute the absolute differences of packed unsigned 8-bit integers in a and b, then horizontally sum each consecutive 8 differences to produce eight unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low 16 bits of 64-bit elements in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
ENDFOR
FOR j := 0 to 7
	i := j*64
	dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] + tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
	dst[i+63:i+16] := 0
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4517</id>
<name>_MM512_SBB_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSBBD</instr>
<desc>Performs element-by-element three-input subtraction of packed 32-bit integer elements of v3 as well as the corresponding bit from k from v2. The borrowed value from the subtraction difference for the nth element is written to the nth bit of borrow (borrow flag). Results are stored in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v2[i+31:i] - v3[i+31:i] - k[j]
	borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k[j])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4519</id>
<name>_MM512_SBBR_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__MMASK16 k,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSBBRD</instr>
<desc>Performs element-by-element three-input subtraction of packed 32-bit integer elements of v2 as well as the corresponding bit from k from v3. The borrowed value from the subtraction difference for the nth element is written to the nth bit of borrow (borrow flag). Results are stored in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v3[i+31:i] - v2[i+31:i] - k[j]
	borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k[j])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4521</id>
<name>_MM512_SCALE_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512I b</sign>
<instr>VSCALEPS</instr>
<desc>Scales each single-precision (32-bit) floating-point element in a by multiplying it by 2**exponent, where the exponent is the corresponding 32-bit integer element in b, storing results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4523</id>
<name>_MM512_SCALE_ROUND_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512I b,INT rounding</sign>
<instr>NONE</instr>
<desc>Scales each single-precision (32-bit) floating-point element in a by multiplying it by 2**exponent, where the exponenet is the corresponding 32-bit integer element in b, storing results in dst. Intermediate elements are rounded using rounding.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4532</id>
<name>_MM512_SCALEF_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4541</id>
<name>_MM512_SCALEF_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst.</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4544</id>
<name>_MM512_SCALEF_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT rounding</sign>
<instr>VSCALEFPD</instr>
<desc>Scale the packed double-precision (64-bit) floating-point elements in a using values from b, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
	RETURN dst[63:0]
}

FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4547</id>
<name>_MM512_SCALEF_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT rounding</sign>
<instr>VSCALEFPS</instr>
<desc>Scale the packed single-precision (32-bit) floating-point elements in a using values from b, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>SCALE(src1, src2){
	IF (src2 == NaN)
		IF (src2 == SNaN)
			RETURN QNAN(src2)
		FI
	ELSE IF (src1 == NaN)
		IF (src1 == SNaN)
			RETURN QNAN(src1)
		FI
		IF (src2 != INF)
			RETURN QNAN(src1)
		FI
	ELSE
		tmp_src2 := src2
		tmp_src1 := src1
		IF (src2 is denormal AND MXCSR.DAZ)
			tmp_src2 := 0
		FI
		IF (src1 is denormal AND MXCSR.DAZ)
			tmp_src1 := 0
		FI
	FI
	dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
	RETURN dst[31:0]
}

FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4564</id>
<name>_MM512_SET_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>INT e15,INT e14,INT e13,INT e12,INT e11,INT e10,INT e9,INT e8,INT e7,INT e6,INT e5,INT e4,INT e3,INT e2,INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3
dst[159:128] := e4
dst[191:160] := e5
dst[223:192] := e6
dst[255:224] := e7
dst[287:256] := e8
dst[319:288] := e9
dst[351:320] := e10
dst[383:352] := e11
dst[415:384] := e12
dst[447:416] := e13
dst[479:448] := e14
dst[511:480] := e15
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4566</id>
<name>_MM512_SET_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__INT64 e7,__INT64 e6,__INT64 e5,__INT64 e4,__INT64 e3,__INT64 e2,__INT64 e1,__INT64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1
dst[191:128] := e2
dst[255:192] := e3
dst[319:256] := e4
dst[383:320] := e5
dst[447:384] := e6
dst[511:448] := e7
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4579</id>
<name>_MM512_SET_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>DOUBLE e7,DOUBLE e6,DOUBLE e5,DOUBLE e4,DOUBLE e3,DOUBLE e2,DOUBLE e1,DOUBLE e0</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the supplied values.</desc>
<oper>dst[63:0] := e0
dst[127:64] := e1
dst[191:128] := e2
dst[255:192] := e3
dst[319:256] := e4
dst[383:320] := e5
dst[447:384] := e6
dst[511:448] := e7
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4586</id>
<name>_MM512_SET_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>FLOAT e15,FLOAT e14,FLOAT e13,FLOAT e12,FLOAT e11,FLOAT e10,FLOAT e9,FLOAT e8,FLOAT e7,FLOAT e6,FLOAT e5,FLOAT e4,FLOAT e3,FLOAT e2,FLOAT e1,FLOAT e0</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the supplied values.</desc>
<oper>dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3
dst[159:128] := e4
dst[191:160] := e5
dst[223:192] := e6
dst[255:224] := e7
dst[287:256] := e8
dst[319:288] := e9
dst[351:320] := e10
dst[383:352] := e11
dst[415:384] := e12
dst[447:416] := e13
dst[479:448] := e14
dst[511:480] := e15
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4599</id>
<name>_MM512_SET1_EPI16</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>SHORT a</sign>
<instr>NONE</instr>
<desc>Broadcast the low packed 16-bit integer from a to all all elements of dst.
	</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4608</id>
<name>_MM512_SET1_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>INT a</sign>
<instr>VPBROADCASTD</instr>
<desc>Broadcast 32-bit integer a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4616</id>
<name>_MM512_SET1_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__INT64 a</sign>
<instr>VPBROADCASTQ</instr>
<desc>Broadcast 64-bit integer a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4627</id>
<name>_MM512_SET1_EPI8</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>CHAR a</sign>
<instr>NONE</instr>
<desc>Broadcast 8-bit integer a to all elements of dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4630</id>
<name>_MM512_SET1_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>DOUBLE a</sign>
<instr>NONE</instr>
<desc>Broadcast double-precision (64-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4636</id>
<name>_MM512_SET1_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>FLOAT a</sign>
<instr>NONE</instr>
<desc>Broadcast single-precision (32-bit) floating-point value a to all elements of dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4637</id>
<name>_MM512_SET4_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>INT d,INT c,INT b,INT a</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the repeated 4 element sequence.</desc>
<oper>dst[31:0] := d
dst[63:32] := c
dst[95:64] := b
dst[127:96] := a
dst[159:128] := d
dst[191:160] := c
dst[223:192] := b
dst[255:224] := a
dst[287:256] := d
dst[319:288] := c
dst[351:320] := b
dst[383:352] := a
dst[415:384] := d
dst[447:416] := c
dst[479:448] := b
dst[511:480] := a
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4638</id>
<name>_MM512_SET4_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__INT64 d,__INT64 c,__INT64 b,__INT64 a</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the repeated 4 element sequence.</desc>
<oper>dst[63:0] := d
dst[127:64] := c
dst[191:128] := b
dst[255:192] := a
dst[319:256] := d
dst[383:320] := c
dst[447:384] := b
dst[511:448] := a
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4639</id>
<name>_MM512_SET4_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>DOUBLE d,DOUBLE c,DOUBLE b,DOUBLE a</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the repeated 4 element sequence.</desc>
<oper>dst[63:0] := d
dst[127:64] := c
dst[191:128] := b
dst[255:192] := a
dst[319:256] := d
dst[383:320] := c
dst[447:384] := b
dst[511:448] := a
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4640</id>
<name>_MM512_SET4_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>FLOAT d,FLOAT c,FLOAT b,FLOAT a</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the repeated 4 element sequence.</desc>
<oper>dst[31:0] := d
dst[63:32] := c
dst[95:64] := b
dst[127:96] := a
dst[159:128] := d
dst[191:160] := c
dst[223:192] := b
dst[255:224] := a
dst[287:256] := d
dst[319:288] := c
dst[351:320] := b
dst[383:352] := a
dst[415:384] := d
dst[447:416] := c
dst[479:448] := b
dst[511:480] := a
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4646</id>
<name>_MM512_SETR_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>INT e15,INT e14,INT e13,INT e12,INT e11,INT e10,INT e9,INT e8,INT e7,INT e6,INT e5,INT e4,INT e3,INT e2,INT e1,INT e0</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e15
dst[63:32] := e14
dst[95:64] := e13
dst[127:96] := e12
dst[159:128] := e11
dst[191:160] := e10
dst[223:192] := e9
dst[255:224] := e8
dst[287:256] := e7
dst[319:288] := e6
dst[351:320] := e5
dst[383:352] := e4
dst[415:384] := e3
dst[447:416] := e2
dst[479:448] := e1
dst[511:480] := e0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4648</id>
<name>_MM512_SETR_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__INT64 e7,__INT64 e6,__INT64 e5,__INT64 e4,__INT64 e3,__INT64 e2,__INT64 e1,__INT64 e0</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the supplied values in reverse order.</desc>
<oper>dst[63:0] := e7
dst[127:64] := e6
dst[191:128] := e5
dst[255:192] := e4
dst[319:256] := e3
dst[383:320] := e2
dst[447:384] := e1
dst[511:448] := e0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4657</id>
<name>_MM512_SETR_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>DOUBLE e7,DOUBLE e6,DOUBLE e5,DOUBLE e4,DOUBLE e3,DOUBLE e2,DOUBLE e1,DOUBLE e0</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the supplied values in reverse order.</desc>
<oper>dst[63:0] := e7
dst[127:64] := e6
dst[191:128] := e5
dst[255:192] := e4
dst[319:256] := e3
dst[383:320] := e2
dst[447:384] := e1
dst[511:448] := e0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4663</id>
<name>_MM512_SETR_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>FLOAT e15,FLOAT e14,FLOAT e13,FLOAT e12,FLOAT e11,FLOAT e10,FLOAT e9,FLOAT e8,FLOAT e7,FLOAT e6,FLOAT e5,FLOAT e4,FLOAT e3,FLOAT e2,FLOAT e1,FLOAT e0</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the supplied values in reverse order.</desc>
<oper>dst[31:0] := e15
dst[63:32] := e14
dst[95:64] := e13
dst[127:96] := e12
dst[159:128] := e11
dst[191:160] := e10
dst[223:192] := e9
dst[255:224] := e8
dst[287:256] := e7
dst[319:288] := e6
dst[351:320] := e5
dst[383:352] := e4
dst[415:384] := e3
dst[447:416] := e2
dst[479:448] := e1
dst[511:480] := e0
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4664</id>
<name>_MM512_SETR4_EPI32</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>INT d,INT c,INT b,INT a</sign>
<instr>NONE</instr>
<desc>Set packed 32-bit integers in dst with the repeated 4 element sequence in reverse order.</desc>
<oper>dst[31:0] := a
dst[63:32] := b
dst[95:64] := c
dst[127:96] := d
dst[159:128] := a
dst[191:160] := b
dst[223:192] := c
dst[255:224] := d
dst[287:256] := a
dst[319:288] := b
dst[351:320] := c
dst[383:352] := d
dst[415:384] := a
dst[447:416] := b
dst[479:448] := c
dst[511:480] := d
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4665</id>
<name>_MM512_SETR4_EPI64</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512i</ret>
<sign>__INT64 d,__INT64 c,__INT64 b,__INT64 a</sign>
<instr>NONE</instr>
<desc>Set packed 64-bit integers in dst with the repeated 4 element sequence in reverse order.</desc>
<oper>dst[63:0] := a
dst[127:64] := b
dst[191:128] := c
dst[255:192] := d
dst[319:256] := a
dst[383:320] := b
dst[447:384] := c
dst[511:448] := d
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4666</id>
<name>_MM512_SETR4_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>DOUBLE d,DOUBLE c,DOUBLE b,DOUBLE a</sign>
<instr>NONE</instr>
<desc>Set packed double-precision (64-bit) floating-point elements in dst with the repeated 4 element sequence in reverse order.</desc>
<oper>dst[63:0] := a
dst[127:64] := b
dst[191:128] := c
dst[255:192] := d
dst[319:256] := a
dst[383:320] := b
dst[447:384] := c
dst[511:448] := d
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4667</id>
<name>_MM512_SETR4_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>FLOAT d,FLOAT c,FLOAT b,FLOAT a</sign>
<instr>NONE</instr>
<desc>Set packed single-precision (32-bit) floating-point elements in dst with the repeated 4 element sequence in reverse order.</desc>
<oper>dst[31:0] := a
dst[63:32] := b
dst[95:64] := c
dst[127:96] := d
dst[159:128] := a
dst[191:160] := b
dst[223:192] := c
dst[255:224] := d
dst[287:256] := a
dst[319:288] := b
dst[351:320] := c
dst[383:352] := d
dst[415:384] := a
dst[447:416] := b
dst[479:448] := c
dst[511:480] := d
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4668</id>
<name>_MM512_SETZERO</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign></sign>
<instr>VPXORQ</instr>
<desc>Return vector of type __m512 with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4669</id>
<name>_MM512_SETZERO_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign></sign>
<instr>VPXORQ</instr>
<desc>Return vector of type __m512i with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4672</id>
<name>_MM512_SETZERO_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign></sign>
<instr>VPXORQ</instr>
<desc>Return vector of type __m512d with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4675</id>
<name>_MM512_SETZERO_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign></sign>
<instr>VPXORQ</instr>
<desc>Return vector of type __m512 with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4678</id>
<name>_MM512_SETZERO_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign></sign>
<instr>VPXORQ</instr>
<desc>Return vector of type __m512i with all elements set to zero.</desc>
<oper>dst[MAX:0] := 0</oper>
</intrinsic>
<intrinsic>
<id>4696</id>
<name>_MM512_SHUFFLE_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,_MM_PERM_ENUM imm8</sign>
<instr>VPSHUFD</instr>
<desc>Shuffle 32-bit integers in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(a[255:128], imm8[5:4])
dst[255:224] := SELECT4(a[255:128], imm8[7:6])
dst[287:256] := SELECT4(a[383:256], imm8[1:0])
dst[319:288] := SELECT4(a[383:256], imm8[3:2])
dst[351:320] := SELECT4(a[383:256], imm8[5:4])
dst[383:352] := SELECT4(a[383:256], imm8[7:6])
dst[415:384] := SELECT4(a[511:384], imm8[1:0])
dst[447:416] := SELECT4(a[511:384], imm8[3:2])
dst[479:448] := SELECT4(a[511:384], imm8[5:4])
dst[511:480] := SELECT4(a[511:384], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4705</id>
<name>_MM512_SHUFFLE_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSHUFB</instr>
<desc>Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	IF b[i+7] == 1
		dst[i+7:i] := 0
	ELSE
		index[3:0] := b[i+3:i]
		dst[i+7:i] := a[index*8+7:index*8]
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4711</id>
<name>_MM512_SHUFFLE_F32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VSHUFF32X4</instr>
<desc>Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst.
	</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4717</id>
<name>_MM512_SHUFFLE_F64X2</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VSHUFF64X2</instr>
<desc>Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by imm8 from a and b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4723</id>
<name>_MM512_SHUFFLE_I32X4</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VSHUFI32X4</instr>
<desc>Shuffle 128-bits (composed of 4 32-bit integers) selected by imm8 from a and b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4729</id>
<name>_MM512_SHUFFLE_I64X2</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,CONST_INT imm8</sign>
<instr>VSHUFI64X2</instr>
<desc>Shuffle 128-bits (composed of 2 64-bit integers) selected by imm8 from a and b, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[127:0] := src[127:0]
	1:	tmp[127:0] := src[255:128]
	2:	tmp[127:0] := src[383:256]
	3:	tmp[127:0] := src[511:384]
	ESAC
	RETURN tmp[127:0]
}

dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4738</id>
<name>_MM512_SHUFFLE_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,CONST_INT imm8</sign>
<instr>VSHUFPD</instr>
<desc>Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in imm8, and store the results in dst. </desc>
<oper>dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4749</id>
<name>_MM512_SHUFFLE_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,CONST_INT imm8</sign>
<instr>VSHUFPS</instr>
<desc>Shuffle single-precision (32-bit) floating-point elements in a within 128-bit lanes using the control in imm8, and store the results in dst.</desc>
<oper>SELECT4(src, control){
	CASE(control[1:0])
	0:	tmp[31:0] := src[31:0]
	1:	tmp[31:0] := src[63:32]
	2:	tmp[31:0] := src[95:64]
	3:	tmp[31:0] := src[127:96]
	ESAC
	RETURN tmp[31:0]
}

dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(b[127:0], imm8[5:4])
dst[127:96] := SELECT4(b[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(b[255:128], imm8[5:4])
dst[255:224] := SELECT4(b[255:128], imm8[7:6])
dst[287:256] := SELECT4(a[383:256], imm8[1:0])
dst[319:288] := SELECT4(a[383:256], imm8[3:2])
dst[351:320] := SELECT4(b[383:256], imm8[5:4])
dst[383:352] := SELECT4(b[383:256], imm8[7:6])
dst[415:384] := SELECT4(a[511:384], imm8[1:0])
dst[447:416] := SELECT4(a[511:384], imm8[3:2])
dst[479:448] := SELECT4(b[511:384], imm8[5:4])
dst[511:480] := SELECT4(b[511:384], imm8[7:6])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4758</id>
<name>_MM512_SHUFFLEHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VPSHUFHW</instr>
<desc>Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the high 64 bits of 128-bit lanes of dst, with the low 64 bits of 128-bit lanes being copied from from a to dst.</desc>
<oper>dst[63:0] := a[63:0]
dst[79:64] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[79:64]
dst[95:80] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[79:64]
dst[111:96] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[79:64]
dst[127:112] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[79:64]
dst[191:128] := a[191:128]
dst[207:192] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[207:192]
dst[223:208] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[207:192]
dst[239:224] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[207:192]
dst[255:240] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[207:192]
dst[319:256] := a[319:256]
dst[335:320] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[335:320]
dst[351:336] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[335:320]
dst[367:352] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[335:320]
dst[383:368] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[335:320]
dst[447:384] := a[447:384]
dst[463:448] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[463:448]
dst[479:464] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[463:448]
dst[495:480] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[463:448]
dst[511:496] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[463:448]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4767</id>
<name>_MM512_SHUFFLELO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,INT imm8</sign>
<instr>VPSHUFLW</instr>
<desc>Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of a using the control in imm8. Store the results in the low 64 bits of 128-bit lanes of dst, with the high 64 bits of 128-bit lanes being copied from from a to dst.</desc>
<oper>dst[15:0] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[15:0]
dst[31:16] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[15:0]
dst[47:32] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[15:0]
dst[63:48] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[15:0]
dst[127:64] := a[127:64]
dst[143:128] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[143:128]
dst[159:144] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[143:128]
dst[175:160] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[143:128]
dst[191:176] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[143:128]
dst[255:192] := a[255:192]
dst[271:256] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[271:256]
dst[287:272] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[271:256]
dst[303:288] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[271:256]
dst[319:304] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[271:256]
dst[383:320] := a[383:320]
dst[399:384] := (a &amp;gt;&amp;gt; (imm8[1:0] * 16))[399:384]
dst[415:400] := (a &amp;gt;&amp;gt; (imm8[3:2] * 16))[399:384]
dst[431:416] := (a &amp;gt;&amp;gt; (imm8[5:4] * 16))[399:384]
dst[447:432] := (a &amp;gt;&amp;gt; (imm8[7:6] * 16))[399:384]
dst[511:448] := a[511:448]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4780</id>
<name>_MM512_SIN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SIN(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4784</id>
<name>_MM512_SIN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SIN(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4788</id>
<name>_MM512_SINCOS_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D_PTR cos_res,__M512D a</sign>
<instr>NONE</instr>
<desc>Computes the sine and cosine of the packed double-precision (64-bit) floating-point elements in a and stores the results of the sine computation in dst and the results of the cosine computation in cos_res.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SIN(a[i+63:i])
	cos_res[i+63:i] := COS(a[i+63:i])
ENDFOR
dst[MAX:512] := 0
cos_res[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4792</id>
<name>_MM512_SINCOS_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512_PTR cos_res,__M512 a</sign>
<instr>NONE</instr>
<desc>Computes the sine and cosine of the packed single-precision (32-bit) floating-point elements in a and stores the results of the sine computation in dst and the results of the cosine computation in cos_res.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SIN(a[i+31:i])
	cos_res[i+31:i] := COS(a[i+31:i])
ENDFOR
dst[MAX:512] := 0
cos_res[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4796</id>
<name>_MM512_SIND_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SIND(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4800</id>
<name>_MM512_SIND_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the sine of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SIND(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4804</id>
<name>_MM512_SINH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SINH(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4808</id>
<name>_MM512_SINH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic sine of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SINH(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4817</id>
<name>_MM512_SLL_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4826</id>
<name>_MM512_SLL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4835</id>
<name>_MM512_SLL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4847</id>
<name>_MM512_SLLI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLW</instr>
<desc>Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4856</id>
<name>_MM512_SLLI_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLD</instr>
<desc>Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4865</id>
<name>_MM512_SLLI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSLLQ</instr>
<desc>Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4879</id>
<name>_MM512_SLLV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSLLVW</instr>
<desc>Shift packed 16-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;lt;&amp;lt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4888</id>
<name>_MM512_SLLV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSLLVD</instr>
<desc>Shift packed 32-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;lt;&amp;lt; count[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4897</id>
<name>_MM512_SLLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSLLVQ</instr>
<desc>Shift packed 64-bit integers in a left by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;lt;&amp;lt; count[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4908</id>
<name>_MM512_SQRT_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4917</id>
<name>_MM512_SQRT_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4920</id>
<name>_MM512_SQRT_ROUND_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,INT rounding</sign>
<instr>VSQRTPD</instr>
<desc>Compute the square root of packed double-precision (64-bit) floating-point elements in a, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4923</id>
<name>_MM512_SQRT_ROUND_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,INT rounding</sign>
<instr>VSQRTPS</instr>
<desc>Compute the square root of packed single-precision (32-bit) floating-point elements in a, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4944</id>
<name>_MM512_SRA_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4953</id>
<name>_MM512_SRA_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4962</id>
<name>_MM512_SRA_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := SignBit
	ELSE
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4973</id>
<name>_MM512_SRAI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := SignBit
	ELSE
		dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4982</id>
<name>_MM512_SRAI_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := SignBit
	ELSE
		dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>4991</id>
<name>_MM512_SRAI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRAQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := SignBit
	ELSE
		dst[i+63:i] := SignExtend(a[i+63:i] &amp;lt;&amp;lt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5002</id>
<name>_MM512_SRAV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSRAVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := SignExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5011</id>
<name>_MM512_SRAV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSRAVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := SignExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5020</id>
<name>_MM512_SRAV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSRAVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in sign bits, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := SignExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5029</id>
<name>_MM512_SRL_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF count[63:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5038</id>
<name>_MM512_SRL_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF count[63:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5047</id>
<name>_MM512_SRL_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M128I count</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF count[63:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[63:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5059</id>
<name>_MM512_SRLI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLW</instr>
<desc>Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF imm8[7:0] &amp;gt; 15
		dst[i+15:i] := 0
	ELSE
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5068</id>
<name>_MM512_SRLI_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLD</instr>
<desc>Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	IF imm8[7:0] &amp;gt; 31
		dst[i+31:i] := 0
	ELSE
		dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5077</id>
<name>_MM512_SRLI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,UNSIGNED_INT imm8</sign>
<instr>VPSRLQ</instr>
<desc>Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	IF imm8[7:0] &amp;gt; 63
		dst[i+63:i] := 0
	ELSE
		dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; imm8[7:0])
	FI
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5091</id>
<name>_MM512_SRLV_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSRLVW</instr>
<desc>Shift packed 16-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 31
	i := j*16
	IF k[j]
		dst[i+15:i] := ZeroExtend(a[i+15:i] &amp;gt;&amp;gt; count[i+15:i])
	ELSE
		dst[i+15:i] := 0
	FI	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5100</id>
<name>_MM512_SRLV_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSRLVD</instr>
<desc>Shift packed 32-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := ZeroExtend(a[i+31:i] &amp;gt;&amp;gt; count[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5109</id>
<name>_MM512_SRLV_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I count</sign>
<instr>VPSRLVQ</instr>
<desc>Shift packed 64-bit integers in a right by the amount specified by the corresponding element in count while shifting in zeros, and store the results in dst. </desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ZeroExtend(a[i+63:i] &amp;gt;&amp;gt; count[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5113</id>
<name>_MM512_STORE_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512I a</sign>
<instr>VMOVDQA32</instr>
<desc>Store 512-bits (composed of 16 packed 32-bit integers) from a into memory. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5117</id>
<name>_MM512_STORE_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512I a</sign>
<instr>VMOVDQA64</instr>
<desc>Store 512-bits (composed of 8 packed 64-bit integers) from a into memory. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5123</id>
<name>_MM512_STORE_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512D a</sign>
<instr>VMOVAPD</instr>
<desc>Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory.
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5130</id>
<name>_MM512_STORE_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512 a</sign>
<instr>VMOVAPS</instr>
<desc>Store 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from a into memory. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5136</id>
<name>_MM512_STORE_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512I a</sign>
<instr>VMOVDQA32</instr>
<desc>Store 512-bits of integer data from a into memory. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5149</id>
<name>_MM512_STORENR_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v</sign>
<instr>VMOVNRAPD</instr>
<desc>Stores packed double-precision (64-bit) floating-point elements from v to memory address mt with a no-read hint to the processor.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 7
	i := j*64
	addr[i+63:i] := v[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5150</id>
<name>_MM512_STORENR_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v</sign>
<instr>VMOVNRAPS</instr>
<desc>Stores packed single-precision (32-bit) floating-point elements from v to memory address mt with a no-read hint to the processor.</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 15
	i := j*32
	addr[i+31:i] := v[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5151</id>
<name>_MM512_STORENRNGO_PD</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512D v</sign>
<instr>VMOVNRNGOAPD</instr>
<desc>Stores packed double-precision (64-bit) floating-point elements from v to memory address mt with a no-read hint and using a weakly-ordered memory consistency model (stores performed with this function are not globally ordered, and subsequent stores from the same thread can be observed before them).</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 7
	i := j*64
	addr[i+63:i] := v[i+63:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5152</id>
<name>_MM512_STORENRNGO_PS</name>
<cpuid>KNCNI</cpuid>
<ret>void</ret>
<sign>VOID_PTR mt,__M512 v</sign>
<instr>VMOVNRNGOAPS</instr>
<desc>Stores packed single-precision (32-bit) floating-point elements from v to memory address mt with a no-read hint and using a weakly-ordered memory consistency model (stores performed with this function are not globally ordered, and subsequent stores from the same thread can be observed before them).</desc>
<oper>addr := MEM[mt]
FOR j := 0 to 15
	i := j*32
	addr[i+31:i] := v[i+31:i]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5172</id>
<name>_MM512_STOREU_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512D a</sign>
<instr>VMOVUPD</instr>
<desc>Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory. 
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5178</id>
<name>_MM512_STOREU_PS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512 a</sign>
<instr>VMOVUPS</instr>
<desc>Store 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from a into memory. 
	mem_addr does not need to be aligned on any particular boundary.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5185</id>
<name>_MM512_STOREU_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512I a</sign>
<instr>VMOVDQU32</instr>
<desc>Store 512-bits of integer data from a into memory.
	mem_addr does not need to be aligned on any particular boundary.
	</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5193</id>
<name>_MM512_STREAM_LOAD_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>VOID_CONST_PTR mem_addr</sign>
<instr>VMOVNTDQA</instr>
<desc>Load 512-bits of integer data from memory into dst using a non-temporal memory hint. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5196</id>
<name>_MM512_STREAM_PD</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512D a</sign>
<instr>VMOVNTPD</instr>
<desc>Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory using a non-temporal memory hint. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5200</id>
<name>_MM512_STREAM_PS</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512 a</sign>
<instr>VMOVNTPS</instr>
<desc>Store 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from a into memory using a non-temporal memory hint. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5204</id>
<name>_MM512_STREAM_SI512</name>
<cpuid>AVX512F</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,__M512I a</sign>
<instr>VMOVNTDQA</instr>
<desc>Store 512-bits of integer data from a into memory using a non-temporal memory hint. 
	mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.</desc>
<oper>MEM[mem_addr+511:mem_addr] := a[511:0]</oper>
</intrinsic>
<intrinsic>
<id>5214</id>
<name>_MM512_SUB_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := a[i+15:i] - b[i+15:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5223</id>
<name>_MM512_SUB_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBD</instr>
<desc>Subtract packed 32-bit integers in b from packed 32-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5232</id>
<name>_MM512_SUB_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBQ</instr>
<desc>Subtract packed 64-bit integers in b from packed 64-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5241</id>
<name>_MM512_SUB_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := a[i+7:i] - b[i+7:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5250</id>
<name>_MM512_SUB_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5262</id>
<name>_MM512_SUB_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5265</id>
<name>_MM512_SUB_ROUND_PD</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b,INT rounding</sign>
<instr>VSUBPD</instr>
<desc>Subtract packed double-precision (64-bit) floating-point elements in b from packed double-precision (64-bit) floating-point elements in a, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5268</id>
<name>_MM512_SUB_ROUND_PS</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b,INT rounding</sign>
<instr>VSUBPS</instr>
<desc>Subtract packed single-precision (32-bit) floating-point elements in b from packed single-precision (32-bit) floating-point elements in a, and store the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] - b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5285</id>
<name>_MM512_SUBR_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__M512I v3</sign>
<instr>VPSUBRD</instr>
<desc>Performs element-by-element subtraction of packed 32-bit integer elements in v2 from v3 storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5287</id>
<name>_MM512_SUBR_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v2,__M512D v3</sign>
<instr>VSUBRPD</instr>
<desc>Performs element-by-element subtraction of packed double-precision (64-bit) floating-point elements in v2 from v3 storing the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5289</id>
<name>_MM512_SUBR_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2,__M512 v3</sign>
<instr>VSUBRPS</instr>
<desc>Performs element-by-element subtraction of packed single-precision (32-bit) floating-point elements in v2 from v3 storing the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5291</id>
<name>_MM512_SUBR_ROUND_PD</name>
<cpuid>KNCNI</cpuid>
<ret>__m512d</ret>
<sign>__M512D v2,__M512D v3,INT rounding</sign>
<instr>VSUBRPD</instr>
<desc>Performs element-by-element subtraction of packed double-precision (64-bit) floating-point elements in v2 from v3 storing the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5293</id>
<name>_MM512_SUBR_ROUND_PS</name>
<cpuid>KNCNI</cpuid>
<ret>__m512</ret>
<sign>__M512 v2,__M512 v3,INT rounding</sign>
<instr>VSUBRPS</instr>
<desc>Performs element-by-element subtraction of packed single-precision (32-bit) floating-point elements in v2 from v3 storing the results in dst.
	Rounding is done according to the rounding parameter, which can be one of:    (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
    (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
    (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
    (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
    _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v3[i+31:i] - v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5295</id>
<name>_MM512_SUBRSETB_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSUBRSETBD</instr>
<desc>Performs element-by-element subtraction of packed 32-bit integer elements in v2 from v3, storing the results in dst and v2. The borrowed value from the subtraction difference for the nth element is written to the nth bit of borrow (borrow flag).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
	borrow[j] := Borrow(v3[i+31:i] - v2[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5304</id>
<name>_MM512_SUBS_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBSW</instr>
<desc>Subtract packed 16-bit integers in b from packed 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5313</id>
<name>_MM512_SUBS_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBSB</instr>
<desc>Subtract packed 8-bit integers in b from packed 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5322</id>
<name>_MM512_SUBS_EPU16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBUSW</instr>
<desc>Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5331</id>
<name>_MM512_SUBS_EPU8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPSUBUSB</instr>
<desc>Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit integers in a using saturation, and store the results in dst.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5337</id>
<name>_MM512_SUBSETB_EPI32</name>
<cpuid>KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I v2,__M512I v3,__MMASK16_PTR borrow</sign>
<instr>VPSUBSETBD</instr>
<desc>Performs element-by-element subtraction of packed 32-bit integer elements in v3 from v2, storing the results in dst and the nth borrow bit in the nth position of borrow (borrow flag).</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := v2[i+31:i] - v3[i+31:i]
	borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5349</id>
<name>_MM512_SVML_ROUND_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Round the packed double-precision (64-bit) floating-point elements in a to the nearest integer value, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := ROUND(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5357</id>
<name>_MM512_SWIZZLE_EPI32</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the four groups of packed 4x 32-bit integer elements in v using swizzle parameter s, storing the results in dst.</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 7
		i := j*64
		dst[i+31:i]    := v[i+63:i+32]
		dst[i+63:i+32] := v[i+31:i]
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]	    := v[i+95:i+64]
		dst[i+63:i+32]  := v[i+127:i+96]
		dst[i+95:i+64]  := v[i+31:i]
		dst[i+127:i+96] := v[i+63:i+32]
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]	    := v[i+31:i]
		dst[i+63:i+32]  := v[i+31:i]
		dst[i+95:i+64]  := v[i+31:i]
		dst[i+127:i+96] := v[i+31:i]
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]	    := v[i+63:i+32]
		dst[i+63:i+32]  := v[i+63:i+32]
		dst[i+95:i+64]  := v[i+63:i+32]
		dst[i+127:i+96] := v[i+63:i+32]
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]	    := v[i+95:i+64]
		dst[i+63:i+32]  := v[i+95:i+64]
		dst[i+95:i+64]  := v[i+95:i+64]
		dst[i+127:i+96] := v[i+95:i+64]
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]	    := v[i+127:i+96]
		dst[i+63:i+32]  := v[i+127:i+96]
		dst[i+95:i+64]  := v[i+127:i+96]
		dst[i+127:i+96] := v[i+127:i+96]
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]	    := v[i+63:i+32]
		dst[i+63:i+32]  := v[i+95:i+64]
		dst[i+95:i+64]  := v[i+31:i]
		dst[i+127:i+96] := v[i+127:i+96]
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5359</id>
<name>_MM512_SWIZZLE_EPI64</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512i</ret>
<sign>__M512I v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the two groups of packed 4x64-bit integer elements in v using swizzle parameter s, storing the results in dst.</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 3
		i := j*64
		dst[i+63:i]	    := v[i+127:i+64]
		dst[i+127:i+64] := v[i+63:i]
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+191:i+128]
		dst[i+127:i+64]  := v[i+255:i+192]
		dst[i+191:i+128] := v[i+63:i]
		dst[i+255:i+192] := v[i+127:i+64]
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+63:i]
		dst[i+127:i+64]  := v[i+63:i]
		dst[i+191:i+128] := v[i+63:i]
		dst[i+255:i+192] := v[i+63:i]
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+127:i+63]
		dst[i+127:i+64]  := v[i+127:i+63]
		dst[i+191:i+128] := v[i+127:i+63]
		dst[i+255:i+192] := v[i+127:i+63]
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+191:i+128]
		dst[i+127:i+64]  := v[i+191:i+128]
		dst[i+191:i+128] := v[i+191:i+128]
		dst[i+255:i+192] := v[i+191:i+128]
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+255:i+192]
		dst[i+127:i+64]  := v[i+255:i+192]
		dst[i+191:i+128] := v[i+255:i+192]
		dst[i+255:i+192] := v[i+255:i+192]
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+127:i+64]
		dst[i+127:i+64]  := v[i+191:i+128]
		dst[i+191:i+128] := v[i+63:i]
		dst[i+255:i+192] := v[i+255:i+192]
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5361</id>
<name>_MM512_SWIZZLE_PD</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the two groups of packed 4x double-precision (64-bit) floating-point elements in v using swizzle parameter s, storing the results in dst.</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 3
		i := j*64
		dst[i+63:i]     := v[i+127:i+64]
		dst[i+127:i+64] := v[i+63:i]
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]      := v[i+191:i+128]
		dst[i+127:i+64]  := v[i+255:i+192]
		dst[i+191:i+128] := v[i+63:i]
		dst[i+255:i+192] := v[i+127:i+64]
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]      := v[i+63:i]
		dst[i+127:i+64]  := v[i+63:i]
		dst[i+191:i+128] := v[i+63:i]
		dst[i+255:i+192] := v[i+63:i]
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]      := v[i+127:i+63]
		dst[i+127:i+64]  := v[i+127:i+63]
		dst[i+191:i+128] := v[i+127:i+63]
		dst[i+255:i+192] := v[i+127:i+63]
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]      := v[i+191:i+128]
		dst[i+127:i+64]  := v[i+191:i+128]
		dst[i+191:i+128] := v[i+191:i+128]
		dst[i+255:i+192] := v[i+191:i+128]
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+255:i+192]
		dst[i+127:i+64]  := v[i+255:i+192]
		dst[i+191:i+128] := v[i+255:i+192]
		dst[i+255:i+192] := v[i+255:i+192]
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 1
		i := j*256
		dst[i+63:i]	     := v[i+127:i+64]
		dst[i+127:i+64]  := v[i+191:i+128]
		dst[i+191:i+128] := v[i+63:i]
		dst[i+255:i+192] := v[i+255:i+192]
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5363</id>
<name>_MM512_SWIZZLE_PS</name>
<cpuid>KNCNI, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 v,_MM_SWIZZLE_ENUM s</sign>
<instr>NONE</instr>
<desc>Performs a swizzle transformation of each of the four groups of packed 4xsingle-precision (32-bit) floating-point elements in v using swizzle parameter s, storing the results in dst.</desc>
<oper>CASE s OF
_MM_SWIZ_REG_NONE:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_DCBA:
	dst[511:0] := v[511:0]
_MM_SWIZ_REG_CDAB:
	FOR j := 0 to 7
		i := j*64
		dst[i+31:i]    := v[i+63:i+32]
		dst[i+63:i+32] := v[i+31:i]
	ENDFOR
_MM_SWIZ_REG_BADC:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]     := v[i+95:i+64]
		dst[i+63:i+32]  := v[i+127:i+96]
		dst[i+95:i+64]  := v[i+31:i]
		dst[i+127:i+96] := v[i+63:i+32]
	ENDFOR
_MM_SWIZ_REG_AAAA:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]     := v[i+31:i]
		dst[i+63:i+32]  := v[i+31:i]
		dst[i+95:i+64]  := v[i+31:i]
		dst[i+127:i+96] := v[i+31:i]
	ENDFOR
_MM_SWIZ_REG_BBBB:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]     := v[i+63:i+32]
		dst[i+63:i+32]  := v[i+63:i+32]
		dst[i+95:i+64]  := v[i+63:i+32]
		dst[i+127:i+96] := v[i+63:i+32]
	ENDFOR
_MM_SWIZ_REG_CCCC:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]     := v[i+95:i+64]
		dst[i+63:i+32]  := v[i+95:i+64]
		dst[i+95:i+64]  := v[i+95:i+64]
		dst[i+127:i+96] := v[i+95:i+64]
	ENDFOR
_MM_SWIZ_REG_DDDD:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]     := v[i+127:i+96]
		dst[i+63:i+32]  := v[i+127:i+96]
		dst[i+95:i+64]  := v[i+127:i+96]
		dst[i+127:i+96] := v[i+127:i+96]
	ENDFOR
_MM_SWIZ_REG_DACB:
	FOR j := 0 to 3
		i := j*128
		dst[i+31:i]     := v[i+63:i+32]
		dst[i+63:i+32]  := v[i+95:i+64]
		dst[i+95:i+64]  := v[i+31:i]
		dst[i+127:i+96] := v[i+127:i+96]
	ENDFOR
ESAC
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5367</id>
<name>_MM512_TAN_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := TAN(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5371</id>
<name>_MM512_TAN_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := TAN(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5375</id>
<name>_MM512_TAND_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed double-precision (64-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := TAND(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5379</id>
<name>_MM512_TAND_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the tangent of packed single-precision (32-bit) floating-point elements in a expressed in degrees, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := TAND(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5383</id>
<name>_MM512_TANH_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed double-precision (64-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := TANH(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5387</id>
<name>_MM512_TANH_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Compute the hyperbolic tangent of packed single-precision (32-bit) floating-point elements in a expressed in radians, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := TANH(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5396</id>
<name>_MM512_TERNARYLOGIC_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,__M512I c,INT imm8</sign>
<instr>VPTERNLOGD</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 32-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	FOR h := 0 to 31
		index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
		dst[i+h] := imm8[index[2:0]]
	ENDFOR
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5405</id>
<name>_MM512_TERNARYLOGIC_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b,__M512I c,INT imm8</sign>
<instr>VPTERNLOGQ</instr>
<desc>Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in imm8. For each bit in each packed 64-bit integer, the corresponding bit from a, b, and c are used to form a 3 bit index into imm8, and the value at that bit in imm8 is written to the corresponding bit in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	FOR h := 0 to 63
		index[2:0] := (a[i+h] &amp;lt;&amp;lt; 2) OR (b[i+h] &amp;lt;&amp;lt; 1) OR c[i+h]
		dst[i+h] := imm8[index[2:0]]
	ENDFOR
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5413</id>
<name>_MM512_TEST_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTMW</instr>
<desc>Compute the bitwise AND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5419</id>
<name>_MM512_TEST_EPI32_MASK</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTMD</instr>
<desc>Compute the bitwise AND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5425</id>
<name>_MM512_TEST_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTMQ</instr>
<desc>Compute the bitwise AND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5431</id>
<name>_MM512_TEST_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTMB</instr>
<desc>Compute the bitwise AND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k if the intermediate value is non-zero.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>5444</id>
<name>_MM512_TESTN_EPI16_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask32</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTNMW</instr>
<desc>Compute the bitwise NAND of packed 16-bit integers in a and b, producing intermediate 16-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 31
	i := j*16
	k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5450</id>
<name>_MM512_TESTN_EPI32_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask16</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTNMD</instr>
<desc>Compute the bitwise NAND of packed 32-bit integers in a and b, producing intermediate 32-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0</oper>
</intrinsic>
<intrinsic>
<id>5456</id>
<name>_MM512_TESTN_EPI64_MASK</name>
<cpuid>AVX512F</cpuid>
<ret>__mmask8</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTNMQ</instr>
<desc>Compute the bitwise NAND of packed 64-bit integers in a and b, producing intermediate 64-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0</oper>
</intrinsic>
<intrinsic>
<id>5462</id>
<name>_MM512_TESTN_EPI8_MASK</name>
<cpuid>AVX512BW</cpuid>
<ret>__mmask64</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPTESTNMB</instr>
<desc>Compute the bitwise NAND of packed 8-bit integers in a and b, producing intermediate 8-bit values, and set the corresponding bit in result mask k if the intermediate value is zero.</desc>
<oper>FOR j := 0 to 63
	i := j*8
	k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:64] := 0</oper>
</intrinsic>
<intrinsic>
<id>5481</id>
<name>_MM512_TRUNC_PD</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512d</ret>
<sign>__M512D a</sign>
<instr>NONE</instr>
<desc>Truncate the packed double-precision (64-bit) floating-point elements in a, and store the results as packed double-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := TRUNCATE(a[i+63:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5485</id>
<name>_MM512_TRUNC_PS</name>
<cpuid>AVX512F, SVML</cpuid>
<ret>__m512</ret>
<sign>__M512 a</sign>
<instr>NONE</instr>
<desc>Truncate the packed single-precision (32-bit) floating-point elements in a, and store the results as packed single-precision floating-point elements in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := TRUNCATE(a[i+31:i])
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5508</id>
<name>_MM512_UNDEFINED</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m512 with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5509</id>
<name>_MM512_UNDEFINED_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m512i with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5512</id>
<name>_MM512_UNDEFINED_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m512d with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5515</id>
<name>_MM512_UNDEFINED_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign></sign>
<instr></instr>
<desc>Return vector of type __m512 with undefined elements.</desc>
<oper></oper>
</intrinsic>
<intrinsic>
<id>5526</id>
<name>_MM512_UNPACKHI_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKHWD</instr>
<desc>Unpack and interleave 16-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[79:64]
	dst[31:16] := src2[79:64] 
	dst[47:32] := src1[95:80] 
	dst[63:48] := src2[95:80] 
	dst[79:64] := src1[111:96] 
	dst[95:80] := src2[111:96] 
	dst[111:96] := src1[127:112] 
	dst[127:112] := src2[127:112] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5535</id>
<name>_MM512_UNPACKHI_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKHDQ</instr>
<desc>Unpack and interleave 32-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5544</id>
<name>_MM512_UNPACKHI_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKHQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5553</id>
<name>_MM512_UNPACKHI_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKHBW</instr>
<desc>Unpack and interleave 8-bit integers from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[71:64] 
	dst[15:8] := src2[71:64] 
	dst[23:16] := src1[79:72] 
	dst[31:24] := src2[79:72] 
	dst[39:32] := src1[87:80] 
	dst[47:40] := src2[87:80] 
	dst[55:48] := src1[95:88] 
	dst[63:56] := src2[95:88] 
	dst[71:64] := src1[103:96] 
	dst[79:72] := src2[103:96] 
	dst[87:80] := src1[111:104] 
	dst[95:88] := src2[111:104] 
	dst[103:96] := src1[119:112] 
	dst[111:104] := src2[119:112] 
	dst[119:112] := src1[127:120] 
	dst[127:120] := src2[127:120] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5562</id>
<name>_MM512_UNPACKHI_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VUNPCKHPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[127:64] 
	dst[127:64] := src2[127:64] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5574</id>
<name>_MM512_UNPACKHI_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VUNPCKHPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[95:64] 
	dst[63:32] := src2[95:64] 
	dst[95:64] := src1[127:96] 
	dst[127:96] := src2[127:96] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5583</id>
<name>_MM512_UNPACKLO_EPI16</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKLWD</instr>
<desc>Unpack and interleave 16-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
	dst[15:0] := src1[15:0] 
	dst[31:16] := src2[15:0] 
	dst[47:32] := src1[31:16] 
	dst[63:48] := src2[31:16] 
	dst[79:64] := src1[47:32] 
	dst[95:80] := src2[47:32] 
	dst[111:96] := src1[63:48] 
	dst[127:112] := src2[63:48] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5592</id>
<name>_MM512_UNPACKLO_EPI32</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKLDQ</instr>
<desc>Unpack and interleave 32-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5601</id>
<name>_MM512_UNPACKLO_EPI64</name>
<cpuid>AVX512F</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKLQDQ</instr>
<desc>Unpack and interleave 64-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5610</id>
<name>_MM512_UNPACKLO_EPI8</name>
<cpuid>AVX512BW</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPUNPCKLBW</instr>
<desc>Unpack and interleave 8-bit integers from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
	dst[7:0] := src1[7:0] 
	dst[15:8] := src2[7:0] 
	dst[23:16] := src1[15:8] 
	dst[31:24] := src2[15:8] 
	dst[39:32] := src1[23:16] 
	dst[47:40] := src2[23:16] 
	dst[55:48] := src1[31:24] 
	dst[63:56] := src2[31:24] 
	dst[71:64] := src1[39:32]
	dst[79:72] := src2[39:32] 
	dst[87:80] := src1[47:40] 
	dst[95:88] := src2[47:40] 
	dst[103:96] := src1[55:48] 
	dst[111:104] := src2[55:48] 
	dst[119:112] := src1[63:56] 
	dst[127:120] := src2[63:56] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5619</id>
<name>_MM512_UNPACKLO_PD</name>
<cpuid>AVX512F</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VUNPCKLPD</instr>
<desc>Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
	dst[63:0] := src1[63:0] 
	dst[127:64] := src2[63:0] 
	RETURN dst[127:0]
}

dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5631</id>
<name>_MM512_UNPACKLO_PS</name>
<cpuid>AVX512F</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VUNPCKLPS</instr>
<desc>Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane in a and b, and store the results in dst. </desc>
<oper>INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
	dst[31:0] := src1[31:0] 
	dst[63:32] := src2[31:0] 
	dst[95:64] := src1[63:32] 
	dst[127:96] := src2[63:32] 
	RETURN dst[127:0]
}	

dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5648</id>
<name>_MM512_XOR_EPI32</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of packed 32-bit integers in a and b, and store the results in dst.</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5655</id>
<name>_MM512_XOR_EPI64</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPXORQ</instr>
<desc>Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5664</id>
<name>_MM512_XOR_PD</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512d</ret>
<sign>__M512D a,__M512D b</sign>
<instr>VXORPD</instr>
<desc>Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 7
	i := j*64
	dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5673</id>
<name>_MM512_XOR_PS</name>
<cpuid>AVX512DQ</cpuid>
<ret>__m512</ret>
<sign>__M512 a,__M512 b</sign>
<instr>VXORPS</instr>
<desc>Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in a and b, and store the results in dst.
	</desc>
<oper>FOR j := 0 to 15
	i := j*32
	dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>5676</id>
<name>_MM512_XOR_SI512</name>
<cpuid>AVX512F, KNCNI</cpuid>
<ret>__m512i</ret>
<sign>__M512I a,__M512I b</sign>
<instr>VPXORD</instr>
<desc>Compute the bitwise XOR of 512 bits (representing integer data) in a and b, and store the result in dst.</desc>
<oper>dst[511:0] := (a[511:0] XOR b[511:0])
dst[MAX:512] := 0</oper>
</intrinsic>
<intrinsic>
<id>3854</id>
<name>_PDEP_U32</name>
<cpuid>BMI2</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a,UNSIGNED_INT mask</sign>
<instr>PDEP</instr>
<desc>Deposit contiguous low bits from unsigned 32-bit integer a to dst at the corresponding bit locations specified by mask; all other bits in dst are set to zero.</desc>
<oper>tmp := a
dst := 0
m := 0
k := 0
DO WHILE m &amp;lt; 32
	IF mask[m] = 1
		dst[m] := tmp[k]
		k := k + 1
	FI
	m := m + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>3855</id>
<name>_PDEP_U64</name>
<cpuid>BMI2</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a,UNSIGNED__INT64 mask</sign>
<instr>PDEP</instr>
<desc>Deposit contiguous low bits from unsigned 64-bit integer a to dst at the corresponding bit locations specified by mask; all other bits in dst are set to zero.</desc>
<oper>tmp := a
dst := 0
m := 0
k := 0
DO WHILE m &amp;lt; 64
	IF mask[m] = 1
		dst[m] := tmp[k]
		k := k + 1
	FI
	m := m + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>4032</id>
<name>_PEXT_U32</name>
<cpuid>BMI2</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a,UNSIGNED_INT mask</sign>
<instr>PEXT</instr>
<desc>Extract bits from unsigned 32-bit integer a at the corresponding bit locations specified by mask to contiguous low bits in dst; the remaining upper bits in dst are set to zero.</desc>
<oper>tmp := a
dst := 0
m := 0
k := 0
DO WHILE m &amp;lt; 32
	IF mask[m] = 1
		dst[k] := tmp[m]
		k := k + 1
	FI
	m := m + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>4033</id>
<name>_PEXT_U64</name>
<cpuid>BMI2</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a,UNSIGNED__INT64 mask</sign>
<instr>PEXT</instr>
<desc>Extract bits from unsigned 64-bit integer a at the corresponding bit locations specified by mask to contiguous low bits in dst; the remaining upper bits in dst are set to zero.</desc>
<oper>tmp := a
dst := 0
m := 0
k := 0
DO WHILE m &amp;lt; 64
	IF mask[m] = 1
		dst[k] := tmp[m]
		k := k + 1
	FI
	m := m + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>4047</id>
<name>_POPCNT32</name>
<cpuid>POPCNT</cpuid>
<ret>int</ret>
<sign>INT a</sign>
<instr>POPCNT</instr>
<desc>
		Count the number of bits set to 1 in 32-bit integer a, and return that count in dst. 
	</desc>
<oper>dst := 0
FOR i := 0 to 31
	IF a[i]
		dst := dst + 1
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4048</id>
<name>_POPCNT64</name>
<cpuid>POPCNT</cpuid>
<ret>int</ret>
<sign>__INT64 a</sign>
<instr>POPCNT</instr>
<desc>
		Count the number of bits set to 1 in 64-bit integer a, and return that count in dst. 
	</desc>
<oper>dst := 0
FOR i := 0 to 63
	IF a[i]
		dst := dst + 1
	FI
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>4200</id>
<name>_RDPMC</name>
<cpuid></cpuid>
<ret>__int64</ret>
<sign>INT a</sign>
<instr>RDPMC</instr>
<desc>Read the Performance Monitor Counter (PMC) specified by a, and store up to 64-bits in dst. The width of performance counters is implementation specific.</desc>
<oper>dst[63:0] := ReadPMC(a)</oper>
</intrinsic>
<intrinsic>
<id>4201</id>
<name>_RDRAND16_STEP</name>
<cpuid>RDRAND</cpuid>
<ret>int</ret>
<sign>UNSIGNED_SHORT_PTR val</sign>
<instr>RDRAND</instr>
<desc>Read a hardware generated 16-bit random value and store the result in val. Return 1 if a random value was generated, and 0 otherwise.
</desc>
<oper>IF HW_RND_GEN.ready = 1
	val[15:0] := HW_RND_GEN.data;
	RETURN 1;
ELSE
	val[15:0] := 0;
	RETURN 0;
FI</oper>
</intrinsic>
<intrinsic>
<id>4202</id>
<name>_RDRAND32_STEP</name>
<cpuid>RDRAND</cpuid>
<ret>int</ret>
<sign>UNSIGNED_INT_PTR val</sign>
<instr>RDRAND</instr>
<desc>Read a hardware generated 32-bit random value and store the result in val. Return 1 if a random value was generated, and 0 otherwise.
</desc>
<oper>IF HW_RND_GEN.ready = 1
	val[31:0] := HW_RND_GEN.data;
	RETURN 1;
ELSE
	val[31:0] := 0;
	RETURN 0;
FI</oper>
</intrinsic>
<intrinsic>
<id>4203</id>
<name>_RDRAND64_STEP</name>
<cpuid>RDRAND</cpuid>
<ret>int</ret>
<sign>UNSIGNED__INT64_PTR val</sign>
<instr>RDRAND</instr>
<desc>Read a hardware generated 64-bit random value and store the result in val. Return 1 if a random value was generated, and 0 otherwise.
</desc>
<oper>IF HW_RND_GEN.ready = 1
	val[63:0] := HW_RND_GEN.data;
	RETURN 1;
ELSE
	val[63:0] := 0;
	RETURN 0;
FI</oper>
</intrinsic>
<intrinsic>
<id>4204</id>
<name>_RDSEED16_STEP</name>
<cpuid>RDSEED</cpuid>
<ret>int</ret>
<sign>UNSIGNED_SHORT_PTR val</sign>
<instr>RDSEED</instr>
<desc>Read a 16-bit NIST SP800-90B and SP800-90C compliant random value and store in val. Return 1 if a random value was generated, and 0 otherwise.</desc>
<oper>IF HW_NRND_GEN.ready = 1 THEN
	val[15:0] := HW_NRND_GEN.data
	RETURN 1
ELSE
	val[15:0] := 0
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>4205</id>
<name>_RDSEED32_STEP</name>
<cpuid>RDSEED</cpuid>
<ret>int</ret>
<sign>UNSIGNED_INT_PTR val</sign>
<instr>RDSEED</instr>
<desc>Read a 32-bit NIST SP800-90B and SP800-90C compliant random value and store in val. Return 1 if a random value was generated, and 0 otherwise.</desc>
<oper>IF HW_NRND_GEN.ready = 1 THEN
	val[31:0] := HW_NRND_GEN.data
	RETURN 1
ELSE
	val[31:0] := 0
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>4206</id>
<name>_RDSEED64_STEP</name>
<cpuid>RDSEED</cpuid>
<ret>int</ret>
<sign>UNSIGNED__INT64_PTR val</sign>
<instr>RDSEED</instr>
<desc>Read a 64-bit NIST SP800-90B and SP800-90C compliant random value and store in val. Return 1 if a random value was generated, and 0 otherwise.</desc>
<oper>IF HW_NRND_GEN.ready = 1 THEN
	val[63:0] := HW_NRND_GEN.data
	RETURN 1
ELSE
	val[63:0] := 0
	RETURN 0
FI</oper>
</intrinsic>
<intrinsic>
<id>4207</id>
<name>_RDTSC</name>
<cpuid>TSC</cpuid>
<ret>__int64</ret>
<sign></sign>
<instr>RDTSC</instr>
<desc>Copy the current 64-bit value of the processor's time-stamp counter into dst.</desc>
<oper>dst[63:0] := TimeStampCounter</oper>
</intrinsic>
<intrinsic>
<id>4209</id>
<name>_READFSBASE_U32</name>
<cpuid>FSGSBASE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr>RDFSBASE</instr>
<desc>Read the FS segment base register and store the 32-bit result in dst.</desc>
<oper>dst[31:0] := FS_Segment_Base_Register;
dst[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>4210</id>
<name>_READFSBASE_U64</name>
<cpuid>FSGSBASE</cpuid>
<ret>unsigned __int64</ret>
<sign></sign>
<instr>RDFSBASE</instr>
<desc>Read the FS segment base register and store the 64-bit result in dst.</desc>
<oper>dst[63:0] := FS_Segment_Base_Register;</oper>
</intrinsic>
<intrinsic>
<id>4211</id>
<name>_READGSBASE_U32</name>
<cpuid>FSGSBASE</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr>RDGSBASE</instr>
<desc>Read the GS segment base register and store the 32-bit result in dst.</desc>
<oper>dst[31:0] := GS_Segment_Base_Register;
dst[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>4212</id>
<name>_READGSBASE_U64</name>
<cpuid>FSGSBASE</cpuid>
<ret>unsigned __int64</ret>
<sign></sign>
<instr>RDGSBASE</instr>
<desc>Read the GS segment base register and store the 64-bit result in dst.</desc>
<oper>dst[63:0] := GS_Segment_Base_Register;</oper>
</intrinsic>
<intrinsic>
<id>4411</id>
<name>_ROTL</name>
<cpuid></cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a,INT shift</sign>
<instr>ROL</instr>
<desc>Shift the bits of unsigned 32-bit integer a left by the number of bits specified in shift, rotating the most-significant bit to the least-significant bit location, and store the unsigned result in dst.</desc>
<oper>dst := a
count := shift BITWISE AND 31
DO WHILE (count &amp;gt; 0)
	tmp[0] := dst[31]
	dst := (dst &amp;lt;&amp;lt; 1) OR tmp[0]
	count := count - 1
OD</oper>
</intrinsic>
<intrinsic>
<id>4412</id>
<name>_ROTR</name>
<cpuid></cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a,INT shift</sign>
<instr>ROR</instr>
<desc>Shift the bits of unsigned 32-bit integer a right by the number of bits specified in shift, rotating the least-significant bit to the most-significant bit location, and store the unsigned result in dst.</desc>
<oper>dst := a
count := shift BITWISE AND 31
DO WHILE (count &amp;gt; 0)
	tmp[31] := dst[0]
	dst := (dst &amp;gt;&amp;gt; 1) OR tmp[31]
	count := count - 1
OD</oper>
</intrinsic>
<intrinsic>
<id>4413</id>
<name>_ROTWL</name>
<cpuid></cpuid>
<ret>unsigned short</ret>
<sign>UNSIGNED_SHORT a,INT shift</sign>
<instr>ROL</instr>
<desc>Shift the bits of unsigned 16-bit integer a left by the number of bits specified in shift, rotating the most-significant bit to the least-significant bit location, and store the unsigned result in dst.</desc>
<oper>dst := a
count := shift BITWISE AND 15
DO WHILE (count &amp;gt; 0)
	tmp[0] := dst[15]
	dst := (dst &amp;lt;&amp;lt; 1) OR tmp[0]
	count := count - 1
OD</oper>
</intrinsic>
<intrinsic>
<id>4414</id>
<name>_ROTWR</name>
<cpuid></cpuid>
<ret>unsigned short</ret>
<sign>UNSIGNED_SHORT a,INT shift</sign>
<instr>ROR</instr>
<desc>Shift the bits of unsigned 16-bit integer a right by the number of bits specified in shift, rotating the least-significant bit to the most-significant bit location, and store the unsigned result in dst.</desc>
<oper>dst := a
count := shift BITWISE AND 15
DO WHILE (count &amp;gt; 0)
	tmp[15] := dst[0]
	dst := (dst &amp;gt;&amp;gt; 1) OR tmp[15]
	count := count - 1
OD</oper>
</intrinsic>
<intrinsic>
<id>5141</id>
<name>_STOREBE_I16</name>
<cpuid>SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR ptr,SHORT data</sign>
<instr>NONE</instr>
<desc>Stores word-sized (16-bit) data to address ptr in big-endian format.
	</desc>
<oper>addr := MEM[ptr]
FOR j := 0 to 1
	i := j*8
	addr[i+7:i] := data[15-i:15-i-7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5142</id>
<name>_STOREBE_I32</name>
<cpuid>SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR ptr,INT data</sign>
<instr>NONE</instr>
<desc>Stores double word-sized (32-bit) data to address ptr in big-endian format.
	</desc>
<oper>addr := MEM[ptr]
FOR j := 0 to 4
	i := j*8
	addr[i+7:i] := data[31-i:31-i-7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5143</id>
<name>_STOREBE_I64</name>
<cpuid>SVML</cpuid>
<ret>void</ret>
<sign>VOID_PTR ptr,__INT64 data</sign>
<instr>NONE</instr>
<desc>Stores quad word-sized (64-bit) data to address ptr in big-endian format.
	</desc>
<oper>addr := MEM[ptr]
FOR j := 0 to 7
	i := j*8
	addr[i+7:i] := data[63-i:63-i-7]
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5282</id>
<name>_SUBBORROW_U32</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED_CHAR b_in,UNSIGNED_INT a,UNSIGNED_INT b,UNSIGNED_INT_PTR out</sign>
<instr>SBB</instr>
<desc>Add unsigned 8-bit borrow b_in (carry flag) to unsigned 32-bit integer a, and subtract the result from unsigned 32-bit integer b. Store the unsigned 32-bit result in out, and the carry-out in dst (carry or overflow flag).</desc>
<oper>dst:out[31:0] := (b[31:0] - (a[31:0] + b_in));</oper>
</intrinsic>
<intrinsic>
<id>5283</id>
<name>_SUBBORROW_U64</name>
<cpuid></cpuid>
<ret>unsigned char</ret>
<sign>UNSIGNED_CHAR b_in,UNSIGNED__INT64 a,UNSIGNED__INT64 b,UNSIGNED__INT64_PTR out</sign>
<instr>SBB</instr>
<desc>Add unsigned 8-bit borrow b_in (carry flag) to unsigned 64-bit integer a, and subtract the result from unsigned 64-bit integer b. Store the unsigned 64-bit result in out, and the carry-out in dst (carry or overflow flag).</desc>
<oper>dst:out[63:0] := (b[63:0] - (a[63:0] + b_in));</oper>
</intrinsic>
<intrinsic>
<id>5488</id>
<name>_TZCNT_U32</name>
<cpuid>BMI1</cpuid>
<ret>unsigned int</ret>
<sign>UNSIGNED_INT a</sign>
<instr>TZCNT</instr>
<desc>Count the number of trailing zero bits in unsigned 32-bit integer a, and return that count in dst.</desc>
<oper>tmp := 0
dst := 0
DO WHILE ((tmp &amp;lt; 32) AND a[tmp] = 0)
	tmp := tmp + 1
	dst := dst + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>5489</id>
<name>_TZCNT_U64</name>
<cpuid>BMI1</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>TZCNT</instr>
<desc>Count the number of trailing zero bits in unsigned 64-bit integer a, and return that count in dst.</desc>
<oper>tmp := 0
dst := 0
DO WHILE ((tmp &amp;lt; 64) AND a[tmp] = 0)
	tmp := tmp + 1
	dst := dst + 1
OD</oper>
</intrinsic>
<intrinsic>
<id>5634</id>
<name>_WRITEFSBASE_U32</name>
<cpuid>FSGSBASE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr>WRFSBASE</instr>
<desc>Write the unsigned 32-bit integer a to the FS segment base register.</desc>
<oper>FS_Segment_Base_Register[31:0] := a[31:0];
FS_Segment_Base_Register[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5635</id>
<name>_WRITEFSBASE_U64</name>
<cpuid>FSGSBASE</cpuid>
<ret>void</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>WRFSBASE</instr>
<desc>Write the unsigned 64-bit integer a to the FS segment base register.</desc>
<oper>FS_Segment_Base_Register[63:0] := a[63:0];</oper>
</intrinsic>
<intrinsic>
<id>5636</id>
<name>_WRITEGSBASE_U32</name>
<cpuid>FSGSBASE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a</sign>
<instr>WRGSBASE</instr>
<desc>Write the unsigned 32-bit integer a to the GS segment base register.</desc>
<oper>GS_Segment_Base_Register[31:0] := a[31:0];
GS_Segment_Base_Register[63:32] := 0</oper>
</intrinsic>
<intrinsic>
<id>5637</id>
<name>_WRITEGSBASE_U64</name>
<cpuid>FSGSBASE</cpuid>
<ret>void</ret>
<sign>UNSIGNED__INT64 a</sign>
<instr>WRGSBASE</instr>
<desc>Write the unsigned 64-bit integer a to the GS segment base register.</desc>
<oper>GS_Segment_Base_Register[63:0] := a[63:0];</oper>
</intrinsic>
<intrinsic>
<id>5638</id>
<name>_XABORT</name>
<cpuid>RTM</cpuid>
<ret>void</ret>
<sign>CONST_UNSIGNED_INT imm8</sign>
<instr>XABORT</instr>
<desc>
	Force an RTM abort. The EAX register is updated to reflect an XABORT instruction caused the abort, and the imm8 parameter will be provided in bits [31:24] of EAX.
	Following an RTM abort, the logical processor resumes execution at the fallback address computed through the outermost XBEGIN instruction. 
	</desc>
<oper>IF RTM_ACTIVE = 0
	// nop
ELSE
	// restore architectural register state
	// discard memory updates performed in transaction
	// update EAX with status and imm8 value
	RTM_NEST_COUNT := 0
	RTM_ACTIVE := 0
	IF 64-bit Mode
		RIP := fallbackRIP
	ELSE
		EIP := fallbackEIP
	FI
FI</oper>
</intrinsic>
<intrinsic>
<id>5639</id>
<name>_XBEGIN</name>
<cpuid>RTM</cpuid>
<ret>unsigned int</ret>
<sign></sign>
<instr>XBEGIN</instr>
<desc>
	Specify the start of an RTM code region. 
	If the logical processor was not already in transactional execution, then this call causes the logical processor to transition into transactional execution. 
	On an RTM abort, the logical processor discards all architectural register and memory updates performed during the RTM execution, restores architectural state, and starts execution beginning at the fallback address computed from the outermost XBEGIN instruction.
	</desc>
<oper>IF RTM_NEST_COUNT &amp;lt; MAX_RTM_NEST_COUNT
	RTM_NEST_COUNT := RTM_NEST_COUNT + 1
	IF RTM_NEST_COUNT = 1
		IF 64-bit Mode
			fallbackRIP := RIP + SignExtend(IMM)
		ELSE IF 32-bit Mode
			fallbackEIP := EIP + SignExtend(IMM)
		ELSE // 16-bit Mode
			fallbackEIP := (EIP + SignExtend(IMM)) AND 0x0000FFFF
		FI
		
		RTM_ACTIVE := 1
		// enter RTM execution, record register state, start tracking memory state
	FI
ELSE
	// RTM abort (see _xabort)
FI</oper>
</intrinsic>
<intrinsic>
<id>5640</id>
<name>_XEND</name>
<cpuid>RTM</cpuid>
<ret>void</ret>
<sign></sign>
<instr>XEND</instr>
<desc>
	Specify the end of an RTM code region.
	If this corresponds to the outermost scope, the logical processor will attempt to commit the logical processor state atomically. 
	If the commit fails, the logical processor will perform an RTM abort.
	</desc>
<oper>IF RTM_ACTIVE = 1
	RTM_NEST_COUNT := RTM_NEST_COUNT - 1
	IF RTM_NEST_COUNT = 0
		// try to commit transaction
		IF fail to commit transaction
			// RTM abort (see _xabort)
		ELSE
			RTM_ACTIVE = 0
		FI
	FI
FI</oper>
</intrinsic>
<intrinsic>
<id>5641</id>
<name>_XGETBV</name>
<cpuid>XSAVE</cpuid>
<ret>unsigned __int64</ret>
<sign>UNSIGNED_INT a</sign>
<instr>XGETBV</instr>
<desc>Copy up to 64-bits from the value of the extended control register (XCR) specified by a into dst. Currently only XFEATURE_ENABLED_MASK XCR is supported.</desc>
<oper>dst[63:0] := XCR[a]</oper>
</intrinsic>
<intrinsic>
<id>5678</id>
<name>_XRSTOR</name>
<cpuid>XSAVE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 rs_mask</sign>
<instr>XRSTOR</instr>
<desc>Perform a full or partial restore of the enabled processor states using the state information stored in memory at mem_addr. State is restored based on bits [62:0] in rs_mask, XCR0, and mem_addr.HEADER.XSTATE_BV. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>st_mask = mem_addr.HEADER.XSTATE_BV[62:0]
FOR i := 0 to 62
	IF (rs_mask[i] AND XCR0[i])
		IF st_mask[i]
			CASE (i) OF
			0: ProcessorState[x87 FPU] := mem_addr.FPUSSESave_Area[FPU]
			1: ProcessorState[SSE] := mem_addr.FPUSSESaveArea[SSE]
			DEFAULT: ProcessorState[i] := mem_addr.Ext_Save_Area[i]
			ESAC
		ELSE
			// ProcessorExtendedState := Processor Supplied Values
			CASE (i) OF
			1: MXCSR := mem_addr.FPUSSESave_Area[SSE]
			ESAC
		FI
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5679</id>
<name>_XRSTOR64</name>
<cpuid>XSAVE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 rs_mask</sign>
<instr>XRSTOR64</instr>
<desc>Perform a full or partial restore of the enabled processor states using the state information stored in memory at mem_addr. State is restored based on bits [62:0] in rs_mask, XCR0, and mem_addr.HEADER.XSTATE_BV. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>st_mask = mem_addr.HEADER.XSTATE_BV[62:0]
FOR i := 0 to 62
	IF (rs_mask[i] AND XCR0[i])
		IF st_mask[i]
			CASE (i) OF
			0: ProcessorState[x87 FPU] := mem_addr.FPUSSESave_Area[FPU]
			1: ProcessorState[SSE] := mem_addr.FPUSSESaveArea[SSE]
			DEFAULT: ProcessorState[i] := mem_addr.Ext_Save_Area[i]
			ESAC
		ELSE
			// ProcessorExtendedState := Processor Supplied Values
			CASE (i) OF
			1: MXCSR := mem_addr.FPUSSESave_Area[SSE]
			ESAC
		FI
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5680</id>
<name>_XRSTORS</name>
<cpuid>XSAVE, XSS</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR mem_addr,UNSIGNED__INT64 rs_mask</sign>
<instr>XRSTORS</instr>
<desc>Perform a full or partial restore of the enabled processor states using the state information stored in memory at mem_addr. xrstors differs from xrstor in that it can restore state components corresponding to bits set in the IA32_XSS MSR; xrstors cannot restore from an xsave area in which the extended region is in the standard form. State is restored based on bits [62:0] in rs_mask, XCR0, and mem_addr.HEADER.XSTATE_BV. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>st_mask = mem_addr.HEADER.XSTATE_BV[62:0]
FOR i := 0 to 62
	IF (rs_mask[i] AND XCR0[i])
		IF st_mask[i]
			CASE (i) OF
			0: ProcessorState[x87 FPU] := mem_addr.FPUSSESave_Area[FPU]
			1: ProcessorState[SSE] := mem_addr.FPUSSESaveArea[SSE]
			DEFAULT: ProcessorState[i] := mem_addr.Ext_Save_Area[i]
			ESAC
		ELSE
			// ProcessorExtendedState := Processor Supplied Values
			CASE (i) OF
			1: MXCSR := mem_addr.FPUSSESave_Area[SSE]
			ESAC
		FI
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5681</id>
<name>_XRSTORS64</name>
<cpuid>XSAVE, XSS</cpuid>
<ret>void</ret>
<sign>CONST_VOID_PTR mem_addr,UNSIGNED__INT64 rs_mask</sign>
<instr>XRSTORS64</instr>
<desc>Perform a full or partial restore of the enabled processor states using the state information stored in memory at mem_addr. xrstors differs from xrstor in that it can restore state components corresponding to bits set in the IA32_XSS MSR; xrstors cannot restore from an xsave area in which the extended region is in the standard form. State is restored based on bits [62:0] in rs_mask, XCR0, and mem_addr.HEADER.XSTATE_BV. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>st_mask = mem_addr.HEADER.XSTATE_BV[62:0]
FOR i := 0 to 62
	IF (rs_mask[i] AND XCR0[i])
		IF st_mask[i]
			CASE (i) OF
			0: ProcessorState[x87 FPU] := mem_addr.FPUSSESave_Area[FPU]
			1: ProcessorState[SSE] := mem_addr.FPUSSESaveArea[SSE]
			DEFAULT: ProcessorState[i] := mem_addr.Ext_Save_Area[i]
			ESAC
		ELSE
			// ProcessorExtendedState := Processor Supplied Values
			CASE (i) OF
			1: MXCSR := mem_addr.FPUSSESave_Area[SSE]
			ESAC
		FI
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5682</id>
<name>_XSAVE</name>
<cpuid>XSAVE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVE</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5683</id>
<name>_XSAVE64</name>
<cpuid>XSAVE</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVE64</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5684</id>
<name>_XSAVEC</name>
<cpuid>XSAVE, XSAVEC</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVEC</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr; xsavec differs from xsave in that it uses compaction and that it may use init optimization. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5685</id>
<name>_XSAVEC64</name>
<cpuid>XSAVE, XSAVEC</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVEC64</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr; xsavec differs from xsave in that it uses compaction and that it may use init optimization. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5686</id>
<name>_XSAVEOPT</name>
<cpuid>XSAVE, XSAVEOPT</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVEOPT</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary. The hardware may optimize the manner in which data is saved. The performance of this instruction will be equal to or better than using the XSAVE instruction.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		2: mem_addr.EXT_SAVE_Area2[YMM] := ProcessorState[YMM]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5687</id>
<name>_XSAVEOPT64</name>
<cpuid>XSAVE, XSAVEOPT</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVEOPT64</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary. The hardware may optimize the manner in which data is saved. The performance of this instruction will be equal to or better than using the XSAVE64 instruction.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		2: mem_addr.EXT_SAVE_Area2[YMM] := ProcessorState[YMM]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5688</id>
<name>_XSAVES</name>
<cpuid>XSAVE, XSS</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVES</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr; xsaves differs from xsave in that it can save state components corresponding to bits set in IA32_XSS MSR and that it may use the modified optimization. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5689</id>
<name>_XSAVES64</name>
<cpuid>XSAVE, XSS</cpuid>
<ret>void</ret>
<sign>VOID_PTR mem_addr,UNSIGNED__INT64 save_mask</sign>
<instr>XSAVEC64</instr>
<desc>Perform a full or partial save of the enabled processor states to memory at mem_addr; xsaves differs from xsave in that it can save state components corresponding to bits set in IA32_XSS MSR and that it may use the modified optimization. State is saved based on bits [62:0] in save_mask and XCR0. mem_addr must be aligned on a 64-byte boundary.</desc>
<oper>mask[62:0] := save_mask[62:0] BITWISE AND XCR0[62:0]
FOR i := 0 to 62
	IF mask[i]
		CASE (i) OF
		0: mem_addr.FPUSSESave_Area[FPU] := ProcessorState[x87 FPU]
		1: mem_addr.FPUSSESaveArea[SSE] := ProcessorState[SSE]
		DEFAULT: mem_addr.Ext_Save_Area[i] := ProcessorState[i]
		ESAC
		mem_addr.HEADER.XSTATE_BV[i] := INIT_FUNCTION[i]
	FI
	i := i + 1
ENDFOR</oper>
</intrinsic>
<intrinsic>
<id>5690</id>
<name>_XSETBV</name>
<cpuid>XSAVE</cpuid>
<ret>void</ret>
<sign>UNSIGNED_INT a,UNSIGNED__INT64 val</sign>
<instr>XSETBV</instr>
<desc>Copy 64-bits from val to the extended control register (XCR) specified by a. Currently only XFEATURE_ENABLED_MASK XCR is supported.</desc>
<oper>XCR[a] := val[63:0]</oper>
</intrinsic>
<intrinsic>
<id>5691</id>
<name>_XTEST</name>
<cpuid>RTM</cpuid>
<ret>unsigned char</ret>
<sign></sign>
<instr>XTEST</instr>
<desc>Query the transactional execution status, return 0 if inside a transactionally executing RTM or HLE region, and return 1 otherwise.</desc>
<oper>IF (RTM_ACTIVE = 1 OR HLE_ACTIVE = 1)
	dst := 0
ELSE
	dst := 1
FI</oper>
</intrinsic>
</intrinsicsdudedata>
